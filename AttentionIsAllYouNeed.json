[
    {
        "paper": "Attention Is All You Need",
        "section": "Abstract",
        "content": "The Transformer is a novel neural network architecture that relies entirely on attention mechanisms, removing the need for recurrence and convolutions. It achieves state-of-the-art results on machine translation tasks, training faster and more efficiently than previous models. Additionally, it generalizes well to other NLP tasks like English constituency parsing."
    },
    
    {
        "paper": "Attention Is All You Need",
        "section": "Introduction",
        "content": "Recurrent neural networks, long short-term memory, and gated recurrent neural networks have been state-of-the-art in sequence modeling, but their sequential nature limits parallelization. Attention mechanisms model dependencies regardless of distance in sequences and are usually combined with recurrent models. The Transformer architecture removes recurrence entirely, relying solely on attention to capture global dependencies, enabling greater parallelization and achieving state-of-the-art translation quality after only twelve hours of training on eight GPUs."
    },
    
    {
        "paper": "Attention Is All You Need",
        "section": "Model Architecture",
        "content": "Most neural sequence transduction models use an encoder-decoder structure where the encoder maps input sequences to continuous representations, and the decoder generates output sequences one symbol at a time in an auto-regressive fashion. The Transformer adopts this architecture but replaces recurrence with stacked self-attention and fully connected layers in both encoder and decoder. This design enables the model to capture dependencies within sequences efficiently while allowing parallel computation."
    },
    
    {
        "paper": "Attention Is All You Need",
        "section": "3.1 Encoder and Decoder Stacks",
        "content": "The encoder consists of a stack of 6 identical layers, each containing a multi-head self-attention sub-layer and a position-wise feed-forward network, with residual connections and layer normalization. The decoder also has 6 layers, adding a third sub-layer that performs multi-head attention over the encoder's output. The decoder’s self-attention is masked to prevent positions from attending to subsequent positions, ensuring autoregressive prediction. Both encoder and decoder outputs have a fixed dimension of 512 to support residual connections."
    },
    
    {
        "paper": "Attention Is All You Need",
        "section": "3.2 Attention",
        "content": "Attention maps a query and a set of key-value pairs to an output vector computed as a weighted sum of the values. The weights are determined by a compatibility function measuring the similarity between the query and each key. Multi-head attention runs several attention layers in parallel to capture information from different representation subspaces."
    },
    
    {
        "paper": "Attention Is All You Need",
        "section": "3.2.1 Scaled Dot-Product Attention",
        "content": "Scaled Dot-Product Attention computes attention by taking the dot product of queries and keys, scaling by 1/sqrt(d_k), and applying softmax to obtain weights for the values. This method is more efficient than additive attention while maintaining similar theoretical complexity. Scaling prevents large dot product values from pushing softmax into regions with small gradients, improving training stability. The attention function operates on matrices of queries, keys, and values simultaneously for efficiency."
    },
    
    {
        "paper": "Attention Is All You Need",
        "section": "3.2.2 Multi-Head Attention",
        "content": "Multi-head attention performs attention in parallel across multiple projections of queries, keys, and values, allowing the model to attend to different representation subspaces simultaneously. Each attention head operates on lower-dimensional vectors, and their outputs are concatenated and linearly projected to form the final result. This approach improves model performance and computational efficiency compared to a single attention head. The Transformer uses 8 heads, each with dimension 64, balancing expressiveness and speed."
    },
    
    {
        "paper": "Attention Is All You Need",
        "section": "3.2.3 Applications of Attention in the Transformer",
        "content": "The Transformer uses multi-head attention in three ways: encoder-decoder attention where decoder queries attend to encoder outputs, encoder self-attention where positions attend to all previous encoder positions, and decoder self-attention with masking to prevent attending to future positions, preserving the autoregressive property."
    },
    
    {
        "paper": "Attention Is All You Need",
        "section": "3.3 Position-wise Feed-Forward Networks",
        "content": "Each encoder and decoder layer contains a fully connected feed-forward network applied independently to each position. It consists of two linear transformations separated by a ReLU activation, with different parameters for each layer. The input and output dimensionality is 512, while the inner layer has dimensionality 2048."
    },
    
    {
        "paper": "Attention Is All You Need",
        "section": "3.4 Embeddings and Softmax",
        "content": "The model uses learned embeddings to convert input and output tokens into vectors of dimension d_model, sharing weights between embedding layers and the pre-softmax linear transformation. The embedding weights are scaled by the square root of d_model. This approach is similar to previous models and helps efficiently compute predicted next-token probabilities."
    },
    
    {
        "paper": "Attention Is All You Need",
        "section": "3.5 Positional Encoding",
        "content": "Because the Transformer lacks recurrence and convolution, positional encodings are added to input embeddings to provide sequence order information. These encodings use sine and cosine functions at different frequencies, enabling the model to learn relative positions. The sinusoidal encoding allows extrapolation to longer sequences beyond training lengths. Learned positional embeddings were also tested but showed similar results."
    },
    
    {
        "paper": "Attention Is All You Need",
        "section": "4 Why Self-Attention",
        "content": "Self-attention layers offer shorter paths for learning long-range dependencies compared to recurrent and convolutional layers, enabling easier training of sequence models. They allow greater parallelization and lower computational complexity when sequence length is smaller than representation dimension. While convolutions require multiple layers to capture long-range dependencies, self-attention connects all positions directly. Additionally, self-attention models tend to be more interpretable, with attention heads specializing in syntactic and semantic tasks."
    },
    
    {
        "paper": "Attention Is All You Need",
        "section": "5.1 Training Data and Batching",
        "content": "The model was trained on the WMT 2014 English-German dataset with 4.5 million sentence pairs and byte-pair encoding with a shared 37,000-token vocabulary. For English-French, the larger WMT 2014 dataset with 36 million sentences and a 32,000 word-piece vocabulary was used. Training batches grouped sentence pairs by approximate sequence length, each containing about 25,000 source and target tokens."
    },
    
    {
        "paper": "Attention Is All You Need",
        "section": "5.2 Hardware and Schedule",
        "content": "Training was performed on a single machine with 8 NVIDIA P100 GPUs, with each base model step taking about 0.4 seconds. Base models were trained for 100,000 steps, totaling approximately 12 hours. Larger models required 1 second per step and were trained for 300,000 steps over 3.5 days."
    },
    
    {
        "paper": "Attention Is All You Need",
        "section": "5.3 Optimizer",
        "content": "The model uses the Adam optimizer with β1=0.9, β2=0.98, and ϵ=10^-9. The learning rate increases linearly during the first 4000 warmup steps and then decreases proportionally to the inverse square root of the step number. This schedule helps stabilize training and improve convergence."
    },
    
    {
        "paper": "Attention Is All You Need",
        "section": "5.4 Regularization",
        "content": "The model applies dropout to sub-layer outputs and the sum of embeddings and positional encodings, with a rate of 0.1 for the base model. Label smoothing with a value of 0.1 is used during training to improve accuracy and BLEU scores despite hurting perplexity. These regularization techniques help prevent overfitting and improve model generalization."
    },
    
    {
        "paper": "Attention Is All You Need",
        "section": "6.1 Machine Translation",
        "content": "The big Transformer model achieves state-of-the-art BLEU scores of 28.4 on English-German and 41.0 on English-French translation tasks, outperforming previous models and ensembles. Training took 3.5 days on 8 P100 GPUs for the big model, while the base model surpassed prior results at a fraction of the cost. The model used dropout rates of 0.1 for big and 0.3 for base models, and averaged multiple checkpoints during training. Beam search with a size of 4 and length penalty of 0.6 was employed during inference to optimize output quality."
    },
    
    {
        "paper": "Attention Is All You Need",
        "section": "6.2 Model Variations",
        "content": "The authors evaluated Transformer components by varying the base model and measuring English-to-German translation performance. They found that single-head attention reduced BLEU score by 0.9 compared to the best multi-head setting, while too many heads also hurt performance. Reducing the attention key size decreased quality, suggesting compatibility functions more complex than dot product may help. Larger models and dropout improved results, and sinusoidal positional encodings performed similarly to learned positional embeddings."
    },
    
    {
        "paper": "Attention Is All You Need",
        "section": "6.3 English Constituency Parsing",
        "content": "The Transformer was tested on English constituency parsing using the WSJ Penn Treebank dataset with 40K training sentences and a semi-supervised setting with 17M sentences. It used a vocabulary of 16K tokens for WSJ-only and 32K for semi-supervised training, with minor hyperparameter tuning. Despite minimal task-specific tuning, the Transformer outperformed all previously reported models except the Recurrent Neural Network Grammar. It also outperformed the BerkeleyParser when trained only on WSJ data, showing strong generalization to structured prediction tasks."
    },
    
    {
        "paper": "Attention Is All You Need",
        "section": "7 Conclusion",
        "content": "This work introduced the Transformer, the first sequence transduction model relying solely on multi-headed self-attention instead of recurrence. It achieves state-of-the-art results on WMT 2014 English-to-German and English-to-French translation tasks with faster training times. The authors are optimistic about applying attention-based models to other modalities like images, audio, and video, and aim to improve generation efficiency. The training and evaluation code is publicly available on GitHub."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "Abstract",
        "content": "BERT is a new language representation model that pretrains deep bidirectional representations by considering both left and right context simultaneously. It can be fine-tuned with minimal task-specific changes to achieve state-of-the-art performance across a variety of NLP tasks. BERT significantly improves results on benchmarks like GLUE, MultiNLI, and SQuAD."
    }, 
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "Introduction - Background",
        "content": "Pre-training language models has improved many NLP tasks like natural language inference, paraphrasing, named entity recognition, and question answering. Existing approaches include feature-based methods like ELMo and fine-tuning methods like OpenAI GPT, both using unidirectional language models."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "Introduction - Limitations of Unidirectional Models",
        "content": "Unidirectional language models limit architectural choices because they only attend to tokens in one direction, which is suboptimal for tasks requiring full context like question answering. For example, GPT’s left-to-right model can't fully leverage context from both sides."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "Introduction - BERT Approach",
        "content": "BERT addresses this by using a masked language model (MLM) objective that masks tokens and predicts them using both left and right context, enabling deep bidirectional representations. BERT also employs a next sentence prediction task for pretraining text-pair representations."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "Introduction - Contributions",
        "content": "BERT demonstrates the importance of bidirectional pre-training, unlike unidirectional models used in prior work. It reduces the need for heavily engineered architectures, achieving state-of-the-art results on eleven NLP tasks. The pre-trained models and code are publicly available."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "2.1 Unsupervised Feature-based Approaches - Part 1",
        "content": "Learning widely applicable word representations has been an active research area for decades, with both non-neural (Brown et al., 1992; Ando and Zhang, 2005) and neural (Mikolov et al., 2013; Pennington et al., 2014) methods. Pre-trained word embeddings have become integral in NLP, providing significant improvements over embeddings learned from scratch (Turian et al., 2010). Common pretraining objectives include left-to-right language modeling (Mnih and Hinton, 2009) and discriminating correct from incorrect words based on left and right context (Mikolov et al., 2013)."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "2.1 Unsupervised Feature-based Approaches - Part 2",
        "content": "These embedding approaches have been extended beyond words to coarser granularities, such as sentence embeddings (Kiros et al., 2015; Logeswaran and Lee, 2018) and paragraph embeddings (Le and Mikolov, 2014). Sentence representations have been trained using objectives like ranking candidate next sentences (Jernite et al., 2017; Logeswaran and Lee, 2018) and left-to-right generation of next sentence words conditioned on the previous sentence (Kiros et al., 2015). Denoising autoencoder-derived objectives have also been used for sentence embedding training (Hill et al., 2016)."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "2.1 Unsupervised Feature-based Approaches - Part 3",
        "content": "ELMo (Peters et al., 2017, 2018a) advances traditional word embedding research by extracting context-sensitive features from both left-to-right and right-to-left language models. The contextual token representation is a concatenation of these two directional embeddings. Integrating ELMo embeddings into task-specific architectures significantly improved performance on various NLP benchmarks, such as question answering (Rajpurkar et al., 2016), sentiment analysis (Socher et al., 2013), and named entity recognition (Tjong Kim Sang and De Meulder, 2003)."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "2.1 Unsupervised Feature-based Approaches - Part 4",
        "content": "Other related work includes Melamud et al. (2016), who proposed learning contextual representations by predicting a single word from both left and right context using LSTMs. However, their model is feature-based and not deeply bidirectional like ELMo. Additionally, Fedus et al. (2018) demonstrated that the cloze task can improve the robustness of text generation models."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "2.2 Unsupervised Fine-tuning Approaches - Part 1",
        "content": "Early work in unsupervised fine-tuning approaches focused on pre-training only word embedding parameters from unlabeled text (Collobert and Weston, 2008). More recent methods have pre-trained sentence or document encoders that produce contextual token representations from unlabeled text and fine-tune them for supervised downstream tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018)."
    },
        
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "2.2 Unsupervised Fine-tuning Approaches - Part 2",
        "content": "The advantage of these approaches is that fewer parameters need to be learned from scratch during fine-tuning. This efficiency partly explains why OpenAI GPT (Radford et al., 2018) achieved state-of-the-art results on many sentence-level tasks in the GLUE benchmark (Wang et al., 2018a)."
    },
        
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "2.2 Unsupervised Fine-tuning Approaches - Part 3",
        "content": "Pre-training objectives for these models commonly include left-to-right language modeling and auto-encoder tasks (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015). These objectives help the models learn effective contextual representations that transfer well to downstream tasks."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "3 BERT - Part 1",
        "content": "We introduce BERT and its detailed implementation, which consists of two steps: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data using various tasks."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "3 BERT - Part 2",
        "content": "For fine-tuning, BERT is initialized with the pre-trained parameters and all parameters are fine-tuned using labeled data for specific downstream tasks. Each downstream task has its own fine-tuned model, even though all start from the same pre-trained parameters."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "3 BERT - Part 3",
        "content": "A key characteristic of BERT is its unified architecture across different tasks, with minimal difference between the pre-trained and final downstream architectures. The question-answering example is used as a running example throughout the section."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "Model Architecture - Part 1",
        "content": "BERT's architecture is a multi-layer bidirectional Transformer encoder based on Vaswani et al. It uses layers (L), hidden size (H), and attention heads (A) to define model size."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "Model Architecture - Part 2",
        "content": "There are two main sizes: BERTBASE with 12 layers and 110M parameters, and BERTLARGE with 24 layers and 340M parameters."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "Model Architecture - Part 3",
        "content": "Unlike GPT's uni-directional self-attention, BERT uses bidirectional self-attention, allowing tokens to attend to both left and right context."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "Input/Output Representations",
        "content": "BERT’s input representation handles single sentences or sentence pairs by packing them into one token sequence, starting with a special [CLS] token for classification. Sentence pairs are separated by a [SEP] token, and tokens receive learned embeddings to indicate which sentence they belong to (A or B). Each token’s representation is the sum of its token embedding, segment embedding, and position embedding, enabling BERT to distinguish context within the input sequence."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "3.1 Pre-training BERT - Masked Language Model (MLM)",
        "content": "Unlike traditional left-to-right or right-to-left language models, BERT is pre-trained using a masked language model (MLM) task. This involves randomly masking 15% of input tokens and training the model to predict those masked tokens based on context from both directions. To avoid mismatch with fine-tuning, masked tokens are replaced 80% of the time with [MASK], 10% with a random token, and 10% kept unchanged. This enables BERT to learn deep bidirectional representations."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "3.1 Pre-training BERT - Next Sentence Prediction (NSP)",
        "content": "To help BERT understand relationships between sentences, a next sentence prediction (NSP) task is used during pre-training. For each sentence pair, 50% are actual consecutive sentences labeled 'IsNext' and 50% are random pairs labeled 'NotNext'. This simple binary task improves performance on downstream tasks like question answering and natural language inference."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "3.1 Pre-training BERT - Pre-training Data",
        "content": "BERT is pre-trained on a large corpus combining the BooksCorpus (800 million words) and English Wikipedia (2.5 billion words). Only plain text passages are used from Wikipedia, excluding lists, tables, and headers. Using document-level data is crucial to provide long contiguous sequences, unlike shuffled sentence-level corpora such as the Billion Word Benchmark."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "3.2 Fine-tuning BERT - Overview",
        "content": "Fine-tuning BERT is straightforward because its self-attention mechanism unifies encoding of single texts and text pairs, enabling bidirectional cross attention in one step. Tasks such as paraphrasing, entailment, question answering, and classification are handled by plugging in task-specific inputs and outputs, with token-level tasks using token representations and classification tasks using the [CLS] token representation."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "3.2 Fine-tuning BERT - Efficiency",
        "content": "Fine-tuning is relatively inexpensive compared to pre-training. The models can be fine-tuned in at most 1 hour on a single Cloud TPU or a few hours on a GPU, starting from the same pre-trained parameters. Task-specific details and configurations are provided in the paper’s later sections and appendices."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "4.1 GLUE Benchmark",
        "content": "BERT was fine-tuned and evaluated on the GLUE benchmark, a suite of diverse natural language understanding tasks. Using batch size 32 and 3 epochs, the best learning rate was selected from 5e-5 to 2e-5, with random restarts applied for stability on smaller datasets. Both BERTBASE and BERTLARGE significantly outperformed prior systems, with BERTLARGE achieving an 80.5 GLUE score, a 7.7 point improvement over OpenAI GPT."
    },
    {
        "section": "5.1 Effect of Pre-training Tasks - Part 1",
        "content": "We evaluate two pre-training objectives using the same data, fine-tuning, and hyperparameters as BERTBASE: one without the next sentence prediction (NSP) task, and a left-to-right (LTR) model without NSP. Removing NSP significantly hurts performance on QNLI, MNLI, and SQuAD 1.1. The left-to-right model performs worse than the masked language model (MLM) on all tasks, especially on MRPC and SQuAD."
    },
    {
        "section": "5.1 Effect of Pre-training Tasks - Part 2",
        "content": "For SQuAD, a left-to-right model struggles because token predictions lack right-side context. Adding a BiLSTM improves SQuAD results but still falls short of the bidirectional model's performance. The BiLSTM addition decreases performance on the GLUE tasks."
    },
    {
        "section": "5.1 Effect of Pre-training Tasks - Part 3",
        "content": "Training separate left-to-right and right-to-left models, like ELMo, is possible but twice as expensive and less effective for tasks like QA. This approach is non-intuitive because the right-to-left model cannot condition answers on questions. Overall, a deep bidirectional model is strictly more powerful since it uses both left and right context at every layer."
    },
    {
        "section": "5.2 Effect of Model Size - Part 1",
        "content": "We trained multiple BERT models with varying layers, hidden units, and attention heads using the same hyperparameters and procedure. Larger models consistently improve fine-tuning accuracy across GLUE tasks, even on small datasets like MRPC with only 3,600 labeled examples. These improvements surpass previous large Transformer models in literature."
    },
    {
        "section": "5.2 Effect of Model Size - Part 2",
        "content": "BERTBASE has 110 million parameters and BERTLARGE has 340 million, larger than prior models such as those in Vaswani et al. and Al-Rfou et al. Increasing model size is known to help on large-scale tasks like machine translation and language modeling, shown by better perplexity on held-out data. This work is the first to show convincingly that extreme model scaling also improves performance on very small downstream tasks if the model is well pre-trained."
    },
    {
        "section": "5.2 Effect of Model Size - Part 3",
        "content": "Previous work showed mixed results on increasing pre-trained bi-LM size and hidden dimensions for feature-based models. However, fine-tuning directly on downstream tasks with minimal new parameters allows these tasks to benefit from larger, more expressive pre-trained representations. This benefit is evident even when the downstream task has very limited data."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "5.3 Feature-based Approach with BERT",
        "content": "Besides fine-tuning, BERT can be used as a feature extractor by taking fixed contextual embeddings from pre-trained layers. This approach is useful for tasks requiring task-specific architectures or for computational efficiency by reusing pre-computed features. Experiments on Named Entity Recognition show the feature-based method performs nearly as well as fine-tuning, demonstrating BERT’s flexibility."
    },
    {
        "paper": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "section": "6 Conclusion",
        "content": "Transfer learning with rich unsupervised pre-training has become essential for language understanding systems, benefiting even low-resource tasks. This work extends these advances by introducing deep bidirectional architectures, which improve performance across a wide range of NLP tasks. The same pre-trained BERT model can be effectively fine-tuned for many different language tasks."
    }
      
      
      
      
          
          
          
          
          
      
    
          
  ]
  