In machine learning, supervised learning (SL) is a type of machine learning paradigm where an algorithm learns to map input data to a specific output based on example input-output pairs. This process involves training a statistical model using labeled data, meaning each piece of input data is provided with the correct output. For instance, if you want a model to identify cats in images, supervised learning would involve feeding it many images of cats (inputs) that are explicitly labeled "cat" (outputs).
The goal of supervised learning is for the trained model to accurately predict the output for new, unseen data. This requires the algorithm to effectively generalize from the training examples, a quality measured by its generalization error. Supervised learning is commonly used for tasks like classification (predicting a category, e.g., spam or not spam) and regression (predicting a continuous value, e.g., house prices).
To solve a given problem of supervised learning, the following steps must be performed:
Determine the type of training samples. Before doing anything else, the user should decide what kind of data is to be used as a training set. In the case of handwriting analysis, for example, this might be a single handwritten character, an entire handwritten word, an entire sentence of handwriting, or a full paragraph of handwriting.
Gather a training set. The training set needs to be representative of the real-world use of the function. Thus, a set of input objects is gathered together with corresponding outputs, either from human experts or from measurements.
Determine the input feature representation of the learned function. The accuracy of the learned function depends strongly on how the input object is represented. Typically, the input object is transformed into a feature vector, which contains a number of features that are descriptive of the object. The number of features should not be too large, because of the curse of dimensionality; but should contain enough information to accurately predict the output.
Determine the structure of the learned function and corresponding learning algorithm. For example, one may choose to use support-vector machines or decision trees.
Complete the design. Run the learning algorithm on the gathered training set. Some supervised learning algorithms require the user to determine certain control parameters. These parameters may be adjusted by optimizing performance on a subset (called a validation set) of the training set, or via cross-validation.
Evaluate the accuracy of the learned function. After parameter adjustment and learning, the performance of the resulting function should be measured on a test set that is separate from the training set.
A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the No free lunch theorem).
There are four major issues to consider in supervised learning:
A first issue is the tradeoff between bias and variance. Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input
if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for
. A learning algorithm has high variance for a particular input
if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm. Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be "flexible" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).
The second issue is of the amount of training data available relative to the complexity of the "true" function (classifier or regression function). If the true function is simple, then an "inflexible" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn with a large amount of training data paired with a "flexible" learning algorithm with low bias and high variance.
A third issue is the dimensionality of the input space. If the input feature vectors have large dimensions, learning the function can be difficult even if the true function only depends on a small number of those features. This is because the many "extra" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, input data of large dimensions typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, it will likely improve the accuracy of the learned function. In addition, there are many algorithms for feature selection that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.
A fourth issue is the degree of noise in the desired output values (the supervisory target variables). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to overfitting. You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled "corrupts" your training data – this phenomenon has been called deterministic noise. When either type of noise is present, it is better to go with a higher bias, lower variance estimator.
In practice, there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased generalization error with statistical significance.
Other factors to consider when choosing and applying a learning algorithm include the following:
Heterogeneity of the data. If the feature vectors include features of many different kinds (discrete, discrete ordered, counts, continuous values), some algorithms are easier to apply than others. Many algorithms, including support-vector machines, linear regression, logistic regression, neural networks, and nearest neighbor methods, require that the input features be numerical and scaled to similar ranges (e.g., to the [-1,1] interval). Methods that employ a distance function, such as nearest neighbor methods and support-vector machines with Gaussian kernels, are particularly sensitive to this. An advantage of decision trees is that they easily handle heterogeneous data.
Redundancy in the data. If the input features contain redundant information (e.g., highly correlated features), some learning algorithms (e.g., linear regression, logistic regression, and distance-based methods) will perform poorly because of numerical instabilities. These problems can often be solved by imposing some form of regularization.
Presence of interactions and non-linearities. If each of the features makes an independent contribution to the output, then algorithms based on linear functions (e.g., linear regression, logistic regression, support-vector machines, naive Bayes) and distance functions (e.g., nearest neighbor methods, support-vector machines with Gaussian kernels) generally perform well. However, if there are complex interactions among features, then algorithms such as decision trees and neural networks work better, because they are specifically designed to discover these interactions. Linear methods can also be applied, but the engineer must manually specify the interactions when using them.
When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see cross-validation). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.
The most widely used learning algorithms are:
Linear discriminant analysis
k-nearest neighbors algorithm
Neural networks (e.g., Multilayer perceptron)
Given a set of
training examples of the form
is the feature vector of the
-th example and
is its label (i.e., class), a learning algorithm seeks a function

is the input space and
is the output space. The function
is an element of some space of possible functions
, usually called the hypothesis space. It is sometimes convenient to represent
using a scoring function
 
is defined as returning the
value that gives the highest score:
\;f(x,y)
denote the space of scoring functions.
can be any space of functions, many learning algorithms are probabilistic models where
takes the form of a conditional probability model
\;P(y|x)
takes the form of a joint probability model
. For example, naive Bayes and linear discriminant analysis are joint probability models, whereas logistic regression is a conditional probability model.
There are two basic approaches to choosing
: empirical risk minimization and structural risk minimization. Empirical risk minimization seeks the function that best fits the training data. Structural risk minimization includes a penalty function that controls the bias/variance tradeoff.
In both cases, it is assumed that the training set consists of a sample of independent and identically distributed pairs,
. In order to measure how well a function fits the training data, a loss function
 ^
is defined. For training example
, the loss of predicting the value

,)
is defined as the expected loss of
. This can be estimated from the training data as
(g)=\sum _L(y_,g(x_))
In empirical risk minimization, the supervised learning algorithm seeks the function
. Hence, a supervised learning algorithm can be constructed by applying an optimization algorithm to find
is a conditional probability distribution
and the loss function is the negative log likelihood:
)=-\log P(y|x)
, then empirical risk minimization is equivalent to maximum likelihood estimation.
contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well (overfitting).
Structural risk minimization seeks to prevent overfitting by incorporating a regularization penalty into the optimization. The regularization penalty can be viewed as implementing a form of Occam's razor that prefers simpler functions over more complex ones.
A wide variety of penalties have been employed that correspond to different definitions of complexity. For example, consider the case where the function
is a linear function of the form
^\beta _x_
A popular regularization penalty is
\beta _^
, which is the squared Euclidean norm of the weights, also known as the
norm. Other norms include the
|\beta _|
, and the
"norm", which is the number of non-zero

s. The penalty will be denoted by
The supervised learning optimization problem is to find the function
(g)+\lambda C(g).

controls the bias-variance tradeoff. When

, this gives empirical risk minimization with low bias and high variance. When

is large, the learning algorithm will have high bias and low variance. The value of

can be chosen empirically via cross-validation.
The complexity penalty has a Bayesian interpretation as the negative log prior probability of

, in which case
is the posterior probability of
The training methods described above are discriminative training methods, because they seek to find a function
that discriminates well between the different output values (see discriminative model). For the special case where
is a joint probability distribution and the loss function is the negative log likelihood
\log P(x_,y_),
a risk minimization algorithm is said to perform generative training, because
can be regarded as a generative model that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in naive Bayes and linear discriminant analysis.
There are several ways in which the standard supervised learning problem can be generalized:
Semi-supervised learning or weak supervision: the desired output values are provided only for a subset of the training data. The remaining data is unlabeled or imprecisely labeled.
Active learning: Instead of assuming that all of the training examples are given at the start, active learning algorithms interactively collect new examples, typically by making queries to a human user. Often, the queries are based on unlabeled data, which is a scenario that combines semi-supervised learning with active learning.
Structured prediction: When the desired output value is a complex object, such as a parse tree or a labeled graph, then standard methods must be extended.
Learning to rank: When the input is a set of objects and the desired output is a ranking of those objects, then again the standard methods must be extended.
Artificial neural network
Decision tree learning
Inductive logic programming
Gaussian process regression
Group method of data handling
Learning classifier systems
Learning vector quantization
Minimum message length (decision trees, decision graphs, etc.)
Multilinear subspace learning
Naive Bayes classifier
Maximum entropy classifier
Conditional random field
Nearest neighbor algorithm
Probably approximately correct learning (PAC) learning
Ripple down rules, a knowledge acquisition methodology
Symbolic machine learning algorithms
Subsymbolic machine learning algorithms
Support vector machines
Minimum complexity machines (MCM)
Ensembles of classifiers
Handling imbalanced datasets
Statistical relational learning
Proaftn, a multicriteria classification algorithm
Quantitative structure–activity relationship
Learning to rank
Object recognition in computer vision
Optical character recognition
Supervised learning is a special case of downward causation in biological systems
Landform classification using satellite imagery
Spend classification in procurement processes
Computational learning theory
(Uncalibrated) class membership probabilities
List of datasets for machine-learning research
Machine Learning Open Source Software (MLOSS) Self-supervised learning (SSL) is a paradigm in machine learning where a model is trained on a task using the data itself to generate supervisory signals, rather than relying on externally-provided labels. In the context of neural networks, self-supervised learning aims to leverage inherent structures or relationships within the input data to create meaningful training signals. SSL tasks are designed so that solving them requires capturing essential features or relationships in the data. The input data is typically augmented or transformed in a way that creates pairs of related samples, where one sample serves as the input, and the other is used to formulate the supervisory signal. This augmentation can involve introducing noise, cropping, rotation, or other transformations. Self-supervised learning more closely imitates the way humans learn to classify objects.
During SSL, the model learns in two steps. First, the task is solved based on an auxiliary or pretext classification task using pseudo-labels, which help to initialize the model parameters. Next, the actual task is performed with supervised or unsupervised learning.
Self-supervised learning has produced promising results in recent years, and has found practical application in fields such as audio processing, and is being used by Facebook and others for speech recognition.
Autoassociative self-supervised learning is a specific category of self-supervised learning where a neural network is trained to reproduce or reconstruct its own input data. In other words, the model is tasked with learning a representation of the data that captures its essential features or structure, allowing it to regenerate the original input.
The term "autoassociative" comes from the fact that the model is essentially associating the input data with itself. This is often achieved using autoencoders, which are a type of neural network architecture used for representation learning. Autoencoders consist of an encoder network that maps the input data to a lower-dimensional representation (latent space), and a decoder network that reconstructs the input from this representation.
The training process involves presenting the model with input data and requiring it to reconstruct the same data as closely as possible. The loss function used during training typically penalizes the difference between the original input and the reconstructed output (e.g. mean squared error). By minimizing this reconstruction error, the autoencoder learns a meaningful representation of the data in its latent space.
For a binary classification task, training data can be divided into positive examples and negative examples. Positive examples are those that match the target. For example, if training a classifier to identify birds, the positive training data would include images that contain birds. Negative examples would be images that do not. Contrastive self-supervised learning uses both positive and negative examples. The loss function in contrastive learning is used to minimize the distance between positive sample pairs, while maximizing the distance between negative sample pairs.
An early example uses a pair of 1-dimensional convolutional neural networks to process a pair of images and maximize their agreement.
Contrastive Language-Image Pre-training (CLIP) allows joint pretraining of a text encoder and an image encoder, such that a matching image-text pair have image encoding vector and text encoding vector that span a small angle (having a large cosine similarity).
InfoNCE (Noise-Contrastive Estimation) is a method to optimize two models jointly, based on Noise Contrastive Estimation (NCE). Given a set
,\ldots x_\right\
random samples containing one positive sample from
\mid c_\right)
negative samples from the 'proposal' distribution
, it minimizes the following loss function:
_ =-\mathbb  _\left[\log \left(x_,c_\right)\in Xf_\left(x_,c_\right)\right]
Non-contrastive self-supervised learning (NCSSL) uses only positive examples. Counterintuitively, NCSSL converges on a useful local minimum rather than reaching a trivial solution, with zero loss. For the example of binary classification, it would trivially learn to classify each example as positive. Effective NCSSL requires an extra predictor on the online side that does not back-propagate on the target side.
SSL belongs to supervised learning methods insofar as the goal is to generate a classified output from the input. At the same time, however, it does not require the explicit use of labeled input-output pairs. Instead, correlations, metadata embedded in the data, or domain knowledge present in the input are implicitly and autonomously extracted from the data. These supervisory signals, extracted from the data, can then be used for training.
SSL is similar to unsupervised learning in that it does not require labels in the sample data. Unlike unsupervised learning, however, learning is not done using inherent data structures.
Semi-supervised learning combines supervised and unsupervised learning, requiring only a small portion of the learning data be labeled.
In transfer learning, a model designed for one task is reused on a different task.
Training an autoencoder intrinsically constitutes a self-supervised process, because the output pattern needs to become an optimal reconstruction of the input pattern itself. However, in current jargon, the term 'self-supervised' often refers to tasks based on a pretext-task training setup. This involves the (human) design of such pretext task(s), unlike
the case of fully self-contained autoencoder training.
In reinforcement learning, self-supervising learning from a combination of losses can create abstract representations where only the most important information about the state are kept in a compressed way.
Self-supervised learning is particularly suitable for speech recognition. For example, Facebook developed wav2vec, a self-supervised algorithm, to perform speech recognition using two deep convolutional neural networks that build on each other.
Google's Bidirectional Encoder Representations from Transformers (BERT) model is used to better understand the context of search queries.
OpenAI's GPT-3 is an autoregressive language model that can be used in language processing. It can be used to translate texts or answer questions, among other things.
Bootstrap Your Own Latent (BYOL) is a NCSSL that produced excellent results on ImageNet and on transfer and semi-supervised benchmarks.
The Yarowsky algorithm is an example of self-supervised learning in natural language processing. From a small number of labeled examples, it learns to predict which word sense of a polysemous word is being used at a given point in text.
DirectPred is a NCSSL that directly sets the predictor weights instead of learning it via typical gradient descent.
Self-GenomeNet is an example of self-supervised learning in genomics.
Self-supervised learning continues to gain prominence as a new approach across diverse fields. Its ability to leverage unlabeled data effectively opens new possibilities for advancement in machine learning, especially in data-driven application domains.
Balestriero, Randall; Ibrahim, Mark; Sobal, Vlad; Morcos, Ari; Shekhar, Shashank; Goldstein, Tom; Bordes, Florian; Bardes, Adrien; Mialon, Gregoire; Tian, Yuandong; Schwarzschild, Avi; Wilson, Andrew Gordon; Geiping, Jonas; Garrido, Quentin; Fernandez, Pierre (24 April 2023). "A Cookbook of Self-Supervised Learning". arXiv:2304.12210 [cs.LG].
Doersch, Carl; Zisserman, Andrew (October 2017). "Multi-task Self-Supervised Visual Learning". 2017 IEEE International Conference on Computer Vision (ICCV). pp. 2070–2079. arXiv:1708.07860. doi:10.1109/ICCV.2017.226. ISBN 978-1-5386-1032-9. S2CID 473729.
Doersch, Carl; Gupta, Abhinav; Efros, Alexei A. (December 2015). "Unsupervised Visual Representation Learning by Context Prediction". 2015 IEEE International Conference on Computer Vision (ICCV). pp. 1422–1430. arXiv:1505.05192. doi:10.1109/ICCV.2015.167. ISBN 978-1-4673-8391-2. S2CID 9062671.
Zheng, Xin; Wang, Yong; Wang, Guoyou; Liu, Jianguo (1 April 2018). "Fast and robust segmentation of white blood cell images by self-supervised learning". Micron. 107: 55–71. doi:10.1016/j.micron.2018.01.010. ISSN 0968-4328. PMID 29425969. S2CID 3796689.
Yarowsky, David (1995). "Unsupervised Word Sense Disambiguation Rivaling Supervised Methods". Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics. Cambridge, MA: Association for Computational Linguistics: 189–196. doi:10.3115/981658.981684. Retrieved 1 November 2022. Unsupervised learning is a framework in machine learning where, in contrast to supervised learning, algorithms learn patterns exclusively from unlabeled data. Other frameworks in the spectrum of supervisions include weak- or semi-supervision, where a small portion of the data is tagged, and self-supervision. Some researchers consider self-supervised learning a form of unsupervised learning.
Conceptually, unsupervised learning divides into the aspects of data, training, algorithm, and downstream applications. Typically, the dataset is harvested cheaply "in the wild", such as massive text corpus obtained by web crawling, with only minor filtering (such as Common Crawl). This compares favorably to supervised learning, where the dataset (such as the ImageNet1000) is typically constructed manually, which is much more expensive.
There were algorithms designed specifically for unsupervised learning, such as clustering algorithms like k-means, dimensionality reduction techniques like principal component analysis (PCA), Boltzmann machine learning, and autoencoders. After the rise of deep learning, most large-scale unsupervised learning have been done by training general-purpose neural network architectures by gradient descent, adapted to performing unsupervised learning by designing an appropriate training procedure.
Sometimes a trained model can be used as-is, but more often they are modified for downstream applications. For example, the generative pretraining method trains a model to generate a textual dataset, before finetuning it for other applications, such as text classification. As another example, autoencoders are trained to good features, which can then be used as a module for other models, such as in a latent diffusion model.
Tasks are often categorized as discriminative (recognition) or generative (imagination). Often but not always, discriminative tasks use supervised methods and generative tasks use unsupervised (see Venn diagram); however, the separation is very hazy. For example, object recognition favors supervised learning but unsupervised learning can also cluster objects into groups. Furthermore, as progress marches onward, some tasks employ both methods, and some tasks swing from one to another. For example, image recognition started off as heavily supervised, but became hybrid by employing unsupervised pre-training, and then moved towards supervision again with the advent of dropout, ReLU, and adaptive learning rates.
A typical generative task is as follows. At each step, a datapoint is sampled from the dataset, and part of the data is removed, and the model must infer the removed part. This is particularly clear for the denoising autoencoders and BERT.
During the learning phase, an unsupervised network tries to mimic the data it's given and uses the error in its mimicked output to correct itself (i.e. correct its weights and biases). Sometimes the error is expressed as a low probability that the erroneous output occurs, or it might be expressed as an unstable high energy state in the network.
In contrast to supervised methods' dominant use of backpropagation, unsupervised learning also employs other methods including: Hopfield learning rule, Boltzmann learning rule, Contrastive Divergence, Wake Sleep, Variational Inference, Maximum Likelihood, Maximum A Posteriori, Gibbs Sampling, and backpropagating reconstruction errors or hidden state reparameterizations. See the table below for more details.
An energy function is a macroscopic measure of a network's activation state. In Boltzmann machines, it plays the role of the Cost function. This analogy with physics is inspired by Ludwig Boltzmann's analysis of a gas' macroscopic energy from the microscopic probabilities of particle motion

, where k is the Boltzmann constant and T is temperature. In the RBM network the relation is
vary over every possible activation pattern and
e^)
. To be more precise,
is an activation pattern of all neurons (visible and hidden). Hence, some early neural networks bear the name Boltzmann Machine. Paul Smolensky calls
the Harmony. A network seeks low energy which is high Harmony.
This table shows connection diagrams of various unsupervised networks, the details of which will be given in the section Comparison of Networks. Circles are neurons and edges between them are connection weights. As network design changes, features are added on to enable new capabilities or removed to make learning faster. For instance, neurons change between deterministic (Hopfield) and stochastic (Boltzmann) to allow robust output, weights are removed within a layer (RBM) to hasten learning, or connections are allowed to become asymmetric (Helmholtz).
Of the networks bearing people's names, only Hopfield worked directly with neural networks. Boltzmann and Helmholtz came before artificial neural networks, but their work in physics and physiology inspired the analytical methods that were used.
Here, we highlight some characteristics of select networks. The details of each are given in the comparison table below.
Ferromagnetism inspired Hopfield networks. A neuron correspond to an iron domain with binary magnetic moments Up and Down, and neural connections correspond to the domain's influence on each other. Symmetric connections enable a global energy formulation. During inference the network updates each state using the standard activation step function. Symmetric weights and the right energy functions guarantees convergence to a stable activation pattern. Asymmetric weights are difficult to analyze. Hopfield nets are used as Content Addressable Memories (CAM).
These are stochastic Hopfield nets. Their state value is sampled from this pdf as follows: suppose a binary neuron fires with the Bernoulli probability p(1) = 1/3 and rests with p(0) = 2/3. One samples from it by taking a uniformly distributed random number y, and plugging it into the inverted cumulative distribution function, which in this case is the step function thresholded at 2/3. The inverse function = .
Introduced by Radford Neal in 1992, this network applies ideas from probabilistic graphical models to neural networks. A key difference is that nodes in graphical models have pre-assigned meanings, whereas Belief Net neurons' features are determined after training. The network is a sparsely connected directed acyclic graph composed of binary stochastic neurons. The learning rule comes from Maximum Likelihood on p(X): Δwij

sj * (si - pi), where pi = 1 / ( 1 + eweighted inputs into neuron i ). sj's are activations from an unbiased sample of the posterior distribution and this is problematic due to the Explaining Away problem raised by Judea Perl. Variational Bayesian methods uses a surrogate posterior and blatantly disregard this complexity.
Introduced by Hinton, this network is a hybrid of RBM and Sigmoid Belief Network. The top 2 layers is an RBM and the second layer downwards form a sigmoid belief network. One trains it by the stacked RBM method and then throw away the recognition weights below the top RBM. As of 2009, 3-4 layers seems to be the optimal depth.
These are early inspirations for the Variational Auto Encoders. Its 2 networks combined into one—forward weights operates recognition and backward weights implements imagination. It is perhaps the first network to do both. Helmholtz did not work in machine learning but he inspired the view of "statistical inference engine whose function is to infer probable causes of sensory input". the stochastic binary neuron outputs a probability that its state is 0 or 1. The data input is normally not considered a layer, but in the Helmholtz machine generation mode, the data layer receives input from the middle layer and has separate weights for this purpose, so it is considered a layer. Hence this network has 3 layers.
These are inspired by Helmholtz machines and combines probability network with neural networks. An Autoencoder is a 3-layer CAM network, where the middle layer is supposed to be some internal representation of input patterns. The encoder neural network is a probability distribution qφ(z given x) and the decoder network is pθ(x given z). The weights are named phi & theta rather than W and V as in Helmholtz—a cosmetic difference. These 2 networks here can be fully connected, or use another NN scheme.
The classical example of unsupervised learning in the study of neural networks is Donald Hebb's principle, that is, neurons that fire together wire together. In Hebbian learning, the connection is reinforced irrespective of an error, but is exclusively a function of the coincidence between action potentials between the two neurons. A similar version that modifies synaptic weights takes into account the time between the action potentials (spike-timing-dependent plasticity or STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such as pattern recognition and experiential learning.
Among neural network models, the self-organizing map (SOM) and adaptive resonance theory (ART) are commonly used in unsupervised learning algorithms. The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties. The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a user-defined constant called the vigilance parameter. ART networks are used for many pattern recognition tasks, such as automatic target recognition and seismic signal processing.
Two of the main methods used in unsupervised learning are principal component and cluster analysis. Cluster analysis is used in unsupervised learning to group, or segment, datasets with shared attributes in order to extrapolate algorithmic relationships. Cluster analysis is a branch of machine learning that groups the data that has not been labelled, classified or categorized. Instead of responding to feedback, cluster analysis identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data. This approach helps detect anomalous data points that do not fit into either group.
A central application of unsupervised learning is in the field of density estimation in statistics, though unsupervised learning encompasses many other domains involving summarizing and explaining data features. It can be contrasted with supervised learning by saying that whereas supervised learning intends to infer a conditional probability distribution conditioned on the label of input data; unsupervised learning intends to infer an a priori probability distribution .
Some of the most common algorithms used in unsupervised learning include: (1) Clustering, (2) Anomaly detection, (3) Approaches for learning latent variable models. Each approach uses several methods as follows:
Clustering methods include: hierarchical clustering, k-means, mixture models, model-based clustering, DBSCAN, and OPTICS algorithm
Anomaly detection methods include: Local Outlier Factor, and Isolation Forest
Approaches for learning latent variable models such as Expectation–maximization algorithm (EM), Method of moments, and Blind signal separation techniques (Principal component analysis, Independent component analysis, Non-negative matrix factorization, Singular value decomposition)
One of the statistical approaches for unsupervised learning is the method of moments. In the method of moments, the unknown parameters (of interest) in the model are related to the moments of one or more random variables, and thus, these unknown parameters can be estimated given the moments. The moments are usually estimated from samples empirically. The basic moments are first and second order moments. For a random vector, the first order moment is the mean vector, and the second order moment is the covariance matrix (when the mean is zero). Higher order moments are usually represented using tensors which are the generalization of matrices to higher orders as multi-dimensional arrays.
In particular, the method of moments is shown to be effective in learning the parameters of latent variable models. Latent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. A highly practical example of latent variable models in machine learning is the topic modeling which is a statistical model for generating the words (observed variables) in the document based on the topic (latent variable) of the document. In the topic modeling, the words in the document are generated according to different statistical parameters when the topic of the document is changed. It is shown that method of moments (tensor decomposition techniques) consistently recover the parameters of a large class of latent variable models under some assumptions.
The Expectation–maximization algorithm (EM) is also one of the most practical methods for learning latent variable models. However, it can get stuck in local optima, and it is not guaranteed that the algorithm will converge to the true unknown parameters of the model. In contrast, for the method of moments, the global convergence is guaranteed under some conditions.
Automated machine learning
Generative topographic map
Meta-learning (computer science)
Radial basis function network Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent should take actions in a dynamic environment in order to maximize a reward signal. Reinforcement learning is one of the three basic machine learning paradigms, alongside supervised learning and unsupervised learning.
Reinforcement learning differs from supervised learning in not needing labelled input-output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead, the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge) with the goal of maximizing the cumulative reward (the feedback of which might be incomplete or delayed). The search for this balance is known as the exploration–exploitation dilemma.
The environment is typically stated in the form of a Markov decision process (MDP), as many reinforcement learning algorithms use dynamic programming techniques. The main difference between classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process, and they target large MDPs where exact methods become infeasible.
Due to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics. In the operations research and control literature, RL is called approximate dynamic programming, or neuro-dynamic programming. The problems of interest in RL have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation (particularly in the absence of a mathematical model of the environment).
Basic reinforcement learning is modeled as a Markov decision process:
A set of environment and agent states (the state space),

A set of actions (the action space),

, of the agent;
(s,s')=\Pr(S_s'\mid S_s,A_a)
, the transition probability (at time
) from state
, the immediate reward after transition from
The purpose of reinforcement learning is for the agent to learn an optimal (or near-optimal) policy that maximizes the reward function or other user-provided reinforcement signal that accumulates from immediate rewards. This is similar to processes that appear to occur in animal psychology. For example, biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements, and interpret pleasure and food intake as positive reinforcements. In some circumstances, animals learn to adopt behaviors that optimize these rewards. This suggests that animals are capable of reinforcement learning.
A basic reinforcement learning agent interacts with its environment in discrete time steps. At each time step t, the agent receives the current state
. It then chooses an action
from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state
and the reward
associated with the transition
is determined. The goal of a reinforcement learning agent is to learn a policy:
&\pi :\times \to [0,1]\\&\pi (s,a)=\Pr(A_a\mid S_s)\end
that maximizes the expected cumulative reward.
Formulating the problem as a Markov decision process assumes the agent directly observes the current environmental state; in this case, the problem is said to have full observability. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to have partial observability, and formally the problem must be formulated as a partially observable Markov decision process. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed.
When the agent's performance is compared to that of an agent that acts optimally, the difference in performance yields the notion of regret. In order to act near optimally, the agent must reason about long-term consequences of its actions (i.e., maximize future rewards), although the immediate reward associated with this might be negative.
Thus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including energy storage, robot control, photovoltaic generators, backgammon, checkers, Go (AlphaGo), and autonomous driving systems.
Two elements make reinforcement learning powerful: the use of samples to optimize performance, and the use of function approximation to deal with large environments. Thanks to these two key components, RL can be used in large environments in the following situations:
A model of the environment is known, but an analytic solution is not available;
Only a simulation model of the environment is given (the subject of simulation-based optimization);
The only way to collect information about the environment is to interact with it.
The first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems.
The exploration vs. exploitation trade-off has been most thoroughly studied through the multi-armed bandit problem and for finite state space Markov decision processes in Burnetas and Katehakis (1997).
Reinforcement learning requires clever exploration mechanisms; randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite Markov decision processes is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical.
One such method is


is a parameter controlling the amount of exploration vs. exploitation. With probability

, exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability

, exploration is chosen, and the action is chosen uniformly at random.

is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.
Even if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards.
The agent's action selection is modeled as a map called policy:
&\pi :\times \to [0,1]\\&\pi (a,s)=\Pr(A_a\mid S_s)\end
The policy map gives the probability of taking action
when in state
. There are also deterministic policies


denotes the action that should be played at state
The state-value function
(s)
is defined as, expected discounted return starting with state
, and successively following policy

. Hence, roughly speaking, the value function estimates "how good" it is to be in a given state.
(s)=\operatorname   [G\mid S_s]=\operatorname   \left[\sum _^\gamma ^R_\mid S_s\right],
where the random variable
denotes the discounted return, and is defined as the sum of future discounted rewards:
^\gamma ^R_=R_+\gamma R_+\gamma ^R_+\cdots ,
is the reward for transitioning from state

is the discount rate.

is less than 1, so rewards in the distant future are weighted less than rewards in the immediate future.
The algorithm must find a policy with maximum expected discounted return. From the theory of Markov decision processes it is known that, without loss of generality, the search can be restricted to the set of so-called stationary policies. A policy is stationary if the action-distribution returned by it depends only on the last state visited (from the observation agent's history). The search can be further restricted to deterministic stationary policies. A deterministic stationary policy deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality.
The brute force approach entails two steps:
For each possible policy, sample returns while following it
Choose the policy with the largest expected discounted return
One problem with this is that the number of policies can be large, or even infinite. Another is that the variance of the returns may be large, which requires many samples to accurately estimate the discounted return of each policy.
These problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others. The two main approaches for achieving this are value function estimation and direct policy search.
Value function approaches attempt to find a policy that maximizes the discounted return by maintaining a set of estimates of expected discounted returns
  [G]
for some policy (usually either the "current" [on-policy] or the optimal [off-policy] one).
These methods rely on the theory of Markov decision processes, where optimality is defined in a sense stronger than the one above: A policy is optimal if it achieves the best-expected discounted return from any initial state (i.e., initial distributions play no role in this definition). Again, an optimal policy can always be found among stationary policies.
To define optimality in a formal manner, define the state-value of a policy

(s)=\operatorname   [G\mid s,\pi ],
stands for the discounted return associated with following

from the initial state
as the maximum possible state-value of
(s)

is allowed to change,
(s)=\max _V^(s).
A policy that achieves these optimal state-values in each state is called optimal. Clearly, a policy that is optimal in this sense is also optimal in the sense that it maximizes the expected discounted return, since
(s)=\max _\mathbb  [G\mid s,\pi ]
is a state randomly sampled from the distribution

of initial states (so
=s)
Although state-values suffice to define optimality, it is useful to define action-values. Given a state
, an action
and a policy

, the action-value of the pair

is defined by
(s,a)=\operatorname   [G\mid s,a,\pi ],
now stands for the random discounted return associated with first taking action

The theory of Markov decision processes states that if

is an optimal policy, we act optimally (take the optimal action) by choosing the action from
(s,\cdot )
with the highest action-value at each state,
. The action-value function of such an optimal policy (

) is called the optimal action-value function and is commonly denoted by
. In summary, the knowledge of the optimal action-value function alone suffices to know how to act optimally.
Assuming full knowledge of the Markov decision process, the two basic approaches to compute the optimal action-value function are value iteration and policy iteration. Both algorithms compute a sequence of functions

) that converge to
. Computing these functions involves computing expectations over the whole state-space, which is impractical for all but the smallest (finite) Markov decision processes. In reinforcement learning methods, expectations are approximated by averaging over samples and using function approximation techniques to cope with the need to represent value functions over large state-action spaces.
Monte Carlo methods are used to solve reinforcement learning problems by averaging sample returns. Unlike methods that require full knowledge of the environment's dynamics, Monte Carlo methods rely solely on actual or simulated experience—sequences of states, actions, and rewards obtained from interaction with an environment. This makes them applicable in situations where the complete dynamics are unknown. Learning from actual experience does not require prior knowledge of the environment and can still lead to optimal behavior. When using simulated experience, only a model capable of generating sample transitions is required, rather than a full specification of transition probabilities, which is necessary for dynamic programming methods.
Monte Carlo methods apply to episodic tasks, where experience is divided into episodes that eventually terminate. Policy and value function updates occur only after the completion of an episode, making these methods incremental on an episode-by-episode basis, though not on a step-by-step (online) basis. The term "Monte Carlo" generally refers to any method involving random sampling; however, in this context, it specifically refers to methods that compute averages from complete returns, rather than partial returns.
These methods function similarly to the bandit algorithms, in which returns are averaged for each state-action pair. The key difference is that actions taken in one state affect the returns of subsequent states within the same episode, making the problem non-stationary. To address this non-stationarity, Monte Carlo methods use the framework of general policy iteration (GPI). While dynamic programming computes value functions using full knowledge of the Markov decision process (MDP), Monte Carlo methods learn these functions through sample returns. The value functions and policies interact similarly to dynamic programming to achieve optimality, first addressing the prediction problem and then extending to policy improvement and control, all based on sampled experience.
The first problem is corrected by allowing the procedure to change the policy (at some or all states) before the values settle. This too may be problematic as it might prevent convergence. Most current algorithms do this, giving rise to the class of generalized policy iteration algorithms. Many actor-critic methods belong to this category.
The second issue can be corrected by allowing trajectories to contribute to any state-action pair in them. This may also help to some extent with the third problem, although a better solution when returns have high variance is Sutton's temporal difference (TD) methods that are based on the recursive Bellman equation. The computation in TD methods can be incremental (when after each transition the memory is changed and the transition is thrown away), or batch (when the transitions are batched and the estimates are computed once based on the batch). Batch methods, such as the least-squares temporal difference method, may use the information in the samples better, while incremental methods are the only choice when batch methods are infeasible due to their high computational or memory complexity. Some methods try to combine the two approaches. Methods based on temporal differences also overcome the fourth issue.
Another problem specific to TD comes from their reliance on the recursive Bellman equation. Most TD methods have a so-called


that can continuously interpolate between Monte Carlo methods that do not rely on the Bellman equations and the basic TD methods that rely entirely on the Bellman equations. This can be effective in palliating this issue.
In order to address the fifth issue, function approximation methods are used. Linear function approximation starts with a mapping

that assigns a finite-dimensional vector to each state-action pair. Then, the action values of a state-action pair
are obtained by linearly combining the components of

with some weights

^\theta _\phi _(s,a).
The algorithms then adjust the weights, instead of adjusting the values associated with the individual state-action pairs. Methods based on ideas from nonparametric statistics (which can be seen to construct their own features) have been explored.
Value iteration can also be used as a starting point, giving rise to the Q-learning algorithm and its many variants. Including Deep Q-learning methods when a neural network is used to represent Q, with various applications in stochastic search problems.
The problem with using action-values is that they may need highly precise estimates of the competing action values that can be hard to obtain when the returns are noisy, though this problem is mitigated to some extent by temporal difference methods. Using the so-called compatible function approximation method compromises generality and efficiency.
An alternative method is to search directly in (some subset of) the policy space, in which case the problem becomes a case of stochastic optimization. The two approaches available are gradient-based and gradient-free methods.
Gradient-based methods (policy gradient methods) start with a mapping from a finite-dimensional (parameter) space to the space of policies: given the parameter vector


denote the policy associated to

. Defining the performance function by

under mild conditions this function will be differentiable as a function of the parameter vector

. If the gradient of

was known, one could use gradient ascent. Since an analytic expression for the gradient is not available, only a noisy estimate is available. Such an estimate can be constructed in many ways, giving rise to algorithms such as Williams's REINFORCE method (which is known as the likelihood ratio method in the simulation-based optimization literature).
A large class of methods avoids relying on gradient information. These include simulated annealing, cross-entropy search or methods of evolutionary computation. Many gradient-free methods can achieve (in theory and in the limit) a global optimum.
Policy search methods may converge slowly given noisy data. For example, this happens in episodic problems when the trajectories are long and the variance of the returns is large. Value-function based methods that rely on temporal differences might help in this case. In recent years, actor–critic methods have been proposed and performed well on various problems.
Policy search methods have been used in the robotics context. Many policy search methods may get stuck in local optima (as they are based on local search).
Finally, all of the above methods can be combined with algorithms that first learn a model of the Markov decision process, the probability of each next state given an action taken from an existing state. For instance, the Dyna algorithm learns a model from experience, and uses that to provide more modelled transitions for a value function, in addition to the real transitions. Such methods can sometimes be extended to use of non-parametric models, such as when the transitions are simply stored and "replayed" to the learning algorithm.
Model-based methods can be more computationally intensive than model-free approaches, and their utility can be limited by the extent to which the Markov decision process can be learnt.
There are other ways to use models than to update a value function. For instance, in model predictive control the model is used to update the behavior directly.
Both the asymptotic and finite-sample behaviors of most algorithms are well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.
Efficient exploration of Markov decision processes is given in Burnetas and Katehakis (1997). Finite-time performance bounds have also appeared for many algorithms, but these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.
For incremental algorithms, asymptotic convergence issues have been settled. Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).
Research topics include:
adaptive methods that work with fewer (or no) parameters under a large number of conditions
bug detection in software projects
combinations with logic-based frameworks
exploration in large Markov decision processes
entity-based reinforcement learning
interaction between implicit and explicit learning in skill acquisition
intrinsic motivation which differentiates information-seeking, curiosity-type behaviours from task-dependent goal-directed behaviours large-scale empirical evaluations
large (or continuous) action spaces
modular and hierarchical reinforcement learning
multiagent/distributed reinforcement learning is a topic of interest. Applications are expanding.
optimization of computing resources
partial information (e.g., using predictive state representation)
reward function based on maximising novel information
sample-based planning (e.g., based on Monte Carlo tree search).
TD learning modeling dopamine-based learning in the brain. Dopaminergic projections from the substantia nigra to the basal ganglia function are the prediction error.
value-function and policy search methods
The following table lists the key algorithms for learning a policy depending on several criteria:
The algorithm can be on-policy (it performs policy updates using trajectories sampled via the current policy) or off-policy.
The action space may be discrete (e.g. the action space could be "going up", "going left", "going right", "going down", "stay") or continuous (e.g. moving the arm with a given angle).
The state space may be discrete (e.g. the agent could be in a cell in a grid) or continuous (e.g. the agent could be located at a given position in the plane).
Associative reinforcement learning tasks combine facets of stochastic learning automata tasks and supervised learning pattern classification tasks. In associative reinforcement learning tasks, the learning system interacts in a closed loop with its environment.
This approach extends reinforcement learning by using a deep neural network and without explicitly designing the state space. The work on learning ATARI games by Google DeepMind increased attention to deep reinforcement learning or end-to-end reinforcement learning.
Adversarial deep reinforcement learning is an active area of research in reinforcement learning focusing on vulnerabilities of learned policies. In this research area some studies initially showed that reinforcement learning policies are susceptible to imperceptible adversarial manipulations. While some methods have been proposed to overcome these susceptibilities, in the most recent studies it has been shown that these proposed solutions are far from providing an accurate representation of current vulnerabilities of deep reinforcement learning policies.
By introducing fuzzy inference in reinforcement learning, approximating the state-action value function with fuzzy rules in continuous space becomes possible. The IF - THEN form of fuzzy rules make this approach suitable for expressing the results in a form close to natural language. Extending FRL with Fuzzy Rule Interpolation allows the use of reduced size sparse fuzzy rule-bases to emphasize cardinal rules (most important state-action values).
In inverse reinforcement learning (IRL), no reward function is given. Instead, the reward function is inferred given an observed behavior from an expert. The idea is to mimic observed behavior, which is often optimal or close to optimal. One popular IRL paradigm is named maximum entropy inverse reinforcement learning (MaxEnt IRL). MaxEnt IRL estimates the parameters of a linear model of the reward function by maximizing the entropy of the probability distribution of observed trajectories subject to constraints related to matching expected feature counts. Recently it has been shown that MaxEnt IRL is a particular case of a more general framework named random utility inverse reinforcement learning (RU-IRL). RU-IRL is based on random utility theory and Markov decision processes. While prior IRL approaches assume that the apparent random behavior of an observed agent is due to it following a random policy, RU-IRL assumes that the observed agent follows a deterministic policy but randomness in observed behavior is due to the fact that an observer only has partial access to the features the observed agent uses in decision making. The utility function is modeled as a random variable to account for the ignorance of the observer regarding the features the observed agent actually considers in its utility function.
Multi-objective reinforcement learning (MORL) is a form of reinforcement learning concerned with conflicting alternatives. It is distinct from multi-objective optimization in that it is concerned with agents acting in environments.
Safe reinforcement learning (SRL) can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes. An alternative approach is risk-averse reinforcement learning, where instead of the expected return, a risk-measure of the return is optimized, such as the conditional value at risk (CVaR). In addition to mitigating risk, the CVaR objective increases robustness to model uncertainties. However, CVaR optimization in risk-averse RL requires special care, to prevent gradient bias and blindness to success.
Self-reinforcement learning (or self-learning), is a learning paradigm which does not use the concept of immediate reward
after transition from
. It does not use an external reinforcement, it only uses the agent internal self-reinforcement. The internal self-reinforcement is provided by mechanism of feelings and emotions. In the learning process emotions are backpropagated by a mechanism of secondary reinforcement. The learning equation does not include the immediate reward, it only includes the state evaluation.
The self-reinforcement algorithm updates a memory matrix
such that in each iteration executes the following machine learning routine:
Receive a consequence situation
Compute state evaluation
of how good is to be in the consequence situation
Update crossbar memory
Initial conditions of the memory are received as input from the genetic environment. It is a system with only one input (situation), and only one output (action, or behavior).
Self-reinforcement (self-learning) was introduced in 1982 along with a neural network capable of self-reinforcement learning, named Crossbar Adaptive Array (CAA). The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence states. The system is driven by the interaction between cognition and emotion.
In recent years, Reinforcement learning has become a significant concept in Natural Language Processing (NLP), where tasks are often sequential decision-making rather than static classification. Reinforcement learning is where an agent take actions in an environment to maximize the accumulation of rewards. This framework is best fit for many NLP tasks, including dialogue generation, text summarization, and machine translation, where the quality of the output depends on optimizing long-term or human-centered goals rather than the prediction of single correct label.
Early application of RL in NLP emerged in dialogue systems, where conversation was determined as a series of actions optimized for fluency and coherence. These early attempts, including policy gradient and sequence-level training techniques, laid a foundation for the broader application of reinforcement learning to other areas of NLP.
A major breakthrough happened with the introduction of Reinforcement Learning from Human Feedback (RLHF), a method in which human feedbacks are used to train a reward model that guides the RL agent. Unlike traditional rule-based or supervised systems, RLHF allows models to align their behavior with human judgments on complex and subjective tasks. This technique was initially used in the development of InstructGPT, an effective language model trained to follow human instructions and later in ChatGPT which incorporates RLHF for improving output responses and ensuring safety.
More recently, researchers have explored the use of offline RL in NLP to improve dialogue systems without the need of live human interaction. These methods optimize for user engagement, coherence, and diversity based on past conversation logs and pre-trained reward models.
Efficient comparison of RL algorithms is essential for research, deployment and monitoring of RL systems. To compare different algorithms on a given environment, an agent can be trained for each algorithm. Since the performance is sensitive to implementation details, all algorithms should be implemented as closely as possible to each other. After the training is finished, the agents can be run on a sample of test episodes, and their scores (returns) can be compared. Since episodes are typically assumed to be i.i.d, standard statistical tools can be used for hypothesis testing, such as T-test and permutation test. This requires to accumulate all the rewards within an episode into a single number—the episodic return. However, this causes a loss of information, as different time-steps are averaged together, possibly with different levels of noise. Whenever the noise level varies across the episode, the statistical power can be improved significantly, by weighting the rewards according to their estimated noise.
Despite significant advancements, reinforcement learning (RL) continues to face several challenges and limitations that hinder its widespread application in real-world scenarios.
RL algorithms often require a large number of interactions with the environment to learn effective policies, leading to high computational costs and time-intensive to train the agent. For instance, OpenAI's Dota-playing bot utilized thousands of years of simulated gameplay to achieve human-level performance. Techniques like experience replay and curriculum learning have been proposed to deprive sample inefficiency, but these techniques add more complexity and are not always sufficient for real-world applications.
Training RL models, particularly for deep neural network-based models, can be unstable and prone to divergence. A small change in the policy or environment can lead to extreme fluctuations in performance, making it difficult to achieve consistent results. This instability is further enhanced in the case of the continuous or high-dimensional action space, where the learning step becomes more complex and less predictable.
The RL agents trained in specific environments often struggle to generalize their learned policies to new, unseen scenarios. This is the major setback preventing the application of RL to dynamic real-world environments where adaptability is crucial. The challenge is to develop such algorithms that can transfer knowledge across tasks and environments without extensive retraining.
Designing appropriate reward functions is critical in RL because poorly designed reward functions can lead to unintended behaviors. In addition, RL systems trained on biased data may perpetuate existing biases and lead to discriminatory or unfair outcomes. Both of these issues requires careful consideration of reward structures and data sources to ensure fairness and desired behaviors.
Annaswamy, Anuradha M. (3 May 2023). "Adaptive Control and Intersections with Reinforcement Learning". Annual Review of Control, Robotics, and Autonomous Systems. 6 (1): 65–93. doi:10.1146/annurev-control-062922-090153. ISSN 2573-5144. S2CID 255702873.
Auer, Peter; Jaksch, Thomas; Ortner, Ronald (2010). "Near-optimal regret bounds for reinforcement learning". Journal of Machine Learning Research. 11: 1563–1600.
Bertsekas, Dimitri P. (2023) [2019]. REINFORCEMENT LEARNING AND OPTIMAL CONTROL (1st ed.). Athena Scientific. ISBN 978-1-886-52939-7.
Busoniu, Lucian; Babuska, Robert; De Schutter, Bart; Ernst, Damien (2010). Reinforcement Learning and Dynamic Programming using Function Approximators. Taylor & Francis CRC Press. ISBN 978-1-4398-2108-4.
François-Lavet, Vincent; Henderson, Peter; Islam, Riashat; Bellemare, Marc G.; Pineau, Joelle (2018). "An Introduction to Deep Reinforcement Learning". Foundations and Trends in Machine Learning. 11 (3–4): 219–354. arXiv:1811.12560. Bibcode:2018arXiv181112560F. doi:10.1561/2200000071. S2CID 54434537.
Li, Shengbo Eben (2023). Reinforcement Learning for Sequential Decision and Optimal Control (1st ed.). Springer Verlag, Singapore. doi:10.1007/978-981-19-7784-8. ISBN 978-9-811-97783-1.
Powell, Warren (2011). Approximate dynamic programming: solving the curses of dimensionality. Wiley-Interscience. Archived from the original on 2016-07-31. Retrieved 2010-09-08.
Sutton, Richard S. (1988). "Learning to predict by the method of temporal differences". Machine Learning. 3: 9–44. doi:10.1007/BF00115009.
Sutton, Richard S.; Barto, Andrew G. (2018) [1998]. Reinforcement Learning: An Introduction (2nd ed.). MIT Press. ISBN 978-0-262-03924-6.
Szita, Istvan; Szepesvari, Csaba (2010). "Model-based Reinforcement Learning with Nearly Tight Exploration Complexity Bounds" (PDF). ICML 2010. Omnipress. pp. 1031–1038. Archived from the original (PDF) on 2010-07-14.
Dissecting Reinforcement Learning Series of blog post on reinforcement learning with Python code
A (Long) Peek into Reinforcement Learning Deep reinforcement learning (DRL) is a subfield of machine learning that combines principles of reinforcement learning (RL) and deep learning. It involves training agents to make decisions by interacting with an environment to maximize cumulative rewards, while using deep neural networks to represent policies, value functions, or environment models. This integration enables DRL systems to process high-dimensional inputs, such as images or continuous control signals, making the approach effective for solving complex tasks. Since the introduction of the deep Q-network (DQN) in 2015, DRL has achieved significant successes across domains including games, robotics, and autonomous systems, and is increasingly applied in areas such as healthcare, finance, and autonomous vehicles.
Deep reinforcement learning (DRL) is part of machine learning, which combines reinforcement learning (RL) and deep learning. In DRL, agents learn how decisions are to be made by interacting with environments in order to maximize cumulative rewards, while using deep neural networks to represent policies, value functions, or models of the environment. This integration enables agents to handle high-dimensional input spaces, such as raw images or continuous control signals, making DRL a widely used approach for addressing complex tasks.
Since the development of the deep Q-network (DQN) in 2015, DRL has led to major breakthroughs in domains such as games, robotics, and autonomous systems. Research in DRL continues to expand rapidly, with active work on challenges like sample efficiency and robustness, as well as innovations in model-based methods, transformer architectures, and open-ended learning. Applications now range from healthcare and finance to language systems and autonomous vehicles.
Reinforcement learning (RL) is a framework in which agents interact with environments by taking actions and learning from feedback in form of rewards or penalties. Traditional RL methods, such as Q-learning and policy gradient techniques, rely on tabular representations or linear approximations, which are often not scalable to high-dimensional or continuous input spaces.
DRL came out as solution to above limitation by integrating RL and deep neural networks. This combination enables agents to approximate complex functions and handle unstructured input data like raw images, sensor data, or natural language. The approach became widely recognized following the success of DeepMind's deep Q-network (DQN), which achieved human-level performance on several Atari video games using only pixel inputs and game scores as feedback.
Since then, DRL has evolved to include various architectures and learning strategies, including model-based methods, actor-critic frameworks, and applications in continuous control environments. These developments have significantly expanded the applicability of DRL across domains where traditional RL was limited.
Several algorithmic approaches form the foundation of deep reinforcement learning, each with different strategies for learning optimal behavior.
One of the earliest and most influential DRL algorithms is the Deep Q-Network (DQN), which combines Q-learning with deep neural networks. DQN approximates the optimal action-value function using a convolutional neural network and introduced techniques such as experience replay and target networks which stabilize training.
Policy gradient methods directly optimize the agent’s policy by adjusting parameters in the direction that increases expected rewards. These methods are well-suited to high-dimensional or continuous action spaces and form the basis of many modern DRL algorithms.
Actor-critic algorithms combine the advantages of value-based and policy-based methods. The actor updates the policy, while the critic evaluates the current policy using a value function. Popular variants include A2C (Advantage Actor-Critic) and PPO (Proximal Policy Optimization), both of which are widely used in benchmarks and real-world applications.
Other methods include multi-agent reinforcement learning, hierarchical RL, and approaches that integrate planning or memory mechanisms, depending on the complexity of the task and environment.
DRL has been applied to wide range of domains that require sequential decision-making and the ability to learn from high-dimensional input data.
One of the most well-known applications is in games, where DRL agents have demonstrated performance comparable to or exceeding human-level benchmarks. DeepMind's AlphaGo and AlphaStar, as well as OpenAI Five, are notable examples of DRL systems mastering complex games such as Go, StarCraft II, and Dota 2. While these systems have demonstrated high performance in constrained environments, their success often depends on extensive computational resources and may not generalize easily to tasks outside their training domains.
In robotics, DRL has been used to train agents for tasks such as locomotion, manipulation, and navigation in both simulated and real-world environments. By learning directly from sensory input, DRL enables robots to adapt to complex dynamics without relying on hand-crafted control rules.
Other growing areas of application include finance (e.g., portfolio optimization), healthcare (e.g., treatment planning and medical decision-making), natural language processing (e.g., dialogue systems), and autonomous vehicles (e.g., path planning and control).All of these applications shows how DRL deals with real-world problems like uncertainty, sequential reasoning, and high-dimensional data.
DRL has several significant challenges which limit its broader deployment.
One of the most prominent issues is sample inefficiency. DRL algorithms often require millions of interactions with the environment to learn effective policies, which is impractical in many real-world settings where data collection is expensive or time-consuming.
Another challenge is sparse or delayed reward problem, where feedback signals are infrequent, which makes it difficult for agents to attribute outcomes to specific decisions. Techniques such as reward shaping and exploration strategies have been developed to address this issue.
DRL systems also tend to be sensitive to hyperparameters and lack robustness across tasks or environments. Models that are trained in simulation fail very often when deployed in the real world due to discrepancies between simulated and real-world dynamics, a problem known as the "reality gap."Bias and fairness in DRL systems have also emerged as concerns, particularly in domains like healthcare and finance where imbalanced data can lead to unequal outcomes for underrepresented groups.
Additionally, concerns about safety, interpretability, and reproducibility have become increasingly important, especially in high-stakes domains such as healthcare or autonomous driving. These issues remain active areas of research in the DRL community.
Recent developments in DRL have introduced new architectures and training strategies which aims to improving performance, efficiency, and generalization.
One key area of progress is model-based reinforcement learning, where agents learn an internal model of the environment to simulate outcomes before acting. This kind of approach improves sample efficiency and planning. An example is the Dreamer algorithm, which learns a latent space model to train agents more efficiently in complex environments.
Another major innovation is the use of transformer-based architectures in DRL. Unlike traditional models that rely on recurrent or convolutional networks, transformers can model long-term dependencies more effectively. The Decision Transformer and other similar models treat RL as a sequence modeling problem, enabling agents to generalize better across tasks.
In addition, research into open-ended learning has led to the creation of capable agents that are able to solve a range of tasks without task-specific tuning. Similar systems like the ones that are developed by OpenAI show that agents trained in diverse, evolving environments can generalize across new challenges, moving toward more adaptive and flexible intelligence.
As deep reinforcement learning continues to evolve, researchers are exploring ways to make algorithms more efficient, robust, and generalizable across a wide range of tasks. Improving sample efficiency through model-based learning, enhancing generalization with open-ended training environments, and integrating foundation models are among the current research goals.
Similar area of interest is safe and ethical deployment, particularly in high-risk settings like healthcare, autonomous driving, and finance. Researchers are developing frameworks for safer exploration, interpretability, and better alignment with human values. Ensuring that DRL systems promote equitable outcomes remains an ongoing challenge, especially where historical data may under‑represent marginalized populations.
The future of DRL may also involve more integration with other subfields of machine learning, such as unsupervised learning, transfer learning, and large language models, enabling agents that can learn from diverse data modalities and interact more naturally with human users. In machine learning, reinforcement learning from human feedback (RLHF) is a technique to align an intelligent agent with human preferences. It involves training a reward model to represent preferences, which can then be used to train other models through reinforcement learning.
In classical reinforcement learning, an intelligent agent's goal is to learn a function that guides its behavior, called a policy. This function is iteratively updated to maximize rewards based on the agent's task performance. However, explicitly defining a reward function that accurately approximates human preferences is challenging. Therefore, RLHF seeks to train a "reward model" directly from human feedback. The reward model is first trained in a supervised manner to predict if a response to a given prompt is good (high reward) or bad (low reward) based on ranking data collected from human annotators. This model then serves as a reward function to improve an agent's policy through an optimization algorithm like proximal policy optimization.
RLHF has applications in various domains in machine learning, including natural language processing tasks such as text summarization and conversational agents, computer vision tasks like text-to-image models, and the development of video game bots. While RLHF is an effective method of training models to act better in accordance with human preferences, it also faces challenges due to the way the human preference data is collected. Though RLHF does not require massive amounts of data to improve performance, sourcing high-quality preference data is still an expensive process. Furthermore, if the data is not carefully collected from a representative sample, the resulting model may exhibit unwanted biases.
Optimizing a model based on human feedback is desirable when a task is difficult to specify yet easy to judge. For example, one may want to train a model to generate safe text that is both helpful and harmless (such as lacking bias, toxicity, or otherwise harmful content). Asking humans to manually create examples of harmless and harmful text would be difficult and time-consuming. However, humans are adept at swiftly assessing and comparing the harmfulness of different AI-generated text. Therefore, a more practical objective would be to allow the model to use this type of human feedback to improve its text generation.
Despite the clear benefits of incorporating human feedback in training models, prior efforts—including some that leverage reinforcement learning—have encountered significant challenges. Most attempts were either narrow and difficult to generalize, breaking down on more complex tasks, or they faced difficulties learning from sparse (lacking specific information and relating to large amounts of text at a time) or noisy (inconsistently rewarding similar outputs) reward functions.
RLHF was not the first successful method of using human feedback for reinforcement learning, but it is one of the most widely used. The foundation for RLHF was introduced as an attempt to create a general algorithm for learning from a practical amount of human feedback. The algorithm as used today was introduced by OpenAI in a paper on enhancing text continuation or summarization based on human feedback, and it began to gain popularity when the same method was reused in their paper on InstructGPT. RLHF has also been shown to improve the robustness of RL agents and their capacity for exploration, which results in an optimization process more adept at handling uncertainty and efficiently exploring its environment in search of the highest reward.
Human feedback is commonly collected by prompting humans to rank instances of the agent's behavior. These rankings can then be used to score outputs, for example, using the Elo rating system, which is an algorithm for calculating the relative skill levels of players in a game based only on the outcome of each game. While ranking outputs is the most widely adopted form of feedback, recent research has explored other forms, such as numerical feedback, natural language feedback, and prompting for direct edits to the model's output.
One initial motivation of RLHF was that it requires relatively small amounts of comparison data to be effective. It has been shown that a small amount of data can lead to comparable results to a larger amount. In addition, increasing the amount of data tends to be less effective than proportionally increasing the size of the reward model. Nevertheless, a larger and more diverse amount of data can be crucial for tasks where it is important to avoid bias from a partially representative group of annotators.
When learning from human feedback through pairwise comparison under the Bradley–Terry–Luce model (or the Plackett–Luce model for K-wise comparisons over more than two comparisons), the maximum likelihood estimator (MLE) for linear reward functions has been shown to converge if the comparison data is generated under a well-specified linear model. This implies that, under certain conditions, if a model is trained to decide which choices people would prefer between pairs (or groups) of choices, it will necessarily improve at predicting future preferences. This improvement is expected as long as the comparisons it learns from are based on a consistent and simple rule.
Both offline data collection models, where the model is learning by interacting with a static dataset and updating its policy in batches, as well as online data collection models, where the model directly interacts with the dynamic environment and updates its policy immediately, have been mathematically studied proving sample complexity bounds for RLHF under different feedback models.
In the offline data collection model, when the objective is policy training, a pessimistic MLE that incorporates a lower confidence bound as the reward estimate is most effective. Moreover, when applicable, it has been shown that considering K-wise comparisons directly is asymptotically more efficient than converting them into pairwise comparisons for prediction purposes.
In the online scenario, when human feedback is collected through pairwise comparisons under the Bradley–Terry–Luce model and the objective is to minimize the algorithm's regret (the difference in performance compared to an optimal agent), it has been shown that an optimistic MLE that incorporates an upper confidence bound as the reward estimate can be used to design sample efficient algorithms (meaning that they require relatively little training data). A key challenge in RLHF when learning from pairwise (or dueling) comparisons is associated with the non-Markovian nature of its optimal policies. Unlike simpler scenarios where the optimal strategy does not require memory of past actions, in RLHF, the best course of action often depends on previous events and decisions, making the strategy inherently memory-dependent.
RLHF has been applied to various domains of natural language processing (NLP), such as conversational agents, text summarization, and natural language understanding. Ordinary reinforcement learning, in which agents learn from their actions based on a predefined "reward function", is difficult to apply to NLP tasks because the rewards tend to be difficult to define or measure, especially when dealing with complex tasks that involve human values or preferences. RLHF can steer NLP models, in particular language models, to provide answers that align with human preferences with regard to such tasks by capturing their preferences beforehand in the reward model. This results in a model capable of generating more relevant responses and rejecting inappropriate or irrelevant queries. Some notable examples of RLHF-trained language models are OpenAI's ChatGPT (and its predecessor InstructGPT), DeepMind's Sparrow, Google's Gemini, and Anthropic's Claude.
In computer vision, RLHF has also been used to align text-to-image models. Studies that successfully used RLHF for this goal have noted that the use of KL regularization in RLHF, which aims to prevent the learned policy from straying too far from the unaligned model, helped to stabilize the training process by reducing overfitting to the reward model. The final image outputs from models trained with KL regularization were noted to be of significantly higher quality than those trained without. Other methods tried to incorporate the feedback through more direct training—based on maximizing the reward without the use of reinforcement learning—but conceded that an RLHF-based approach would likely perform better due to the online sample generation used in RLHF during updates as well as the aforementioned KL regularization over the prior model, which mitigates overfitting to the reward function.
RLHF was initially applied to other areas, such as the development of video game bots and tasks in simulated robotics. For example, OpenAI and DeepMind trained agents to play Atari games based on human preferences. In classical RL-based training of such bots, the reward function is simply correlated to how well the agent is performing in the game, usually using metrics like the in-game score. In comparison, in RLHF, a human is periodically presented with two clips of the agent's behavior in the game and must decide which one looks better. This approach can teach agents to perform at a competitive level without ever having access to their score. In fact, it was shown that RLHF can sometimes lead to superior performance over RL with score metrics because the human's preferences can contain more useful information than performance-based metrics. The agents achieved strong performance in many of the environments tested, often surpassing human performance.
In RLHF, two different models are trained: a reward model and a reinforcement learning (RL) policy. The reward model learns to determine what behavior is desirable based on human feedback, while the policy is guided by the reward model to determine the agent's actions. Both models are commonly initialized using a pre-trained autoregressive language model. This model is then customarily trained in a supervised manner on a relatively small dataset of pairs of prompts to an assistant and their accompanying responses, written by human annotators.
The reward model is usually initialized with a pre-trained model, as this initializes it with an understanding of language and focuses training explicitly on learning human preferences. In addition to being used to initialize the reward model and the RL policy, the model is then also used to sample data to be compared by annotators.
The reward model is then trained by replacing the final layer of the previous model with a randomly initialized regression head. This change shifts the model from its original classification task over its vocabulary to simply outputting a number corresponding to the score of any given prompt and response. This model is trained on the human preference comparison data collected earlier from the supervised model. In particular, it is trained to minimize the following cross-entropy loss function:
(\theta )=-E_,y_)[\log(\sigma (r_(x,y_)-r_(x,y_)))]=-E_,y_)\log \left[(x,y_)(x,y_)+e^(x,y_)\right]
is the number of responses the labelers ranked,
(x,y)
is the output of the reward model for prompt
is the preferred completion over

denotes the sigmoid function, and
denotes the expected value. This can be thought of as a form of logistic regression, where the model predicts the probability that a response
is preferred over
This loss function essentially measures the difference between the reward model's predictions and the decisions made by humans. The goal is to make the model's guesses as close as possible to the humans' preferences by minimizing the difference measured by this equation. In the case of only pairwise comparisons,
, so the factor of
=1
. In general, all

comparisons from each prompt are used for training as a single batch.
After training, the outputs of the model are normalized such that the reference completions have a mean score of 0. That is,
r_(x,y)=0
for each query and reference pair
by calculating the mean reward across the training dataset and setting it as the bias in the reward head.
Similarly to the reward model, the human feedback policy is also initialized from a pre-trained model.
The key is to understand language generation as if it is a game to be learned by RL. In RL, a policy is a function that maps a game state to a game action. In RLHF, the "game" is the game of replying to prompts. A prompt is a game state, and a response is a game action. This is a fairly trivial kind of game, since every game lasts for exactly one step. Nevertheless, it is a game, and so RL algorithms can be applied to it.
The first step in its training is supervised fine-tuning (SFT). This step does not require the reward model. Instead, the pre-trained model is trained on a dataset
that contains prompt-response pairs
. Then, during SFT, the model is trained to auto-regressively generate the corresponding response
when given a random prompt
. The original paper recommends to SFT for only one epoch, since more than that causes overfitting.
is usually written by human contractors, who write both the prompts and responses.
The second step uses a policy gradient method to the reward model. It uses a dataset
, which contains prompts, but not responses. Like most policy gradient methods, this algorithm has an outer loop and two inner loops:
Initialize the policy
^

, the policy output from SFT.
Loop for many steps.
Initialize a new empty dataset
^
Loop for many steps
Sample a random prompt
Generate a response
from the policy
^
Calculate the reward signal
(x,y)
from the reward model

Add the triple
(x,y))
^

by a policy gradient method to increase the objective function
(\phi )=E_^\left[r_(x,y)-\beta \log \left(^(y|x)(y|x)\right)\right]
^
is equivalent to
,y\sim \pi _^(\cdot |x)
, which means "sample a prompt from
, then sample a response from the policy".
The objective function has two parts. The first part is simply the expected reward
, and is standard for any RL algorithm. The second part is a "penalty term" involving the KL divergence. The strength of the penalty term is determined by the hyperparameter

This KL term works by penalizing the KL divergence (a measure of statistical distance between distributions) between the model being fine-tuned and the initial supervised model. By choosing an appropriate

, the training can balance learning from new data while retaining useful information from the initial model, increasing generalization by avoiding fitting too closely to the new data. Aside from preventing the new model from producing outputs too dissimilar those of the initial model, a second motivation of including the KL term is to encourage the model to output high-entropy text, so as to prevent the model from collapsing to a small number of canned responses.
In simpler terms, the objective function calculates how well the policy's responses are expected to align with human feedback. The policy generates responses to prompts, and each response is evaluated both on how well it matches human preferences (as measured by the reward model) and how similar it is to responses the model would naturally generate. The goal is to balance improving alignment with human preferences while ensuring the model's responses remain diverse and not too far removed from what it has learned during its initial training. This helps the model not only to provide answers that people find useful or agreeable but also to maintain a broad understanding and avoid overly narrow or repetitive responses.
The policy function is usually trained by proximal policy optimization (PPO) algorithm. That is, the parameter

is trained by gradient ascent on the clipped surrogate function.
Classically, the PPO algorithm employs generalized advantage estimation, which means that there is an extra value estimator
(x)
, that updates concurrently with the policy
^
during PPO training:
^,V_,\pi _^,V_,\dots 
. The value estimator is used only during training, and not outside of training.
The PPO uses gradient descent on the following clipped surrogate advantage:
(\phi ):=E_,y\sim \pi _(y|x)\left[\min \left(^(y|x)^(y|x)A(x,y),\mathrm  \left(^(y|x)^(y|x),1-\epsilon ,1+\epsilon \right)A(x,y)\right)\right]
where the advantage term
is defined as
(x,y)-V_(x)
. That is, the advantage is computed as the difference between the reward (the expected return) and the value estimation (the expected return from the policy). This is used to train the policy by gradient ascent on it, usually using a standard momentum-gradient optimizer, like the Adam optimizer.
The original paper initialized the value estimator from the trained reward model. Since PPO is an actor-critic algorithm, the value estimator is updated concurrently with the policy, via minimizing the squared TD-error, which in this case equals the squared advantage term:
(\xi )=\mathbb  _^\left[\left(r_(x,y)-\beta \log \left(^(y|x)(y|x)\right)-V_(x)\right)^\right]
which is minimized by gradient descent on it. Other methods than squared TD-error might be used. See the actor-critic algorithm page for details.
A third term is commonly added to the objective function to prevent the model from catastrophic forgetting. For example, if the model is only trained in customer service, then it might forget general knowledge in geography. To prevent this, the RLHF process incorporates the original language modeling objective. That is, some random texts
are sampled from the original pretraining dataset
, and the model is trained to maximize the log-likelihood of the text
^(x))
. The final objective function is written as:
^\left[r_(x,y)-\beta \log \left(^(y|x)(y|x)\right)\right]+\gamma E_[\log(\pi _^(x))]

controls the strength of this pretraining term. This combined objective function is called PPO-ptx, where "ptx" means "Mixing Pretraining Gradients". It was first used in the InstructGPT paper.
In total, this objective function defines the method for adjusting the RL policy, blending the aim of aligning with human feedback and maintaining the model's original language understanding.
So, writing out fully explicitly, the PPO-ptx objective function is:
(\phi ):=E_^\left[\min \left(^(y|x)^(y|x)A(x,y),\mathrm  \left(^(y|x)^(y|x),1-\epsilon ,1+\epsilon \right)A(x,y)\right)-\beta \log \left(^(y|x)(y|x)\right)\right]+\gamma E_[\log(\pi _^(x))]
which is optimized by gradient ascent on it.
RLHF suffers from challenges with collecting human feedback, learning a reward model, and optimizing the policy. Compared to data collection for techniques like unsupervised or self-supervised learning, collecting data for RLHF is less scalable and more expensive. Its quality and consistency may vary depending on the task, interface, and the preferences and biases of individual humans.
The effectiveness of RLHF depends on the quality of human feedback. For instance, the model may become biased, favoring certain groups over others, if the feedback lacks impartiality, is inconsistent, or is incorrect. There is a risk of overfitting, where the model memorizes specific feedback examples instead of learning to generalize. For instance, feedback predominantly from a specific demographic might lead the model to learn peculiarities or noise, along with the intended alignment. Excessive alignment to the specific feedback it received (that is, to the bias therein) can lead to the model performing sub-optimally in new contexts or when used by different groups. A single reward function cannot always represent the opinions of diverse groups of people. Even with a representative sample, conflicting views and preferences may result in the reward model favoring the majority's opinion, potentially disadvantaging underrepresented groups.
In some cases, as is possible in regular reinforcement learning, there may be a risk of the model learning to manipulate the feedback process or game the system to achieve higher rewards rather than genuinely improving its performance. In the case of RLHF, a model may learn to exploit the fact that it is rewarded for what is evaluated positively and not necessarily for what is actually good, which can lead to it learning to persuade and manipulate. For example, models might learn that apparent confidence, even if inaccurate, garners higher rewards. Such behavior, if unchecked, is not just incentivized but can cause significant deployment issues due to the model's potential to mislead. Studies have found that humans are not skilled at identifying mistakes in LLM outputs in complex tasks; therefore, models learning to generate confident-sounding yet incorrect text can lead to significant issues when deployed.
Similarly to RLHF, reinforcement learning from AI feedback (RLAIF) relies on training a preference model, except that the feedback is automatically generated. This is notably used in Anthropic's constitutional AI, where the AI feedback is based on the conformance to the principles of a constitution.
Direct alignment algorithms (DAA) have been proposed as a new class of algorithms that seek to directly optimize large language models (LLMs) on human feedback data in a supervised manner instead of the traditional policy-gradient methods.
These algorithms aim to align models with human intent more transparently by removing the intermediate step of training a separate reward model. Instead of first predicting human preferences and then optimizing against those predictions, direct alignment methods train models end-to-end on human-labeled or curated outputs. This reduces potential misalignment risks introduced by proxy objectives or reward hacking.
By directly optimizing for the behavior preferred by humans, these approaches often enable tighter alignment with human values, improved interpretability, and simpler training pipelines compared to RLHF.
Direct preference optimization (DPO) is a technique to learn human preferences. Like RLHF, it has been applied to align pre-trained large language models using human-generated preference data. Unlike RLHF, however, which first trains a separate intermediate model to understand what good outcomes look like and then teaches the main model how to achieve those outcomes, DPO simplifies the process by directly adjusting the main model according to people's preferences. It uses a change of variables to define the "preference loss" directly as a function of the policy and uses this loss to fine-tune the model, helping it understand and prioritize human preferences without needing a separate step. Essentially, this approach directly shapes the model's decisions based on positive or negative human feedback.
Recall, the pipeline of RLHF is as follows:
We begin by gathering human preference dataset
We then fit a reward model
to data, by maximum likelihood estimation using the Plackett–Luce model
=\arg \max _\mathbb  _,\dots ,y_)\sim D\left[\ln \prod _^)^e^)\right]
We finally train an optimal policy

that maximizes the objective function:
=\arg \max _\mathbb  _\left[r^(x,y)-\beta \log \left((y|x)(y|x)\right)\right]
However, instead of doing the intermediate step of the reward model, DPO directly optimizes for the final policy.
First, solve directly for the optimal policy, which can be done by Lagrange multipliers, as usual in statistical mechanics:
(y|x)=(y|x)\exp(r^(x,y)/\beta ),
is the partition function. This is unfortunately not tractable, since it requires summing over all possible responses:
\pi ^(y|x)\exp(r^(x,y)/\beta )=\mathbb  _(\cdot |x)[\exp(r^(x,y)/\beta )]
Next, invert this relationship to express the reward implicitly in terms of the optimal policy:
(x,y)=\beta \log (y|x)(y|x)+\beta \log Z(x).
Finally, plug it back to the maximum likelihood estimator, we obtain
=\arg \max _\mathbb  _,\dots ,y_)\sim D\left[\ln \prod _^|x)(y_|x)^e^|x)(y_|x)\right]
Usually, DPO is used for modeling human preference in pairwise comparisons, so that
. In that case, we have
=\arg \max _\mathbb  _,y_)\sim D\left[\log \sigma \left(\beta \log |x)(y_|x)-\beta \log |x)(y_|x)\right)\right]
DPO eliminates the need for a separate reward model or reinforcement learning loop, treating alignment as a supervised learning problem over preference data. This is simpler to implement and train than RLHF and has been shown to produce comparable and sometimes superior results. Nevertheless, RLHF has also been shown to beat DPO on some datasets, for example, on benchmarks that attempt to measure truthfulness. Therefore, the choice of method may vary depending on the features of the human preference data and the nature of the task.
Identity preference optimization (IPO) is a modification to the original DPO objective that introduces a regularization term to reduce the chance of overfitting. It remains robust to overtraining by assuming noise in the preference data.
Foremost, IPO first applies a non-linear mapping over the probability distribution of preferences

instead of the Bradley-Terry assumption to soften the probability of preferences and smooth the labels. Here,


preference objective separate from the policy objective. This helps avoid the overfitting issue of the assumption that pairwise preferences can be substituted for point-wise rewards, which weakens the KL regularization by heavily skewing the preference distribution.
As with DPO, IPO is also formulated as an offline learning objective learned over a human preference dataset
. In particular, the IPO introduces a new objective by applying a mapping

over the preference probability distribution. Practically,

is taken as the identity mapping, which results in IPO. Hence, IPO also directly optimizes for the final policy from the preference dataset and bypasses the reward modeling stage by the following objective:
\mathbb  [\Psi (p^(y_\succ y_|x))]-\beta D_(\pi _||\pi _)
(y_\succ y_|x)
is preference distribution of the chosen responses
over the rejected responses
. However, since
is not observed directly, we sample from a Bernoulli distribution from the offline preference dataset as:
(y\succ y'|x)=\mathbb  _[I\yy'x\]
To solve this objective, IPO minimizes the quadratic loss function:
&\mathbb  _,y_)\sim D[(h_(x,y_,y_)-I(y_,y_))]^\\&=\mathbb  _,y_)\sim D[Ih_(x,y_,y_)-(1-I)h_(x,y_,y_)-\beta ^]^\\&=\mathbb  _,y_\sim D[h_(x,y_,y_)-\beta ^]^\end
(x,y_,y_)=\log \left((y_|x)(y_|x))\right)-\log \left((y_|x)(y_|x)\right)
is a function drawn from the Bernoulli distribution from the preference dataset. Here,
is 1 if
is preferred to
which happens with probability
(y\succ y')
, and 0 otherwise. As such, the simplification of the expression directly follows from exploiting the symmetry of
from the Bernoulli such that for each datapoint
,y_)_\sim D
. In particular this symmetry can be represented as
 _[p_]=
 [I(y,y')]=p_
In summary, IPO can control the gap between the log-likelihood ratios of the policy model and the reference by always regularizing the solution towards the reference model. It allows learning directly from preferences without a reward modelling stage and without relying on the Bradley-Terry modelisation assumption that assumes that pairwise preferences can be substituted with pointwise rewards. Thus, it avoids overfitting to the preference dataset especially when preferences are near deterministic and the KL term fails.
Kahneman-Tversky optimization (KTO) is another direct alignment algorithm drawing from prospect theory to model uncertainty in human decisions that may not maximize the expected value.
In general, KTO seeks to optimize a class of new loss functions proposed as “human-aware losses” (HALO) formulated under prospect theory to model “human values” of a query, response pair
(x,y)-E_[r_(x,y)'])
. A function is defined as a human-aware loss for the value described by the general HALO objective:
,\pi _)=\mathbb  _[a_vr_(x,y)\;-\;\underbrace [\,r_(x,y')\,] _]+C_
is the preference data,
is some constant relevant to the dataset, and
is some distribution representing the baseline or “reference”. Each training example is attached a label
\in \
that tells us if the example is desirable (we want to push up its reward) and -1 if it’s undesirable (in order to push down its reward). Unlike previous definitions of the reward, KTO defines
(x,y)
as the “implied reward” taken by the log-likelihood ratio between the policy model and the reference model
(y|x)(y|x)\right)
. Here, the value function
is a non-linear (typically concave) function that mimics human loss aversion and risk aversion. As opposed to previous preference optimization algorithms, the motivation of KTO lies in maximizing the utility of model outputs from a human perspective rather than maximizing the likelihood of a “better” label (chosen vs. rejected responses). Hence, it constructs a more relaxed generalization to preference distributions by requiring only a binary feedback signal
instead of explicit preference pairs. For each example
in the dataset
, KTO explicitly optimizes the HALO objective as:
^\;=\;\arg \max _\;\;\mathbb  _\gamma _\;-\;v(x,y)

is a class-specific constant (e.g.,
=\lambda _\lambda _
) controlling how strongly the model should push up good outputs vs. push down bad ones. The value function
is defined piecewise depending on whether
is desirable (

) or undesirable (

\lambda _\,\sigma \!\,\beta \,r_(x,y)\;-\;z_,&\quad y\sim y_ \mid x,\\[6pt]\lambda _\,\sigma \!\,\beta \,z_\;-\;r_(x,y),&\quad y\sim y_ \mid x\end
=\mathrm  \!\,\pi _(y'\mid x)\;\;\pi _ (y'\mid x)
is a baseline given by the Kullback–Leibler divergence. Here,

controls how “risk-averse” the value function is (larger

= faster saturation in the logistic function

). Intuitively, desirable outputs push the model to increase

-z_
becomes more positive. Undesirable ones push it in the opposite direction, so the reward is less than the reference. Since many real-world feedback pipelines yield "like/dislike" data more easily than pairwise comparisons, KTO is designed to be data-cheap and to reflect "loss aversion" more directly by using a straightforward notion of "good vs. bad" at the example level.
"Learning RLHF (PPO) with codes (Huggingface TRL) | Yiyang Feng". yiyangfeng.me. Retrieved 2025-01-26.
"The N Implementation Details of RLHF with PPO". huggingface.co. 2025-01-19. Retrieved 2025-01-26.
"Proximal Policy Optimization — Spinning Up documentation". spinningup.openai.com. Retrieved 2025-01-26.
Huang, Shengyi; Noukhovitch, Michael; Hosseini, Arian; Rasul, Kashif; Wang, Weixun; Tunstall, Lewis (2024-03-24), The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization, arXiv:2403.17031 Multi-agent reinforcement learning (MARL) is a sub-field of reinforcement learning. It focuses on studying the behavior of multiple learning agents that coexist in a shared environment. Each agent is motivated by its own rewards, and does actions to advance its own interests; in some environments these interests are opposed to the interests of other agents, resulting in complex group dynamics.
Multi-agent reinforcement learning is closely related to game theory and especially repeated games, as well as multi-agent systems. Its study combines the pursuit of finding ideal algorithms that maximize rewards with a more sociological set of concepts. While research in single-agent reinforcement learning is concerned with finding the algorithm that gets the biggest number of points for one agent, research in multi-agent reinforcement learning evaluates and quantifies social metrics, such as cooperation, reciprocity, equity, social influence, language and discrimination.
Similarly to single-agent reinforcement learning, multi-agent reinforcement learning is modeled as some form of a Markov decision process (MDP). Fix a set of agents
. We then define:
of environment states.
_
of actions for each of the agents

(s,s')=\Pr(s_=s'\mid s_=s,_=)
is the probability of transition (at time
) from state
under joint action

_(s,s')
is the immediate joint reward after the transition from
with joint action

In settings with perfect information, such as the games of chess and Go, the MDP would be fully observable. In settings with imperfect information, especially in real-world applications like self-driving cars, each agent would access an observation that only has part of the information about the current state. In the partially observable setting, the core model is the partially observable stochastic game in the general case, and the decentralized POMDP in the cooperative case.
When multiple agents are acting in a shared environment their interests might be aligned or misaligned. MARL allows exploring all the different alignments and how they affect the agents' behavior:
In pure competition settings, the agents' rewards are exactly opposite to each other, and therefore they are playing against each other.
Pure cooperation settings are the other extreme, in which agents get the exact same rewards, and therefore they are playing with each other.
Mixed-sum settings cover all the games that combine elements of both cooperation and competition.
When two agents are playing a zero-sum game, they are in pure competition with each other. Many traditional games such as chess and Go fall under this category, as do two-player variants of video games like StarCraft. Because each agent can only win at the expense of the other agent, many complexities are stripped away. There is no prospect of communication or social dilemmas, as neither agent is incentivized to take actions that benefit its opponent.
The Deep Blue and AlphaGo projects demonstrate how to optimize the performance of agents in pure competition settings.
One complexity that is not stripped away in pure competition settings is autocurricula. As the agents' policy is improved using self-play, multiple layers of learning may occur.
MARL is used to explore how separate agents with identical interests can communicate and work together. Pure cooperation settings are explored in recreational cooperative games such as Overcooked, as well as real-world scenarios in robotics.
In pure cooperation settings all the agents get identical rewards, which means that social dilemmas do not occur.
In pure cooperation settings, oftentimes there are an arbitrary number of coordination strategies, and agents converge to specific "conventions" when coordinating with each other. The notion of conventions has been studied in language and also alluded to in more general multi-agent collaborative tasks.
Most real-world scenarios involving multiple agents have elements of both cooperation and competition. For example, when multiple self-driving cars are planning their respective paths, each of them has interests that are diverging but not exclusive: Each car is minimizing the amount of time it's taking to reach its destination, but all cars have the shared interest of avoiding a traffic collision.
Zero-sum settings with three or more agents often exhibit similar properties to mixed-sum settings, since each pair of agents might have a non-zero utility sum between them.
Mixed-sum settings can be explored using classic matrix games such as prisoner's dilemma, more complex sequential social dilemmas, and recreational games such as
Among Us, Diplomacy and
Mixed-sum settings can give rise to communication and social dilemmas.
As in game theory, much of the research in MARL revolves around social dilemmas, such as prisoner's dilemma, chicken and stag hunt.
While game theory research might focus on Nash equilibria and what an ideal policy for an agent would be, MARL research focuses on how the agents would learn these ideal policies using a trial-and-error process. The reinforcement learning algorithms that are used to train the agents are maximizing the agent's own reward; the conflict between the needs of the agents and the needs of the group is a subject of active research.
Various techniques have been explored in order to induce cooperation in agents: Modifying the environment rules, adding intrinsic rewards, and more.
Social dilemmas like prisoner's dilemma, chicken and stag hunt are "matrix games". Each agent takes only one action from a choice of two possible actions, and a simple 2x2 matrix is used to describe the reward that each agent will get, given the actions that each agent took.
In humans and other living creatures, social dilemmas tend to be more complex. Agents take multiple actions over time, and the distinction between cooperating and defecting is not as clear cut as in matrix games. The concept of a sequential social dilemma (SSD) was introduced in 2017 as an attempt to model that complexity. There is ongoing research into defining different kinds of SSDs and showing cooperative behavior in the agents that act in them.
An autocurriculum (plural: autocurricula) is a reinforcement learning concept that's salient in multi-agent experiments. As agents improve their performance, they change their environment; this change in the environment affects themselves and the other agents. The feedback loop results in several distinct phases of learning, each depending on the previous one. The stacked layers of learning are called an autocurriculum. Autocurricula are especially apparent in adversarial settings, where each group of agents is racing to counter the current strategy of the opposing group.
The Hide and Seek game is an accessible example of an autocurriculum occurring in an adversarial setting. In this experiment, a team of seekers is competing against a team of hiders. Whenever one of the teams learns a new strategy, the opposing team adapts its strategy to give the best possible counter. When the hiders learn to use boxes to build a shelter, the seekers respond by learning to use a ramp to break into that shelter. The hiders respond by locking the ramps, making them unavailable for the seekers to use. The seekers then respond by "box surfing", exploiting a glitch in the game to penetrate the shelter. Each "level" of learning is an emergent phenomenon, with the previous level as its premise. This results in a stack of behaviors, each dependent on its predecessor.
Autocurricula in reinforcement learning experiments are compared to the stages of the evolution of life on Earth and the development of human culture. A major stage in evolution happened 2-3 billion years ago, when photosynthesizing life forms started to produce massive amounts of oxygen, changing the balance of gases in the atmosphere. In the next stages of evolution, oxygen-breathing life forms evolved, eventually leading up to land mammals and human beings. These later stages could only happen after the photosynthesis stage made oxygen widely available. Similarly, human culture could not have gone through the Industrial Revolution in the 18th century without the resources and insights gained by the agricultural revolution at around 10,000 BC.
Multi-agent reinforcement learning has been applied to a variety of use cases in science and industry:
Multi-agent reinforcement learning has been used in research into AI alignment. The relationship between the different agents in a MARL setting can be compared to the relationship between a human and an AI agent. Research efforts in the intersection of these two fields attempt to simulate possible conflicts between a human's intentions and an AI agent's actions, and then explore which variables could be changed to prevent these conflicts.
There are some inherent difficulties about multi-agent deep reinforcement learning. The environment is not stationary anymore, thus the Markov property is violated: transitions and rewards do not only depend on the current state of an agent.
Stefano V. Albrecht, Filippos Christianos, Lukas Schäfer. Multi-Agent Reinforcement Learning: Foundations and Modern Approaches. MIT Press, 2024. https://www.marl-book.com
Kaiqing Zhang, Zhuoran Yang, Tamer Basar. Multi-agent reinforcement learning: A selective overview of theories and algorithms. Studies in Systems, Decision and Control, Handbook on RL and Control, 2021. [1]
Yang, Yaodong; Wang, Jun (2020). "An Overview of Multi-Agent Reinforcement Learning from Game Theoretical Perspective". arXiv:2011.00583 [cs.MA]. Physics-informed neural networks (PINNs), also referred to as Theory-Trained Neural Networks (TTNs), are a type of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set in the learning process, and can be described by partial differential equations (PDEs). Low data availability for some biological and engineering problems limit the robustness of conventional machine learning models used for these applications. The prior knowledge of general physical laws acts in the training of neural networks (NNs) as a regularization agent that limits the space of admissible solutions, increasing the generalizability of the function approximation. This way, embedding this prior information into a neural network results in enhancing the information content of the available data, facilitating the learning algorithm to capture the right solution and to generalize well even with a low amount of training examples. For they process continuous spatial and time coordinates and output continuous PDE solutions, they can be categorized as neural fields.
Most of the physical laws that govern the dynamics of a system can be described by partial differential equations. For example, the Navier–Stokes equations are a set of partial differential equations derived from the conservation laws (i.e., conservation of mass, momentum, and energy) that govern fluid mechanics. The solution of the Navier–Stokes equations with appropriate initial and boundary conditions allows the quantification of flow dynamics in a precisely defined geometry. However, these equations cannot be solved exactly and therefore numerical methods must be used (such as finite differences, finite elements and finite volumes). In this setting, these governing equations must be solved while accounting for prior assumptions, linearization, and adequate time and space discretization.
Recently, solving the governing partial differential equations of physical phenomena using deep learning has emerged as a new field of scientific machine learning (SciML), leveraging the universal approximation theorem and high expressivity of neural networks. In general, deep neural networks could approximate any high-dimensional function given that sufficient training data are supplied. However, such networks do not consider the physical characteristics underlying the problem, and the level of approximation accuracy provided by them is still heavily dependent on careful specifications of the problem geometry as well as the initial and boundary conditions. Without this preliminary information, the solution is not unique and may lose physical correctness. On the other hand, physics-informed neural networks (PINNs) leverage governing physical equations in neural network training. Namely, PINNs are designed to be trained to satisfy the given training data as well as the imposed governing equations. In this fashion, a neural network can be guided with training data that do not necessarily need to be large and complete. Potentially, an accurate solution of partial differential equations can be found without knowing the boundary conditions. Therefore, with some knowledge about the physical characteristics of the problem and some form of training data (even sparse and incomplete), PINN may be used for finding an optimal solution with high fidelity.
PINNs allow for addressing a wide range of problems in computational science and represent a pioneering technology leading to the development of new classes of numerical solvers for PDEs. PINNs can be thought of as a meshfree alternative to traditional approaches (e.g., CFD for fluid dynamics), and new data-driven approaches for model inversion and system identification. Notably, the trained PINN network can be used for predicting the values on simulation grids of different resolutions without the need to be retrained. In addition, being neural fields, they allow for exploiting automatic differentiation (AD) to compute the required derivatives in the partial differential equations, a new class of differentiation techniques widely used to derive neural networks assessed to be superior to numerical or symbolic differentiation.
A general nonlinear partial differential equation can be:
+N[u;\lambda ]=0,\quad x\in \Omega ,\quad t\in [0,T]
denotes the solution,

is a nonlinear operator parameterized by


is a subset of
 ^
. This general form of governing equations summarizes a wide range of problems in mathematical physics, such as conservative laws, diffusion process, advection-diffusion systems, and kinetic equations. Given noisy measurements of a generic dynamic system described by the equation above, PINNs can be designed to solve two classes of problems:
data-driven discovery of partial differential equations.
The data-driven solution of PDE computes the hidden state
of the system given boundary data and/or measurements
, and fixed model parameters

. We solve:
+N[u]=0,\quad x\in \Omega ,\quad t\in [0,T]
By defining the residual
by a deep neural network. This network can be differentiated using automatic differentiation. The parameters of
can be then learned by minimizing the following loss function
=\Vert u-z\Vert _
is the error between the PINN
and the set of boundary conditions and measured data on the set of points

where the boundary conditions and data are defined, and
=\Vert f\Vert _
is the mean-squared error of the residual function. This second term encourages the PINN to learn the structural information expressed by the partial differential equation during the training process.
This approach has been used to yield computationally efficient physics-informed surrogate models with applications in the forecasting of physical processes, model predictive control, multi-physics and multi-scale modeling, and simulation. It has been shown to converge to the solution of the PDE.
Given noisy and incomplete measurements
of the state of the system, the data-driven discovery of PDE results in computing the unknown state
and learning model parameters

that best describe the observed data and it reads as follows:
+N[u;\lambda ]=0,\quad x\in \Omega ,\quad t\in [0,T]
+N[u;\lambda ]=0
by a deep neural network,
results in a PINN. This network can be derived using automatic differentiation. The parameters of
, together with the parameter

of the differential operator can be then learned by minimizing the following loss function
=\Vert u-z\Vert _
state solutions and measurements at sparse location

, respectively and
=\Vert f\Vert _
residual function. This second term requires the structured information represented by the partial differential equations to be satisfied in the training process.
This strategy allows for discovering dynamic models described by nonlinear PDEs assembling computationally efficient and fully differentiable surrogate models that may find application in predictive forecasting, control, and data assimilation.
PINN is unable to approximate PDEs that have strong non-linearity or sharp gradients that commonly occur in practical fluid flow problems. Piece-wise approximation has been an old practice in the field of numerical approximation. With the capability of approximating strong non-linearity extremely light weight PINNs are used to solve PDEs in much larger discrete subdomains that increases accuracy substantially and decreases computational load as well. DPINN (Distributed physics-informed neural networks) and DPIELM (Distributed physics-informed extreme learning machines) are generalizable space-time domain discretization for better approximation. DPIELM is an extremely fast and lightweight approximator with competitive accuracy. Domain scaling on the top has a special effect. Another school of thought is discretization for parallel computation to leverage usage of available computational resources.
XPINNs is a generalized space-time domain decomposition approach for the physics-informed neural networks (PINNs) to solve nonlinear partial differential equations on arbitrary complex-geometry domains. The XPINNs further pushes the boundaries of both PINNs as well as Conservative PINNs (cPINNs), which is a spatial domain decomposition approach in the PINN framework tailored to conservation laws. Compared to PINN, the XPINN method has large representation and parallelization capacity due to the inherent property of deployment of multiple neural networks in the smaller subdomains. Unlike cPINN, XPINN can be extended to any type of PDEs. Moreover, the domain can be decomposed in any arbitrary way (in space and time), which is not possible in cPINN. Thus, XPINN offers both space and time parallelization, thereby reducing the training cost more effectively. The XPINN is particularly effective for the large-scale problems (involving large data set) as well as for the high-dimensional problems where single network based PINN is not adequate. The rigorous bounds on the errors resulting from the approximation of the nonlinear PDEs (incompressible Navier–Stokes equations) with PINNs and XPINNs are proved. However, DPINN debunks the use of residual (flux) matching at the domain interfaces as they hardly seem to improve the optimization.
In the PINN framework, initial and boundary conditions are not analytically satisfied, thus they need to be included in the loss function of the network to be simultaneously learned with the differential equation (DE) unknown functions. Having competing objectives during the network's training can lead to unbalanced gradients while using gradient-based techniques, which causes PINNs to often struggle to accurately learn the underlying DE solution. This drawback is overcome by using functional interpolation techniques such as the Theory of functional connections (TFC)'s constrained expression, in the Deep-TFC framework, which reduces the solution search space of constrained problems to the subspace of neural network that analytically satisfies the constraints. A further improvement of PINN and functional interpolation approach is given by the Extreme Theory of Functional Connections (X-TFC) framework, where a single-layer Neural Network and the extreme learning machine training algorithm are employed. X-TFC allows to improve the accuracy and performance of regular PINNs, and its robustness and reliability are proved for stiff problems, optimal control, aerospace, and rarefied gas dynamics applications.
Regular PINNs are only able to obtain the solution of a forward or inverse problem on a single geometry. It means that for any new geometry (computational domain), one must retrain a PINN. This limitation of regular PINNs imposes high computational costs, specifically for a comprehensive investigation of geometric parameters in industrial designs. Physics-informed PointNet (PIPN) is fundamentally the result of a combination of PINN's loss function with PointNet. In fact, instead of using a simple fully connected neural network, PIPN uses PointNet as the core of its neural network. PointNet has been primarily designed for deep learning of 3D object classification and segmentation by the research group of Leonidas J. Guibas. PointNet extracts geometric features of input computational domains in PIPN. Thus, PIPN is able to solve governing equations on multiple computational domains (rather than only a single domain) with irregular geometries, simultaneously. The effectiveness of PIPN has been shown for incompressible flow, heat transfer and linear elasticity.
Physics-informed neural networks (PINNs) have proven particularly effective in solving inverse problems within differential equations, demonstrating their applicability across science, engineering, and economics. They have shown to be useful for solving inverse problems in a variety of fields, including nano-optics, topology optimization/characterization, multiphase flow in porous media, and high-speed fluid flow. PINNs have demonstrated flexibility when dealing with noisy and uncertain observation datasets. They also demonstrated clear advantages in the inverse calculation of parameters for multi-fidelity datasets, meaning datasets with different quality, quantity, and types of observations. Uncertainties in calculations can be evaluated using ensemble-based or Bayesian-based calculations.
PINNs can also be used in connection with symbolic regression for discovering the mathematical expression in connection with discovery of parameters and functions. One example of such application is the study on chemical ageing of cellulose insulation material, in this example PINNs are used to first discover a parameter for a set of ordinary differential equations (ODEs) and later a function solution, which is later used to find a more fitting expression using a symbolic regression with a combination of operators.
Ensemble of physics-informed neural networks is applied for solving plane elasticity problems. Surrogate networks are intended for the unknown functions, namely, the components of the strain and the stress tensors as well as the unknown displacement field, respectively. The residual network provides the residuals of the partial differential equations (PDEs) and of the boundary conditions. The computational approach is based on principles of artificial intelligence. This approach can be extended to nonlinear elasticity problems, where the constitutive equations are nonlinear. PINNs can also be used for Kirchhoff plate bending problems with transverse distributed loads and to contact models with elastic Winkler’s foundations.
Deep backward stochastic differential equation method is a numerical method that combines deep learning with Backward stochastic differential equation (BSDE) to solve high-dimensional problems in financial mathematics. By leveraging the powerful function approximation capabilities of deep neural networks, deep BSDE addresses the computational challenges faced by traditional numerical methods like finite difference methods or Monte Carlo simulations, which struggle with the curse of dimensionality. Deep BSDE methods use neural networks to approximate solutions of high-dimensional partial differential equations (PDEs), effectively reducing the computational burden. Additionally, integrating Physics-informed neural networks (PINNs) into the deep BSDE framework enhances its capability by embedding the underlying physical laws into the neural network architecture, ensuring solutions adhere to governing stochastic differential equations, resulting in more accurate and reliable solutions.
An extension or adaptation of PINNs are Biologically-informed neural networks (BINNs). BINNs introduce two key adaptations to the typical PINN framework: (i) the mechanistic terms of the governing PDE are replaced by neural networks, and (ii) the loss function
is modified to include
, a term used to incorporate domain-specific knowledge that helps enforce biological applicability. For (i), this adaptation has the advantage of relaxing the need to specify the governing differential equation a priori, either explicitly or by using a library of candidate terms. Additionally, this approach circumvents the potential issue of misspecifying regularization terms in stricter theory-informed cases.
A natural example of BINNs can be found in cell dynamics, where the cell density
is governed by a reaction-diffusion equation with diffusion and growth functions
=\nabla \cdot (D(u)\nabla u)+G(u)u,\quad x\in \Omega ,\quad t\in [0,T]
In this case, a component of

, which penalizes values of
that fall outside a biologically relevant diffusion range defined by
\leq D\leq D_
. Furthermore, the BINN architecture, when utilizing multilayer-perceptrons (MLPs), would function as follows: an MLP is used to construct
from model inputs
, serving as a surrogate model for the cell density
. This surrogate is then fed into the two additional MLPs,
, which model the diffusion and growth functions. Automatic differentiation can then be applied to compute the necessary derivatives of
to form the governing reaction-diffusion equation.
Note that since
is a surrogate for the cell density, it may contain errors, particularly in regions where the PDE is not fully satisfied. Therefore, the reaction-diffusion equation may be solved numerically, for instance using a method-of-lines approach.
Translation and discontinuous behavior are hard to approximate using PINNs. They fail when solving differential equations with slight advective dominance and hence asymptotic behaviour causes the method to fail. Such PDEs could be solved by scaling variables.
This difficulty in training of PINNs in advection-dominated PDEs can be explained by the Kolmogorov n–width of the solution.
They also fail to solve a system of dynamical systems and hence have not been a success in solving chaotic equations. One of the reasons behind the failure of regular PINNs is soft-constraining of Dirichlet and Neumann boundary conditions which pose a multi-objective optimization problem which requires manually weighing the loss terms to be able to optimize.
More generally, posing the solution of a PDE as an optimization problem brings with it all the problems that are faced in the world of optimization, the major one being getting stuck in local optima. Specifically, it was shown that some of these local optima are connected to unstable solutions of a given PDE.
PINN – repository to implement physics-informed neural network in Python
XPINN – repository to implement extended physics-informed neural network (XPINN) in Python
PIPN [2]– repository to implement physics-informed PointNet (PIPN) in Python In the context of artificial neural networks, the rectifier or ReLU (rectified linear unit) activation function is an activation function defined as the non-negative part of its argument, i.e., the ramp function:
 (x)=x^=\max(0,x)==x&x>0,\\0&x\leq 0\end
is the input to a neuron. This is analogous to half-wave rectification in electrical engineering.
ReLU is one of the most popular activation functions for artificial neural networks, and finds application in computer vision and speech recognition using deep neural nets and computational neuroscience.
The ReLU was first used by Alston Householder in 1941 as a mathematical abstraction of biological neural networks.
Kunihiko Fukushima in 1969 used ReLU in the context of visual feature extraction in hierarchical neural networks. Thirty years later, Hahnloser et al. argued that ReLU approximates the biological relationship between neural firing rates and input current, in addition to enabling recurrent neural network dynamics to stabilise under weaker criteria.
Prior to 2010, most activation functions used were the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more numerically efficient counterpart, the hyperbolic tangent. Around 2010, the use of ReLU became common again.
Jarrett et al. (2009) noted that rectification by either absolute or ReLU (which they called "positive part") was critical for object recognition in convolutional neural networks (CNNs), specifically because it allows average pooling without neighboring filter outputs cancelling each other out. They hypothesized that the use of sigmoid or tanh was responsible for poor performance in previous CNNs.
Nair and Hinton (2010) made a theoretical argument that the softplus activation function should be used, in that the softplus function numerically approximates the sum of an exponential number of linear models that share parameters. They then proposed ReLU as a good approximation to it. Specifically, they began by considering a single binary neuron in a Boltzmann machine that takes
as input, and produces 1 as output with probability

. They then considered extending its range of output by making infinitely many copies of it
,X_,X_,\dots 
, that all take the same input, offset by an amount

, then their outputs are added together as
^X_
. They then demonstrated that
^X_
is approximately equal to
(\log(1+e^),\sigma (x))
, which is also approximately equal to
 ((x,\sigma (x)))

stands for the gaussian distribution.
They also argued for another reason for using ReLU: that it allows "intensity equivariance" in image recognition. That is, multiplying input image by a constant
multiplies the output also. In contrast, this is false for other activation functions like sigmoid or tanh. They found that ReLU activation allowed good empirical performance in restricted Boltzmann machines.
Glorot et al (2011) argued that ReLU has the following advantages over sigmoid or tanh. ReLU is more similar to biological neurons' responses in their main operating regime. ReLU avoids vanishing gradients. ReLU is cheaper to compute. ReLU creates sparse representation naturally, because many hidden units output exactly zero for a given input. They also found empirically that deep networks trained with ReLU can achieve strong performance without unsupervised pre-training, especially on large, purely supervised tasks.
Advantages of ReLU include:
Sparse activation: for example, in a randomly initialized network, only about 50% of hidden units are activated (i.e. have a non-zero output).
Better gradient propagation: fewer vanishing gradient problems compared to sigmoidal activation functions that saturate in both directions.
Efficiency: only requires comparison and addition.
Scale-invariant (homogeneous, or "intensity equivariance"):
a\geq 0
Possible downsides can include:
Non-differentiability at zero (however, it is differentiable anywhere else, and the value of the derivative at zero can be chosen to be 0 or 1 arbitrarily).
Not zero-centered: ReLU outputs are always non-negative. This can make it harder for the network to learn during backpropagation, because gradient updates tend to push weights in one direction (positive or negative). Batch normalization can help address this.
ReLU is unbounded.
Redundancy of the parametrization: Because ReLU is scale-invariant, the network computes the exact same function by scaling the weights and biases in front of a ReLU activation by
, and the weights after by
Dying ReLU: ReLU neurons can sometimes be pushed into states in which they become inactive for essentially all inputs. In this state, no gradients flow backward through the neuron, and so the neuron becomes stuck in a perpetually inactive state (it "dies"). This is a form of the vanishing gradient problem. In some cases, large numbers of neurons in a network can become stuck in dead states, effectively decreasing the model capacity and potentially even halting the learning process. This problem typically arises when the learning rate is set too high. It may be mitigated by using "leaky" ReLU instead, where a small positive slope is assigned for
. However, depending on the task, performance may be reduced.
Leaky ReLU (2014) allows a small, positive gradient when the unit is inactive, helping to mitigate the vanishing gradient problem. This gradient is defined by a parameter

, typically set to 0.01–0.3.
x&x>0,\\\alpha x&x\leq 0,\end\qquad f'(x)=1&x>0,\\\alpha &x\leq 0.\end
The same function can also be expressed without the piecewise notation as:
x+|x|
Parametric ReLU (PReLU, 2016) takes this idea further by making

a learnable parameter along with the other network parameters.
Note that for

, this is equivalent to

and thus has a relation to "maxout" networks.
Concatenated ReLU (CReLU, 2016) preserves positive and negative phase information by returning two values:
 (x),\operatorname  (-x)].
A smooth approximation to the rectifier is the analytic function
),\qquad f'(x)==
which is called the softplus (2000) or SmoothReLU function. For large negative
it is roughly

, so just above 0, while for large positive
it is roughly
, so just above
This function can be approximated as:
\right)\approx \ln 2,&x=0,\\[6pt],&x\neq 0\end
By making the change of variables
, this is equivalent to
(1+2^)\approx 1,&y=0,\\[6pt],&y\neq 0\end
A sharpness parameter
may be included:
),\qquad f'(x)==
The derivative of softplus is the logistic function. This in turn can be viewed as a smooth approximation of the derivative of the rectifier, the Heaviside step function.
The multivariable generalization of single-variable softplus is the LogSumExp with the first argument set to zero:
 ^(x_,\dots ,x_):=\operatorname  (0,x_,\dots ,x_)=\ln(1+e^+\cdots +e^)
The LogSumExp function is
 (x_,\dots ,x_)=\ln(e^+\cdots +e^)
and its gradient is the softmax; the softmax with the first argument set to zero is the multivariable generalization of the logistic function. Both LogSumExp and softmax are used in machine learning.
Exponential linear units (2015) smoothly allow negative values. This is an attempt to make the mean activations closer to zero, which speeds up learning. It has been shown that ELUs can obtain higher classification accuracy than ReLUs.
x&x>0,\\\alpha \left(e^-1\right)&x\leq 0\end\qquad f'(x)=1&x>0,\\\alpha e^&x\leq 0\end
In these formulas,

is a hyperparameter to be tuned with the constraint

Given the same interpretation of

, ELU can be viewed as a smoothed version of a shifted ReLU (SReLU), which has the form

GELU (2016) is a smooth approximation to the rectifier:



is the cumulative distribution function of the standard normal distribution.
This activation function is illustrated in the figure at the start of this article. It has a "bump" with negative derivative to the left of x < 0. It serves as the default activation for many transformer models such as BERT.
The SiLU (sigmoid linear unit) or swish function is another smooth approximation which uses the sigmoid (logistic) function, first introduced in the 2016 GELU paper:
 (x),
 '(x)+\operatorname  (x)
It is cheaper to calculate than GELU. It also has a "bump".
The mish function (2019) can also be used as a smooth approximation of the rectifier. It is defined as
\operatorname  (x),
is the hyperbolic tangent, and
 (x)
is the softplus function.
Mish was obtained by experimenting with functions similar to Swish (SiLU, see above). It is non-monotonic (has a "bump") like Swish. The main new feature is that it exhibits a "self-regularizing" behavior attributed to a term in its first derivative.
Squareplus (2021) is the function
+b

is a hyperparameter that determines the "size" of the curved region near
. (For example, letting
yields ReLU, and letting
yields the metallic mean function.)
Squareplus shares many properties with softplus: It is monotonic, strictly positive, approaches 0 as

, approaches the identity as

, and is

smooth. However, squareplus can be computed using only algebraic functions, making it well-suited for settings where computational resources or instruction sets are limited. Additionally, squareplus requires no special consideration to ensure numerical stability when
ExtendeD Exponential Linear Unit (DELU, 2023) is an activation function which is smoother within the neighborhood of zero and sharper for bigger values, allowing better allocation of neurons in the learning process for higher performance. Thanks to its unique design, it has been shown that DELU may obtain higher classification accuracy than ReLU and ELU.
x&x>x_,\\(e^-1)/b&x\leq x_\end\qquad f'(x)=1&x>x_,\\(a/b)e^&x\leq x_\end
In these formulas,
are hyperparameter values which could be set as default constraints
, as done in the original work.
Layer (deep learning) In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and "training" them to process data. The adjective "deep" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be supervised, semi-supervised or unsupervised.
Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.
Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.
Most modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.
Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face.
Importantly, a deep learning process can learn which features to optimally place at which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate on. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.
The word "deep" in "deep learning" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than two. CAP of depth two has been shown to be a universal approximator in the sense that it can emulate any function. Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > two) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.
Deep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance.
Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data is more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.
The term deep learning was introduced to the machine learning community by Rina Dechter in 1986, and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons. Although the history of its appearance is apparently more complicated.
Deep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference.
The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima's rectified linear unit.
The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al. proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator.
The probabilistic interpretation derives from the field of machine learning. It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function. The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.
There are two types of artificial neural network (ANN): feedforward neural network (FNN) or multilayer perceptron (MLP) and recurrent neural networks (RNN). RNNs have cycles in their connectivity structure, FNNs don't. In the 1920s, Wilhelm Lenz and Ernst Ising created the Ising model which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive. His learning RNN was republished by John Hopfield in 1982. Other early recurrent neural networks were published by Kaoru Nakano in 1971. Already in 1948, Alan Turing produced work on "Intelligent Machinery" that was not published in his lifetime, containing "ideas related to artificial evolution and learning RNNs".
Frank Rosenblatt (1958) proposed the perceptron, an MLP with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. He later published a 1962 book that also introduced variants and computer experiments, including a version with four-layer perceptrons "with adaptive preterminal networks" where the last two layers have learned weights (here he credits H. D. Block and B. W. Knight). The book cites an earlier network by R. D. Joseph (1960) "functionally equivalent to a variation of" this four-layer system (the book mentions Joseph over 30 times). Should Joseph therefore be considered the originator of proper adaptive multilayer perceptrons with learning hidden units? Unfortunately, the learning algorithm was not a functional one, and fell into oblivion.
The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in 1965. They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron to handle more complex, nonlinear, and hierarchical relationships. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or "gates".
The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.
In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning.
Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation.
Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology "back-propagating errors" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. The modern form of backpropagation was first published in Seppo Linnainmaa's master thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work.
The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation. In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.
In 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days. In 1990, Wei Zhang implemented a CNN on optical computing hardware. In 1991, a CNN was applied to medical image object segmentation and breast cancer detection in mammograms. LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images.
Recurrent neural networks (RNN) were further developed in the 1980s. Recurrence is used for sequence processing, and when a recurrent network is unrolled, it mathematically resembles a deep feedforward layer. Consequently, they have similar properties and issues, and their developments had mutual influences. In RNN, two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study problems in cognitive psychology.
In the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, in 1991, Jürgen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning where each RNN tries to predict its own next input, which is the next unexpected input of the RNN below. This "neural history compressor" uses predictive coding to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by distilling a higher level chunker network into a lower level automatizer network. In 1993, a neural history compressor solved a "Very Deep Learning" task that required more than 1000 subsequent layers in an RNN unfolded in time. The "P" in ChatGPT refers to such pre-training.
Sepp Hochreiter's diploma thesis (1991) implemented the neural history compressor, and identified and analyzed the vanishing gradient problem. Hochreiter proposed recurrent residual connections to solve the vanishing gradient problem. This led to the long short-term memory (LSTM), published in 1995. LSTM can learn "very deep learning" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. That LSTM was not yet the modern architecture, which required a "forget gate", introduced in 1999, which became the standard RNN architecture.
In 1991, Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called "artificial curiosity". In 2014, this principle was used in generative adversarial networks (GANs).
During 1985–1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine, restricted Boltzmann machine, Helmholtz machine, and the wake-sleep algorithm. These were designed for unsupervised learning of deep generative models. However, those were more computationally expensive compared to backpropagation. Boltzmann machine learning algorithm, published in 1985, was briefly popular before being eclipsed by the backpropagation algorithm in 1986. (p. 112 ). A 1988 network became state of the art in protein structure prediction, an early application of deep learning to bioinformatics.
Both shallow and deep learning (e.g., recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed, including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power.
Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI researched in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 NIST Speaker Recognition benchmark. It was deployed in the Nuance Verifier, representing the first major industrial application of deep learning.
The principle of elevating "raw" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the "raw" spectrogram or linear filter-bank features in the late 1990s, showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.
Neural networks entered a lull, and simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) became the preferred choices in the 1990s and 2000s, because of artificial neural networks' computational cost and a lack of understanding of how the brain wires its biological networks.
In 2003, LSTM became competitive with traditional speech recognizers on certain tasks. In 2006, Alex Graves, Santiago Fernández, Faustino Gomez, and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTMs. In 2009, it became the first RNN to win a pattern recognition contest, in connected handwriting recognition.
In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh deep belief networks were developed for generative modeling. They are trained by training one restricted Boltzmann machine, then freezing it and training another one on top of the first one, and so on, then optionally fine-tuned using supervised backpropagation. They could model high-dimensional probability distributions, such as the distribution of MNIST images, but convergence was slow.
The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010.
The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different, offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009–2010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.
In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.
The deep learning revolution started around CNN- and GPU-based computer vision.
Although CNNs trained by backpropagation had been around for decades and GPU implementations of NNs for years, including CNNs, faster implementations of CNNs on GPUs were needed to progress on computer vision. Later, as deep learning becomes widespread, specialized hardware and algorithm optimizations were developed specifically for deep learning.
A key advance for the deep learning revolution was hardware advances, especially GPU. Some early work dated back to 2004. In 2009, Raina, Madhavan, and Andrew Ng reported a 100M deep belief network trained on 30 Nvidia GeForce GTX 280 GPUs, an early demonstration of GPU-based deep learning. They reported up to 70 times faster training.
In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. It then won more contests. They also showed how max-pooling CNNs on GPU improved performance significantly.
In 2012, Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos.
In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3.
The success in image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.
In 2014, the state of the art was training “very deep neural network” with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the "degradation" problem. In 2015, two techniques were developed to train very deep networks: the Highway Network was published in May 2015, and the residual neural network (ResNet) in Dec 2015. ResNet behaves like an open-gated Highway Net.
Around the same time, deep learning started impacting the field of art. Early examples included Google DeepDream (2015), and neural style transfer (2015), both of which were based on pretrained image classification neural networks, such as VGG-19.
Generative adversarial network (GAN) by (Ian Goodfellow et al., 2014) (based on Jürgen Schmidhuber's principle of artificial curiosity)
became state of the art in generative modeling during 2014-2018 period. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes. Diffusion models (2015) eclipsed GANs in generative modeling since then, with systems such as DALL·E 2 (2022) and Stable Diffusion (2022).
In 2015, Google's speech recognition improved by 49% by an LSTM-based model, which they made available through Google Voice Search on smartphone.
Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks were superseded for ASR by LSTM. but are more successful in computer vision.
Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the 2018 Turing Award for "conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing".
Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as "cat" or "no cat" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.
An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.
Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.
The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.
Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, or playing "Go").
A deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers. There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions. These components as a whole function in a way that mimics functions of the human brain, and can be trained like any other ML algorithm.
For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, and complex DNN have many layers, hence the name "deep" networks.
DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives. The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network. For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.
Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.
DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or "weights", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights. That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.
Recurrent neural networks, in which data can flow in any direction, are used for applications such as language modeling. Long short-term memory is particularly effective for this use.
Convolutional neural networks (CNNs) are used in computer vision. CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).
As with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.
DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning or weight decay (

-regularization) or sparsity (

-regularization) can be applied during training to combat overfitting. Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies. Another interesting recent development is research into models of just enough complexity through an estimation of the intrinsic complexity of the task being modelled. This approach has been successfully applied for multivariate time series prediction tasks such as traffic prediction. Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.
DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.
Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.
Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method for training large-scale commercial cloud AI . OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.
Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones and cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. Cerebras Systems has also built a dedicated system to handle large deep learning models, the CS-2, based on the largest processor in the industry, the second-generation Wafer Scale Engine (WSE-2).
Atomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage.
In 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs).
In 2021, J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs, and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second, indicating the potential of integrated photonics in data-heavy AI applications.
Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn "Very Deep Learning" tasks that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates is competitive with traditional speech recognizers on certain tasks.
The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences. Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.
The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007, accelerated progress in eight major areas:
Scale-up/out and accelerated DNN training and decoding
Sequence discriminative training
Feature processing by deep models with solid understanding of the underlying mechanisms
Adaptation of DNNs and related deep models
Multi-task and transfer learning by DNNs and related deep models
CNNs and how to design them to best exploit domain knowledge of speech
RNN and its rich LSTM variants
Other types of deep models including tensor-based models and integrated deep generative/discriminative models.
More recent speech recognition models use Transformers or Temporal Convolution Networks with significant success and widespread applications. All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.
A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.
Deep learning-based image recognition has become "superhuman", producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs, and in 2014, with recognition of human faces.
Deep learning-trained vehicles now interpret 360° camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.
Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of
identifying the style period of a given painting
Neural Style Transfer – capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video
generating striking imagery based on random visual input fields.
Neural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling.
Other key techniques in this field are negative sampling and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN. Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures provide the best results for constituency parsing, sentiment analysis, information retrieval, spoken language understanding, machine translation, contextual entity linking, writing style recognition, named-entity recognition (token classification), text classification, and others.
Recent developments generalize word embedding to sentence embedding.
Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network. Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system "learns from millions of examples". It translates "whole sentences at a time, rather than pieces". Google Translate supports over one hundred languages. The network encodes the "semantics of the sentence rather than simply memorizing phrase-to-phrase translations". GT uses English as an intermediate between most language pairs.
A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects. Research has explored use of deep learning to predict the biomolecular targets, off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.
AtomNet is a deep learning system for structure-based rational drug design. AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus and multiple sclerosis.
In 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set. In 2019, generative neural networks were used to produce molecules that were validated experimentally all the way into mice.
Deep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value.
Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations. Multi-view deep learning has been applied for learning user preferences from multiple domains. The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.
An autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.
In medical informatics, deep learning was used to predict sleep quality based on data from wearables and predictions of health complications from electronic health record data.
Deep neural networks have shown unparalleled performance in predicting protein structure, according to the sequence of the amino acids that make it up. In 2020, AlphaFold, a deep-learning based system, achieved a level of accuracy significantly higher than all previous computational methods.
Deep neural networks can be used to estimate the entropy of a stochastic process and called Neural Joint Entropy Estimator (NJEE). Such an estimation provides insights on the effects of input random variables on an independent random variable. Practically, the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y, given input X. For example, in image classification tasks, the NJEE maps a vector of pixels' color values to probabilities over possible image classes. In practice, the probability distribution of Y is obtained by a Softmax layer with number of nodes that is equal to the alphabet size of Y. NJEE uses continuously differentiable activation functions, such that the conditions for the universal approximation theorem holds. It is shown that this method provides a strongly consistent estimator and outperforms other methods in case of large alphabet sizes.
Deep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement. Modern deep learning tools demonstrate the high accuracy of detecting various diseases and the helpfulness of their use by specialists to improve the diagnosis efficiency.
Finding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server. Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.
Deep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization. These applications include learning methods such as "Shrinkage Fields for Effective Image Restoration" which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.
Deep learning is being successfully applied to financial fraud detection, tax evasion detection, and anti-money laundering.
In November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.
The United States Department of Defense applied deep learning to train robots in new tasks through observation.
Physics informed neural networks have been used to solve partial differential equations in both forward and inverse problems in a data driven manner. One example is the reconstructing fluid flow governed by the Navier-Stokes equations. Using physics informed neural networks does not require the often expensive mesh generation that conventional CFD methods rely on.
Deep backward stochastic differential equation method is a numerical method that combines deep learning with Backward stochastic differential equation (BSDE). This method is particularly useful for solving high-dimensional problems in financial mathematics. By leveraging the powerful function approximation capabilities of deep neural networks, deep BSDE addresses the computational challenges faced by traditional numerical methods in high-dimensional settings. Specifically, traditional methods like finite difference methods or Monte Carlo simulations often struggle with the curse of dimensionality, where computational cost increases exponentially with the number of dimensions. Deep BSDE methods, however, employ deep neural networks to approximate solutions of high-dimensional partial differential equations (PDEs), effectively reducing the computational burden.
In addition, the integration of Physics-informed neural networks (PINNs) into the deep BSDE framework enhances its capability by embedding the underlying physical laws directly into the neural network architecture. This ensures that the solutions not only fit the data but also adhere to the governing stochastic differential equations. PINNs leverage the power of deep learning while respecting the constraints imposed by the physical models, resulting in more accurate and reliable solutions for financial mathematics problems.
Image reconstruction is the reconstruction of the underlying images from the image-related measurements. Several works showed the better and superior performance of the deep learning methods compared to analytical methods for various applications, e.g., spectral imaging and ultrasound imaging.
Traditional weather prediction systems solve a very complex system of partial differential equations. GraphCast is a deep learning based model, trained on a long history of weather data to predict how weather patterns change over time. It is able to predict weather conditions for up to 10 days globally, at a very detailed level, and in under a minute, with precision similar to state of the art systems.
An epigenetic clock is a biochemical test that can be used to measure age. Galkin et al. used deep neural networks to train an epigenetic aging clock of unprecedented accuracy using >6,000 blood samples. The clock uses information from 1000 CpG sites and predicts people with certain conditions older than healthy controls: IBD, frontotemporal dementia, ovarian cancer, obesity. The aging clock was planned to be released for public use in 2021 by an Insilico Medicine spinoff company Deep Longevity.
Deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s. These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, "...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature".
A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism. Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality. In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.
Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons and neural populations. Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels.
Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.
Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages.
In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.
As of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as "good job" and "bad job".
Deep learning has attracted both criticism and comment, in some cases from outside the field of computer science.
A main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear. (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.
In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's website.
Furthermore, some researchers have argued that standard loss functions and differentiable architectures in deep learning may limit the discovery of deeper causal or generative mechanisms. Building on Algorithmic information theory (AIT), Hernández-Orozco et al. (2021) proposed an algorithmic loss function to measure the discrepancy between predicted and observed system behavior. Their approach integrates AIT with Machine learning to formulate a framework for learning generative rules in non-differentiable spaces, bridging discrete algorithmic theory with continuous optimization techniques. This framework provides a new perspective on generalization and model interpretability by grounding learning dynamics in algorithmic complexity.
Some deep learning architectures display problematic behaviors, such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI).
As deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an "adversarial attack".
In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points, and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.
Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.
ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.
In 2016, another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address, and hypothesized that this could "serve as a stepping stone for further attacks (e.g., opening a web page hosting drive-by malware)".
In "data poisoning", false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery.
The deep learning systems that are trained using supervised learning often rely on data that is created or annotated by humans, or both. It has been argued that not only low-paid clickwork (such as on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such. The philosopher Rainer Mühlhoff distinguishes five types of "machinic capture" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game), (2) "trapping and tracking" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g. tagging faces on Facebook to obtain labeled facial images), (4) information mining (e.g. by leveraging quantified-self devices such as activity trackers) and (5) clickwork.
Applications of artificial intelligence
Comparison of deep learning software
Echo state network
List of artificial intelligence projects
Liquid state machine
List of datasets for machine-learning research
Scale space and deep learning
Topological deep learning In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.
Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets.
The modern version of the transformer was proposed in the 2017 paper "Attention Is All You Need" by researchers at Google. Transformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multimodal learning, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).
For many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens.
A key breakthrough was LSTM (1995), a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units. Neural networks using multiplicative units were later called sigma-pi networks or higher-order networks. LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers.
However, LSTM still used sequential processing, like most other RNNs. Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence.
Modern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window. The linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input. One of its two networks has "fast weights" or "dynamic links" (1981). A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries. This was later shown to be equivalent to the unnormalized linear Transformer.
The idea of encoder-decoder sequence transduction had been developed in the early 2010s; commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014.
A 380M-parameter model for machine translation uses two long short-term memories (LSTM). Its architecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of tokens. Similarly, another 130M-parameter model used gated recurrent units (GRU) instead of LSTM. Later research showed that GRUs are neither better nor worse than LSTMs for seq2seq.
These early seq2seq models had no attention mechanism, and the state vector is accessible only after the last word of the source text was processed. Although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved. This is because the input is processed sequentially by one recurrent network into a fixed-size output vector, which is then processed by another recurrent network into an output. If the input is long, then the output vector would not be able to contain all relevant information, degrading the output. As evidence, reversing the input sentence improved seq2seq translation.
The RNNsearch model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of the fixed-size output vector), allowing the model to process long-distance dependencies more easily. The name is because it "emulates searching through a source sentence during decoding a translation".
The relative performances were compared between global (that of RNNsearch) and local (sliding window) attention model architectures for machine translation, finding that mixed attention had higher quality than global attention, while local attention reduced translation time.
In 2016, Google Translate was revamped to Google Neural Machine Translation, which replaced the previous model based on statistical machine translation. The new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional LSTM. It took nine months to develop, and it outperformed the statistical approach, which took ten years to develop.
Seq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, which prevented them from being accelerated on GPUs. In 2016, decomposable attention applied a self-attention mechanism to feedforward networks, which are easy to parallelize, and achieved SOTA result in textual entailment with an order of magnitude fewer parameters than LSTMs. One of its authors, Jakob Uszkoreit, suspected that attention without recurrence would be sufficient for language translation, thus the title "attention is all you need". That hypothesis was against conventional wisdom at the time, and even his father Hans Uszkoreit, a well-known computational linguist, was skeptical. In the same year, self-attention (called intra-attention or intra-sentence attention) was proposed for LSTMs.
In 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the "Attention is all you need" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance. This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. Its parallelizability was an important factor to its widespread use in large neural networks.
Already in spring 2017, even before the "Attention is all you need" preprint was published, one of the co-authors applied the "decoder-only" variation of the architecture to generate fictitious Wikipedia articles. Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom.
In language modelling, ELMo (2018) was a bi-directional LSTM that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec. It was followed by BERT (2018), an encoder-only Transformer model. In 2019 October, Google started using BERT to process search queries. In 2020, Google Translate replaced the previous RNN-encoder–RNN-decoder model by a Transformer-encoder–RNN-decoder model.
Starting in 2018, the OpenAI GPT series of decoder-only Transformers became state of the art in natural language generation. In 2022, a chatbot based on GPT-3, ChatGPT, became unexpectedly popular, triggering a boom around large language models.
Since 2020, Transformers have been applied in modalities beyond text, including the vision transformer, speech recognition, robotics, and multimodal. The vision transformer, in turn, stimulated new developments in convolutional neural networks. Image and video generators like DALL-E (2021), Stable Diffusion 3 (2024), and Sora (2024), use Transformers to analyse input data (like text prompts) by breaking it down into "tokens" and then calculating the relevance between each token using self-attention, which helps the model understand the context and relationships within the data.
The plain transformer architecture had difficulty converging. In the original paper the authors recommended using learning rate warmup. That is, the learning rate should linearly scale up from 0 to maximal value for the first part of the training (usually recommended to be 2% of the total number of training steps), before decaying again.
A 2020 paper found that using layer normalization before (instead of after) multiheaded attention and feedforward layers stabilizes training, not requiring learning rate warmup.
Transformers typically are first pretrained by self-supervised learning on a large generic dataset, followed by supervised fine-tuning on a small task-specific dataset. The pretrain dataset is typically an unlabeled large corpus, such as The Pile. Tasks for pretraining and fine-tuning commonly include:
The T5 transformer report documents a large number of natural language pretraining tasks. Some examples are:
restoring or repairing incomplete or corrupted text. For example, the input, "Thank you ~~ me to your party ~~ week", might generate the output, "Thank you for inviting me to your party last week".
translation between natural languages (machine translation)
judging the pragmatic acceptability of natural language. For example, the following sentence might be judged "not acceptable", because even though it is syntactically well-formed, it is improbable in ordinary human usage: The course is jumping well.
Note that while each of these tasks is trivial or obvious for human native speakers of the language (or languages), they have typically proved challenging for previous generations of machine learning architecture.
In general, there are 3 classes of language modelling tasks: "masked", "autoregressive", and "prefixLM". These classes are independent of a specific modeling architecture such as Transformer, but they are often discussed in the context of Transformer.
In a masked task, one or more of the tokens is masked out, and the model would produce a probability distribution predicting what the masked-out tokens are based on the context. The loss function for the task is typically sum of log-perplexities for the masked-out tokens:
conditional on its context
=-\sum _\ln(t)
and the model is trained to minimize this loss function. The BERT series of models are trained for masked token prediction and another task.
In an autoregressive task, the entire sequence is masked at first, and the model produces a probability distribution for the first token. Then the first token is revealed and the model predicts the second token, and so on. The loss function for the task is still typically the same. The GPT series of models are trained by autoregressive tasks.
In a prefixLM task, the sequence is divided into two parts. The first part is presented as context, and the model predicts the first token of the second part. Then that would be revealed, and the model predicts the second token, and so on. The loss function for the task is still typically the same. The T5 series of models are trained by prefixLM tasks.
Note that "masked" as in "masked language modelling" is not "masked" as in "masked attention", and "prefixLM" (prefix language modeling) is not "prefixLM" (prefix language model).
All transformers have the same primary components:
Tokenizers, which convert text into tokens.
Embedding layer, which converts tokens and positions of the tokens into vector representations.
Transformer layers, which carry out repeated transformations on the vector representations, extracting more and more linguistic information. These consist of alternating attention and feedforward layers. There are two major types of transformer layers: encoder layers and decoder layers, with further variants.
Un-embedding layer, which converts the final vector representations back to a probability distribution over the tokens.
The following description follows exactly the Transformer as described in the original paper. There are variants, described in the following section.
By convention, we write all vectors as row vectors. This, for example, means that pushing a vector through a linear layer means multiplying it by a weight matrix on the right, as
As the Transformer architecture natively processes numerical data, not text, there must be a translation between text and tokens. A token is an integer that represents a character, or a short segment of characters. On the input side, the input text is parsed into a token sequence. Similarly, on the output side, the output tokens are parsed back to text. The module doing the conversion between texts and token sequences is a tokenizer.
The set of all tokens is the vocabulary of the tokenizer, and its size is the vocabulary size
. When faced with tokens outside the vocabulary, typically a special token is used, written as "[UNK]" for "unknown".
Some commonly used tokenizers are byte pair encoding, WordPiece, and SentencePiece.
Each token is converted into an embedding vector via a lookup table. Equivalently stated, it multiplies a one-hot representation of the token by an embedding matrix
. For example, if the input token is
, then the one-hot representation is

, and its embedding vector is
 (3)=[0,0,0,1,0,0,\dots ]M
The token embedding vectors are added to their respective positional encoding vectors (see below), producing the sequence of input vectors.
The number of dimensions in an embedding vector is called hidden size or embedding size and written as
. This size is written as
in the original Transformer paper.
An un-embedding layer is almost the reverse of an embedding layer. Whereas an embedding layer converts a token into a vector, an un-embedding layer converts a vector into a probability distribution over tokens.
The un-embedding layer is a linear-softmax layer:
 (x)=\mathrm  (xW+b)
The matrix has shape
. The embedding matrix
and the un-embedding matrix
are sometimes required to be transposes of each other, a practice called weight tying.
A positional encoding is a fixed-size vector representation of the relative positions of tokens within a sequence: it provides the transformer model with information about where the words are in the input sequence. This induces a bias towards the order of the input sequence, so that, for example, the input sequence "man bites dog" is processed differently from "dog bites man".
The positional encoding is defined as a function of type
 \to \mathbb  ^;d\in \mathbb  ,d>0
is a positive even integer. The full positional encoding defined in the original paper is:
,f(t)_)=(\sin(\theta ),\cos(\theta ))\quad \forall k\in \
,r=N^
is a free parameter that should be significantly larger than the biggest
that would be input into the positional encoding function. The original paper uses
The function is in a simpler form when written as a complex function of type
 \to \mathbb  ^
\right)_-1
The main reason for using this positional encoding function is that using it, shifts are linear transformations:
 (f(\Delta t))f(t)
 
is the distance one wishes to shift. This allows the transformer to take any encoded position, and find the encoding of the position n-steps-ahead or n-steps-behind, by a matrix multiplication.
By taking a linear sum, any convolution can also be implemented as linear transformations:
c_f(t+\Delta t_)=\left(\sum _c_\,\mathrm  (f(\Delta t_))\right)f(t)
for any constants
. This allows the transformer to take any encoded position and find a linear sum of the encoded locations of its neighbors. This sum of encoded positions, when fed into the attention mechanism, would create attention weights on its neighbors, much like what happens in a convolutional neural network language model. In the author's words, "we hypothesized it would allow the model to easily learn to attend by relative position."
In typical implementations, all operations are done over the real numbers, not the complex numbers, but since complex multiplication can be implemented as real 2-by-2 matrix multiplication, this is a mere notational difference.
Like earlier seq2seq models, the original transformer model used an encoder-decoder architecture. The encoder consists of encoding layers that process all the input tokens together one layer after another, while the decoder consists of decoding layers that iteratively process the encoder's output and the decoder's output tokens so far.
The purpose of each encoder layer is to create contextualized representations of the tokens, where each representation corresponds to a token that "mixes" information from other input tokens via self-attention mechanism. Each decoder layer contains two attention sublayers: (1) cross-attention for incorporating the output of encoder (contextualized input token representations), and (2) self-attention for "mixing" information among the input tokens to the decoder (i.e. the tokens generated so far during inference time).
Both the encoder and decoder layers have a feed-forward neural network for additional processing of their outputs and contain residual connections and layer normalization steps. These feed-forward layers contain most of the parameters in a Transformer model.
The feedforward network (FFN) modules in a Transformer are 2-layered multilayer perceptrons:
 (x)=\phi (xW^+b^)W^+b^
are weight matrices and
are bias vectors, and

is its activation function. The original Transformer used ReLU activation.
The number of neurons in the middle layer is called intermediate size (GPT), filter size (BERT), or feedforward size (BERT). It is typically larger than the embedding size. For example, in both GPT-2 series and BERT series, the intermediate size of a model is 4 times its embedding size:
The attention mechanism used in the Transformer architecture are scaled dot-product attention units. For each unit, the transformer model learns three weight matrices: the query weights
, the key weights
, and the value weights
The module takes three sequences, a query sequence, a key sequence, and a value sequence. The query sequence is a sequence of length

, and each entry is a vector of dimension

. Similarly for the key and value sequences.
For each vector
in the query sequence, it is multiplied by a matrix
to produce a query vector
. The matrix of all query vectors is the query matrix:
Similarly, we construct the key matrix
and the value matrix
It is usually the case that all
are square matrices, meaning
=d_
Attention weights are calculated using the query and key vectors: the attention weight
is the dot product between
. The attention weights are divided by the square root of the dimension of the key vectors,

, which stabilizes gradients during training, and passed through a softmax which normalizes the weights. The fact that
are different matrices allows attention to be non-symmetric: if token
attends to token
\cdot k_
is large), this does not necessarily mean that token
will attend to token
\cdot k_
could be small). The output of the attention unit for token
is the weighted sum of the value vectors of all tokens, weighted by
, the attention from token
to each token.
The attention calculation for all tokens can be expressed as one large matrix calculation using the softmax function, which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations. The matrices
are defined as the matrices where the
th rows are vectors
respectively. Then we can represent the attention as
(Q,K,V)=\left( \right)V\end
where the softmax is applied over each of the rows of the matrix.
The number of dimensions in a query vector is query size
and similarly for the key size
and value size
. The output dimension of an attention head is its head dimension
. The attention mechanism requires the following three equalities to hold:
=\ell _,\;d_=d_,\;d_=d_
but is otherwise unconstrained.
If the attention head is used in a self-attention fashion, then
. If the attention head is used in a cross-attention fashion, then usually
\neq X_=X_
. It is theoretically possible for all three to be different, but that is rarely the case in practice.
One set of
matrices is called an attention head, and each layer in a transformer model has multiple attention heads. While each attention head attends to the tokens that are relevant to each token, multiple attention heads allow the model to do this for different definitions of "relevance". Specifically, the query and key projection matrices,
, which are involved in the attention score computation, defines the "relevance". Meanwhile, the value projection matrix
, in combination with the part of the output projection matrix
, determines how the attended tokens influence what information is passed to subsequent layers and ultimately the output logits. In addition, the scope of attention, or the range of token relationships captured by each attention head, can expand as tokens pass through successive layers. This allows the model to capture more complex and long-range dependencies in deeper layers. Many transformer attention heads encode relevance relations that are meaningful to humans. For example, some attention heads can attend mostly to the next word, while others mainly attend from verbs to their direct objects. The computations for each attention head can be performed in parallel, which allows for fast processing. The outputs for the attention layer are concatenated to pass into the feed-forward neural network layers.
Concretely, let the multiple attention heads be indexed by
, then we have
(Q,K,V)=_]((XW_^,XW_^,XW_^))W^
where the matrix
is the concatenation of word embeddings, and the matrices
are "projection matrices" owned by individual attention head
is a final projection matrix owned by the whole multi-headed attention head.
It is theoretically possible for each attention head to have a different head dimension
, but that is rarely the case in practice.
As an example, in the smallest GPT-2 model, there are only self-attention mechanisms. It has the following dimensions:

, its output projection matrix
\in \mathbb  ^
is a square matrix.
The Transformer architecture is constructed to calculate output tokens iteratively. Assuming
refers to the calculation of the first output token
, for step
, the output token
shall remain constant. This ensures properties of the model similar to autoregressive models. Therefore, at every time step
, the calculation for all outputs
should not have access to tokens at position
(as it naturally is the case for time step
, when tokens
are not yet calculated). This behavior may be accomplished before the softmax stage by adding a mask matrix

at entries where the attention link must be cut, and
at other places:
(Q,K,V)=\left(M+ \right)V\end
The following matrix is commonly used in decoder self-attention modules, called "causal masking":
=0&-\infty &-\infty &\dots &-\infty \\0&0&-\infty &\dots &-\infty \\0&0&0&\dots &-\infty \\\vdots &\vdots &\vdots &\ddots &\vdots \\0&0&0&\dots &0\end
In words, it means that each token can pay attention to itself, and every token before it, but not any after it. A non-masked attention module can be thought of as a masked attention module where the mask has all entries zero. As an example of an uncommon use of mask matrix, the XLNet considers all masks of the form
is a random permutation matrix.
An encoder consists of an embedding layer, followed by multiple encoder layers.
Each encoder layer consists of two major components: a self-attention mechanism and a feed-forward layer. It takes an input as a sequence of input vectors, applies the self-attention mechanism, to produce an intermediate sequence of vectors, then applies the feed-forward layer for each vector individually. Schematically, we have:
given input vectors
combine them into a matrix
&h_,h_,\dots \\H&=h_\\h_\\\vdots \end\\(H)&=((H,H,H)_)\\((H,H,H)_)\\\vdots \end\\\end
stands for "feed-forward network". We can more succinctly write it as
with the implicit convention that the
is applied to each row of the matrix individually.
The encoder layers are stacked. The first encoder layer takes the sequence of input vectors from the embedding layer, producing a sequence of vectors. This sequence of vectors is processed by the second encoder, and so on. The output from the final encoder layer is then used by the decoder.
As the encoder processes the entire input all at once, every token can attend to every other token (all-to-all attention), so there is no need for causal masking.
A decoder consists of an embedding layer, followed by multiple decoder layers, followed by an un-embedding layer.
Each decoder consists of three major components: a causally masked self-attention mechanism, a cross-attention mechanism, and a feed-forward neural network. The decoder functions in a similar fashion to the encoder, but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders. This mechanism can also be called the encoder-decoder attention.
Like the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings. The transformer must not use the current or future output to predict an output, so the output sequence must be partially masked to prevent this reverse information flow. This allows for autoregressive text generation. For decoding, all-to-all attention is inappropriate, because a token cannot attend to tokens not yet generated. Thus, the self-attention module in the decoder is causally masked.
In contrast, the cross-attention mechanism attends to the output vectors of the encoder, which is computed before the decoder starts decoding. Consequently, there is no need for masking in the cross-attention mechanism.
Schematically, we have:
is the matrix with rows being the output vectors from the encoder.
The last decoder is followed by a final un-embedding layer. to produce the output probabilities over the vocabulary. Then, one of the tokens is sampled according to the probability, and the decoder can be run again to produce the next token, etc, autoregressively generating output text.
Many large language models, since they do not need to predict a whole new sequence from an input sequence, only use the encoder or decoder of the original transformer architecture. Early GPT models are decoder-only models trained to predict the next token in a sequence. BERT, another language model, only makes use of an encoder, and is trained to predict a randomly masked token in a sequence.
Each encoder layer contains 2 sublayers: the self-attention and the feedforward network. Each decoder layer contains 3 sublayers: the causally masked self-attention, the cross-attention, and the feedforward network.
The final points of detail are the residual connections and layer normalization (LayerNorm, or LN), which while conceptually unnecessary, are necessary for numerical stability and convergence.
The residual connection, which is introduced to avoid vanishing gradient issues and stabilize the training process, can be expressed as follows: y = F(x) + x. The expression indicates that an output y is the sum of the transformation of input x (F(x)) and the input itself (x). Adding the input x can preserve the input information and avoid issues when the gradient of F(x) is close to zero.
Similarly to how the feedforward network modules are applied individually to each vector, the LayerNorm is also applied individually to each vector.
There are two common conventions in use: the post-LN and the pre-LN convention. In the post-LN convention, the output of each sublayer is
 (x+\mathrm  (x))
 (x)
is the function implemented by the sublayer itself.
In the pre-LN convention, the output of each sublayer is
 (\mathrm  (x))
The original 2017 Transformer used the post-LN convention. It was difficult to train and required careful hyperparameter tuning and a "warm-up" in learning rate, where it starts small and gradually increases. The pre-LN convention, proposed several times in 2018, was found to be easier to train, requiring no warm-up, leading to faster convergence.
The following is the pseudocode for a standard pre-LN encoder-decoder Transformer, adapted from
input: Encoder input t_e
Decoder input t_d
output: Array of probability distributions, with shape (decoder vocabulary size x length(decoder output sequence))
/* encoder */
z_e ← encoder.tokenizer(t_e)
for each t in 1:length(z_e) do
z_e[t] ← encoder.embedding(z_e[t]) + encoder.positional_embedding(t)
for each l in 1:length(encoder.layers) do
layer ← encoder.layers[l]
/* first sublayer */
z_e_copy ← copy(z_e)
for each t in 1:length(z_e) do
z_e[t] ← layer.layer_norm(z_e[t])
z_e ← layer.multiheaded_attention(z_e, z_e, z_e)
for each t in 1:length(z_e) do
z_e[t] ← z_e[t] + z_e_copy[t]
/* second sublayer */
z_e_copy ← copy(z_e)
for each t in 1:length(z_e) do
z_e[t] ← layer.layer_norm(z_e[t])
z_e ← layer.feedforward(z_e)
for each t in 1:length(z_e) do
z_e[t] ← z_e[t] + z_e_copy[t]
for each t in 1:length(z_e) do
z_e[t] ← encoder.final_layer_norm(z_e[t])
/* decoder */
z_d ← decoder.tokenizer(t_d)
for each t in 1:length(z_d) do
z_d[t] ← decoder.embedding(z_d[t]) + decoder.positional_embedding(t)
for each l in 1:length(decoder.layers) do
layer ← decoder.layers[l]
/* first sublayer */
z_d_copy ← copy(z_d)
for each t in 1:length(z_d) do
z_d[t] ← layer.layer_norm(z_d[t])
z_d ← layer.masked_multiheaded_attention(z_d, z_d, z_d)
for each t in 1:length(z_d) do
z_d[t] ← z_d[t] + z_d_copy[t]
/* second sublayer */
z_d_copy ← copy(z_d)
for each t in 1:length(z_d) do
z_d[t] ← layer.layer_norm(z_d[t])
z_d ← layer.multiheaded_attention(z_d, z_e, z_e)
for each i in 1:length(z_d) do
z_d[t] ← z_d[t] + z_d_copy[t]
/* third sublayer */
z_d_copy ← copy(z_d)
for each t in 1:length(z_d) do
z_d[t] ← layer.layer_norm(z_d[t])
z_d ← layer.feedforward(z_d)
for each t in 1:length(z_d) do
z_d[t] ← z_d[t] + z_d_copy[t]
z_d ← decoder.final_layer_norm(z_d)
output_distributions ← []
for each t in 1:length(z_d) do
The Transformer architecture, being modular, allows variations. Several common variations are described here.
An "encoder-only" Transformer applies the encoder to map an input text into a sequence of vectors that represent the input text. This is usually used for text embedding and representation learning for downstream applications. BERT is encoder-only. They are less often used currently, as they were found to be not significantly better than training an encoder-decoder Transformer, then taking just the encoder.
A "decoder-only" Transformer is not literally decoder-only, since without an encoder, the cross-attention mechanism has nothing to attend to. Thus, the decoder layers in a decoder-only Transformer is composed of just two sublayers: the causally masked self-attention, and the feedforward network. This is usually used for text generation and instruction following. The models in the GPT series and Chinchilla series are decoder-only.
An "encoder-decoder" Transformer is generally the same as the original Transformer, with 2 sublayers per encoder layer and 3 sublayers per decoder layer, etc. They might have minor architectural improvements, such as alternative activation functions, changing the location of normalization, etc. This is also usually used for text generation and instruction following. The models in the T5 series are encoder-decoder.
A "prefixLM" (prefix language model) is a decoder-only architecture, but with prefix masking, which is different from causal masking. Specifically, it has mask of the form
=\mathbf  &-\infty \\\mathbf  &M_\end
where the first columns correspond to the "prefix", and the subsequent columns correspond to the autoregressively generated text based on the prefix. They resemble encoder-decoder models, but has less "sparsity". Such models are rarely used, though they are cited as theoretical possibilities and benchmarked comparisons.
There are also mixed seq2seq models. For example, in 2020, Google Translate replaced the previous RNN-encoder–RNN-decoder model by a Transformer-encoder–RNN-decoder model, on the argument that an RNN-decoder runs much faster than Transformer-decoder when run autoregressively.
The original transformer uses ReLU activation function. Other activation functions were developed. The Llama series and PaLM used SwiGLU; both GPT-1 and BERT used GELU.
Alternative activation functions are often used in combination with Gated Linear Units in the feedforward module.
The normalization used in the Transformer can be different from LayerNorm. One example is RMSNorm which is used in the Llama series. Other examples include CapsuleNorm ScaleNorm, or FixNorm.
Transformers may use other positional encoding methods than sinusoidal.
The original Transformer paper reported using a learned positional encoding, but finding it not superior to the sinusoidal one. Later, found that causal masking itself provides enough signal to a Transformer decoder that it can learn to implicitly perform absolute positional encoding without the positional encoding module.
RoPE (rotary positional embedding), is best explained by considering a list of 2-dimensional vectors
. Now pick some angle

. Then RoPE encoding is
x_^,x_^,m=\cos m\theta &-\sin m\theta \\\sin m\theta &\cos m\theta \endx_^\\x_^\\\end=x_^\cos m\theta -x_^\sin m\theta \\x_^\cos m\theta +x_^\sin m\theta \\\end
Equivalently, if we write the 2-dimensional vectors as complex numbers
, then RoPE encoding is just multiplication by an angle:
z_,m=e^z_
For a list of
-dimensional vectors, a RoPE encoder is defined by a sequence of angles
,...,\theta ^
. Then the RoPE encoding is applied to each pair of coordinates.
The benefit of RoPE is that the dot-product between two vectors depends on their relative location only:
x,m^y,n=x,m+k^y,n+k
for any integer
ALiBi (Attention with Linear Biases) is not a replacement for the positional encoder on the original transformer. Instead, it is an additional positional encoder that is directly plugged into the attention mechanism. Specifically, the ALiBi attention mechanism is
(Q,K,V)=\left( +sB\right)V\end
is a real number ("scalar"), and
is the linear bias matrix defined by
0&1&2&3&\cdots \\-1&0&1&2&\cdots \\-2&-1&0&1&\cdots \\-3&-2&-1&0&\cdots \\\vdots &\vdots &\vdots &\vdots &\ddots \\\end
in other words,
. The idea being that the linear bias matrix is a softened mask. Just as
represent full attention paid, and

represents no attention paid, the linear bias matrix increases attention paid in one direction and decreases attention paid in the other direction.
ALiBi allows pretraining on short context windows, then fine-tuning on longer context windows. Since it is directly plugged into the attention mechanism, it can be combined with any positional encoder that is plugged into the "bottom" of the entire network (which is where the sinusoidal encoder on the original transformer, as well as RoPE and many others, are located).
Relative Position Encodings is similar to ALiBi, but more generic:
(Q,K,V)=\left( +B\right)V\end
is a Toeplitz matrix, that is,
. This is contrasted with the original sinusoidal positional encoding, which is an "absolute positional encoding".
The transformer model has been implemented in standard deep learning frameworks such as TensorFlow and PyTorch. Transformers is a library produced by Hugging Face that supplies transformer-based architectures and pretrained models.
When an autoregressive transformer is used for inference, such as generating text, the query vector is different at each step, but the already-computed key and value vectors are always the same. The KV caching method saves the computed key and value vectors at each attention block, so that they are not recomputed at each new token. PagedAttention applies memory paging to KV caching.
If a transformer is used with a baked-in prompt, such as ["You are a customer support agent..."], then the key and value vectors can be computed for the prompt, and saved on disk. The saving in compute is significant when the model is used for many short interactions, such as in online chatbots.
FlashAttention is an algorithm that implements the transformer attention mechanism efficiently on a GPU. It is a communication-avoiding algorithm that performs matrix multiplications in blocks, such that each block fits within the cache of a GPU, and by careful management of the blocks it minimizes data copying between GPU caches (as data movement is slow). See the page on softmax for details.
An improved version, FlashAttention-2, was developed to cater to the rising demand for language models capable of handling longer context lengths. It offers enhancements in work partitioning and parallelism, enabling it to achieve up to 230 TFLOPs/s on A100 GPUs (FP16/BF16), a 2x speed increase over the original FlashAttention.
Key advancements in FlashAttention-2 include the reduction of non-matmul FLOPs, improved parallelism over the sequence length dimension, better work partitioning between GPU warps, and added support for head dimensions up to 256 and multi-query attention (MQA) and grouped-query attention (GQA).
Benchmarks revealed FlashAttention-2 to be up to 2x faster than FlashAttention and up to 9x faster than a standard attention implementation in PyTorch. Future developments include optimization for new hardware like H100 GPUs and new data types like FP8.
Multi-Query Attention changes the multiheaded attention mechanism. Whereas normally,
(Q,K,V)=_]\left((XW_^,XW_^,XW_^)\right)W^
with Multi-Query Attention, there is just one
(Q,K,V)=_]\left((XW_^,XW^,XW^)\right)W^
This has a neutral effect on model quality and training speed, but increases inference speed.
More generally, grouped-query attention (GQA) partitions attention heads into groups, each of which shares the key-value pair. MQA is GQA with one group, while standard multiheaded attention is GQA with the maximal number of groups.
Multihead Latent Attention (MLA) is a low-rank approximation to standard MHA. Specifically, each hidden vector, before entering the attention mechanism, is first projected to two low-dimensional spaces ("latent space"), one for query and one for key-value (KV vector). This design minimizes the KV cache, as only the low-dimensional KV vector needs to be cached.
Speculative decoding is a method to accelerate token decoding. Similarly to speculative execution in CPUs, future tokens are computed quickly, then verified. If the quickly computed tokens are incorrect, they are discarded and computed slowly.
The key factor in speculative decoding is that a Transformer decoder can verify faster than it can decode, in the following sense.
Suppose we have two transformer models like GPT-3 and GPT-3-small, both with a context window size of 512. To generate an entire context window autoregressively with greedy decoding with GPT-3, it must be run for 512 times, each time generating a token
, taking time
. However, if we had some educated guess for the values of these tokens, we could verify all of them in parallel, in one run of the model, by checking that each
is indeed the token with the largest log-likelihood in the
In speculative decoding, a smaller model or some other simple heuristic is used to generate a few speculative tokens that are subsequently verified by the larger model. For example, suppose we use GPT-3-small to generate four speculative tokens:
_,_,_,_
. This only takes
. These tokens are then run through the larger GPT-3 in one go. Suppose that
_
_
are verified by GPT-3 as what it would have picked, then those are kept, but
_
is not, so
_,_
are discarded, and GPT-3 is run on those. This would take
, which might be shorter than
For non-greedy decoding, similar ideas apply, except the speculative tokens are accepted or rejected stochastically, in a way that guarantees the final output distribution is the same as if speculative decoding was not used.
In Multi-Token Prediction, a single forward pass creates a final embedding vector, which then is un-embedded into a token probability. However, that vector can then be further processed by another Transformer block to predict the next token, and so on for arbitrarily many steps into the future. This trades off accuracy for speed, since each new token costs just one more Transformer block, rather than the entire stack.
Training transformer-based architectures can be expensive, especially for long inputs. Many methods have been developed to attempt to address the issue. In the image domain, Swin Transformer is an efficient architecture that performs attention inside shifting windows. In the audio domain, SepTr decouples the attention in time and frequency domains. Long Range Arena (2020) is a standard benchmark for comparing the behavior of transformer architectures over long inputs.
The standard attention graph is either all-to-all or causal, both of which scales as
is the number of tokens in a sequence.
Reformer (2020) reduces the computational load from

by using locality-sensitive hashing and reversible layers.
Sparse attention uses attention graphs that grows slower than
. For example, BigBird (2020) uses random small-world networks which grows as
Ordinary transformers require a memory size that is quadratic in the size of the context window. Attention-free transformers reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value.
Random Feature Attention (2021) uses Fourier random features:
[\cos \langle w_,x\rangle ,\sin \langle w_,x\rangle ,\cdots \cos \langle w_,x\rangle ,\sin \langle w_,x\rangle ]^
are independent samples from the normal distribution
I)
. This choice of parameters satisfy
 [\langle \varphi (x),\varphi (y)\rangle ]=e^
=\mathbb  [\langle e^/2\sigma ^\varphi (x),e^/2\sigma ^\varphi (y)\rangle ]\approx \langle e^/2\sigma ^\varphi (x),e^/2\sigma ^\varphi (y)\rangle 
Consequently, the one-headed attention, with one query, can be written as
(q,K,V)=\left( \right)V\approx \sum _e^\|^/2\sigma ^\varphi (k_)v_^\sum _e^\|^/2\sigma ^\varphi (k_)
^
. Similarly for multiple queries, and for multiheaded attention.
This approximation can be computed in linear time, as we can compute the matrix
)v_^
first, then multiply it with the query. In essence, we have managed to obtain a more precise version of
(Q,K,V)=\left( \right)V\approx Q(K^V/)
Performer (2022) uses the same Random Feature Attention, but
are first independently sampled from the normal distribution
I)
, then they are Gram-Schmidt processed.
Transformers can also be used/adapted for modalities (input or output) beyond just text, usually by finding a way to "tokenize" the modality.
Multimodal models can either be trained from scratch, or by finetuning. A 2022 study found that Transformers pretrained only on natural language can be finetuned on only 0.03% of parameters and become competitive with LSTMs on a variety of logical and visual tasks, demonstrating transfer learning. The LLaVA was a vision-language model composed of a language model (Vicuna-13B) and a vision model (ViT-L/14), connected by a linear layer. Only the linear layer is finetuned.
Vision transformers adapt the transformer to computer vision by breaking down input images as a series of patches, turning them into vectors, and treating them like tokens in a standard transformer.
Conformer and later Whisper follow the same pattern for speech recognition, first turning the speech signal into a spectrogram, which is then treated like an image, i.e. broken down into a series of patches, turned into vectors and treated like tokens in a standard transformer.
Perceivers are a variant of Transformers designed for multimodality.
For image generation, notable architectures are DALL-E 1 (2021), Parti (2022), Phenaki (2023), and Muse (2023). Unlike later models, DALL-E is not a diffusion model. Instead, it uses a decoder-only Transformer that autoregressively generates a text, followed by the token representation of an image, which is then converted by a variational autoencoder to an image. Parti is an encoder-decoder Transformer, where the encoder processes a text prompt, and the decoder generates a token representation of an image. Muse is an encoder-only Transformer that is trained to predict masked image tokens from unmasked image tokens. During generation, all input tokens are masked, and the highest-confidence predictions are included for the next iteration, until all tokens are predicted. Phenaki is a text-to-video model. It is a bidirectional masked transformer conditioned on pre-computed text tokens. The generated tokens are then decoded to a video.
The transformer has had great success in natural language processing (NLP). Many large language models such as GPT-2, GPT-3, GPT-4, Gemini, AlbertAGPT, Claude, BERT, Grok, XLNet, RoBERTa and ChatGPT demonstrate the ability of transformers to perform a wide variety of NLP-related subtasks and their related real-world applications, including:
time series prediction
named entity recognition (NER)
writing computer code based on requirements expressed in natural language.
Beyond traditional NLP, the transformer architecture has had success in other applications, such as:
biological sequence analysis
protein folding (such as AlphaFold)
evaluating chess board positions. Using static evaluation alone (that is, with no Minimax search) transformer achieved an Elo of 2895, putting it at grandmaster level.
seq2seq – Family of machine learning approaches
Perceiver – Variant of Transformer designed for multimodal data
Vision transformer – Machine learning model for vision processing
Large language model – Type of machine learning model
BERT (language model) – Series of language models developed by Google AI
Generative pre-trained transformer – Type of large language model
T5 (language model) – Series of large language models developed by Google AI "Deep Learning" is the fourth episode of the twenty-sixth season of the American animated television series South Park, and the 323rd episode of the series overall. Written and directed by Trey Parker, it premiered on March 8, 2023. The episode, which parodies the use of the artificial intelligence chatbot ChatGPT (which is credited as a co-writer for the episode) for text messages, centers upon fourth-grader Stan Marsh, who comes to rely on the software for writing both school essays and romantic texts to his girlfriend Wendy Testaburger, bringing him into conflict with her, his classmates, and school officials.
When fourth-grader Bebe Stevens extols the romantic texts written to her by Clyde Donovan, classmate Wendy Testaburger complains to her boyfriend, Stan Marsh, that his replies to her messages consist of merely a thumbs up. Clyde tells Stan about ChatGPT, an AI-based app he uses to write the texts, but cautions Stan not to tell anyone else about it. Stan relies on the app to text Wendy while engaging in other activities. Wendy is buoyed by Stan's more heartfelt texts, though he cannot truthfully answer her questions about them.
Stan and Clyde also use the app to write their school essays, as do their classmates Eric Cartman and Butters Stotch. Cartman complains that more students learning about it would cost them their advantage, and their teacher, Mr. Garrison, would learn they cheated. Meanwhile, Garrison laments to his partner, Rick, that he now has to work harder to grade and comment on his students' essays. Rick tells him about ChatGPT, but instead of realizing that his students used it for essays, Garrison realizes he can use it to grade them. He thanks Rick for his supportive replies to his texts.
School counselor Mr. Mackey informs Stan's class that a student used OpenAI technology for schoolwork. A "technician" dressed as a falconer arrives with his falcon Shadowbane to analyze the students' work and identify the cheater. Stan and Garrison confess to each other that they used ChatGPT. They rationalize that it is merely akin to having a good writer's assistant, but when Garrison learns this can be used for texting, he is angered to realize that Rick used it to text him. The technician reveals Shadowbane detected chatbot writing in Wendy's cell phone, though she denies using the app. Worrying he cannot think of a way out of this, Stan instructs ChatGPT to write a story that is resolved when he convinces everyone that it is okay that he lied about using the app, and that tech companies who monetize OpenAI should not determine the ethical limits of AI. The remainder of the episode depicts this story and that resolution. Stan decides that sometimes a thumbs up from a human is better than machine-generated lies, but when Clyde asks Stan how he pulled this off, Stan simply explains, "ChatGPT, dude."
In the closing credits, the writers of the episode are credited as both Trey Parker and ChatGPT.
Bubbleblabber contributor John Schwarz rated the episode a 7.5 out of 10, stating in his review, "One day we're going to look back on this episode like we do when we think of the many chimps that we've sent to outer space when testing space flight capabilities and marvel at how far we've come in web3 show business production. Trey Parker's genius is still quite evident in this week's episode, in just a few seasons, we may not even need that."
Max Nocerino with The Future of the Force gave the episode a B+ rating, conceding that while the episode was "brilliant", the show was not as "hysterically funny as it used to be", and cited this episode as example of that trend. Nocerino stated, "It just doesn't split my sides anymore. Perhaps like all things, nothing lasts forever. Yet, I will continue to watch and give this episode props for understanding the double-edged sword that is ChatGPT. One of South Park's strengths is that it has its finger on the pulse of current events. And knows how to rip them to shreds."
Cathal Gunning, reviewing the episode for Screen Rant, wrote that in keeping with South Park's proclivity for self-satire, the ending was "intentionally far too tidy and the scene ended the story way too slickly", but nonetheless effective and clever. In particular, Gunning stated, "When Stan used ChatGPT to end the episode's story, the resulting scene sounded very familiar since the sequence that was supposedly written by AI sounded like a lazy episode of South Park. From Cartman's insults to Stan's impassioned ending speech to Mr. Mackey's long-forgotten catchphrase, the final scenes of 'Deep Learning' leaned into tropes that South Park has used ad nauseam."
"Deep Learning" Full Episode at South Park Studios
"Deep Learning" at IMDb
Vainilavičius, Justinas (March 8, 2023). "ChatGPT, dude: viral chatbot makes it to 'South Park'". Cybernews. Archived from the original on March 8, 2023. Deep Learning Super Sampling (DLSS) is a suite of real-time deep learning image enhancement and upscaling technologies developed by Nvidia that are available in a number of video games. The goal of these technologies is to allow the majority of the graphics pipeline to run at a lower resolution for increased performance, and then infer a higher resolution image from this that approximates the same level of detail as if the image had been rendered at this higher resolution. This allows for higher graphical settings and/or frame rates for a given output resolution, depending on user preference.
All generations of DLSS are available on all RTX-branded cards from Nvidia in supported titles. However, the Frame Generation feature is only supported on 40 series GPUs or newer and Multi Frame Generation is only available on 50 series GPUs.
Nvidia advertised DLSS as a key feature of the GeForce 20 series cards when they launched in September 2018. At that time, the results were limited to a few video games, namely Battlefield V, or Metro Exodus, because the algorithm had to be trained specifically on each game on which it was applied and the results were usually not as good as simple resolution upscaling. In 2019, the video game Control shipped with real-time ray tracing and an improved version of DLSS, which did not use the Tensor Cores.
In April 2020, Nvidia advertised and shipped an improved version of DLSS named DLSS 2.0 with driver version 445.75. DLSS 2.0 was available for a few existing games including Control and Wolfenstein: Youngblood, and would later be added to many newly released games and game engines such as Unreal Engine and Unity. This time Nvidia said that it used the Tensor Cores again, and that the AI did not need to be trained specifically on each game. Despite sharing the DLSS branding, the two iterations of DLSS differ significantly and are not backwards-compatible.
In January 2025, Nvidia stated that there are over 540 games and apps supporting DLSS, and that over 80% of Nvidia RTX users activate DLSS.
In March 2025, there were more than 100 games that support DLSS 4, according to Nvidia. By May 2025, over 125 games supported DLSS 4.
The first video game console to use DLSS, the Nintendo Switch 2, was released on June 5, 2025.
When using DLSS, depending on the game, users have access to various quality presets in addition to the option to set the internally rendered, upscaled resolution manually:
The first iteration of DLSS is a predominantly spatial image upscaler with two stages, both relying on convolutional auto-encoder neural networks. The first step is an image enhancement network which uses the current frame and motion vectors to perform edge enhancement, and spatial anti-aliasing. The second stage is an image upscaling step which uses the single raw, low-resolution frame to upscale the image to the desired output resolution. Using just a single frame for upscaling means the neural network itself must generate a large amount of new information to produce the high resolution output, this can result in slight hallucinations such as leaves that differ in style to the source content.
The neural networks are trained on a per-game basis by generating a "perfect frame" using traditional supersampling to 64 samples per pixel, as well as the motion vectors for each frame. The data collected must be as comprehensive as possible, including as many levels, times of day, graphical settings, resolutions, etc. as possible. This data is also augmented using common augmentations such as rotations, colour changes, and random noise to help generalize the test data. Training is performed on Nvidia's Saturn V supercomputer.
This first iteration received a mixed response, with many criticizing the often soft appearance and artifacts in certain situations; likely a side effect of the limited data from only using a single frame input to the neural networks which could not be trained to perform optimally in all scenarios and edge-cases. Nvidia also demonstrated the ability for the auto-encoder networks to learn the ability to recreate depth-of-field and motion blur, although this functionality has never been included in a publicly released product.
DLSS 2.0 is a temporal anti-aliasing upsampling (TAAU) implementation, using data from previous frames extensively through sub-pixel jittering to resolve fine detail and reduce aliasing. The data DLSS 2.0 collects includes: the raw low-resolution input, motion vectors, depth buffers, and exposure / brightness information. It can also be used as a simpler TAA implementation where the image is rendered at 100% resolution, rather than being upsampled by DLSS, Nvidia brands this as DLAA (Deep Learning Anti-Aliasing).
TAA(U) is used in many modern video games and game engines; however, all previous implementations have used some form of manually written heuristics to prevent temporal artifacts such as ghosting and flickering. One example of this is neighborhood clamping which forcefully prevents samples collected in previous frames from deviating too much compared to nearby pixels in newer frames. This helps to identify and fix many temporal artifacts, but deliberately removing fine details in this way is analogous to applying a blur filter, and thus the final image can appear blurry when using this method.
DLSS 2.0 uses a convolutional auto-encoder neural network trained to identify and fix temporal artifacts, instead of manually programmed heuristics as mentioned above. Because of this, DLSS 2.0 can generally resolve detail better than other TAA and TAAU implementations, while also removing most temporal artifacts. This is why DLSS 2.0 can sometimes produce a sharper image than rendering at higher, or even native resolutions using traditional TAA. However, no temporal solution is perfect, and artifacts (ghosting in particular) are still visible in some scenarios when using DLSS 2.0.
Because temporal artifacts occur in most art styles and environments in broadly the same way, the neural network that powers DLSS 2.0 does not need to be retrained when being used in different games. Despite this, Nvidia does frequently ship new minor revisions of DLSS 2.0 with new titles, so this could suggest some minor training optimizations may be performed as games are released, although Nvidia does not provide changelogs for these minor revisions to confirm this. The main advancements compared to DLSS 1.0 include: Significantly improved detail retention, a generalized neural network that does not need to be re-trained per-game, and ~2x less overhead (~1–2 ms vs ~2–4 ms).
It should also be noted that forms of TAAU such as DLSS 2.0 are not upscalers in the same sense as techniques such as ESRGAN or DLSS 1.0, which attempt to create new information from a low-resolution source; instead, TAAU works to recover data from previous frames, rather than creating new data. In practice, this means low resolution textures in games will still appear low-resolution when using current TAAU techniques. This is why Nvidia recommends game developers use higher resolution textures than they would normally for a given rendering resolution by applying a mip-map bias when DLSS 2.0 is enabled.
Augments DLSS 2.0 by making use of motion interpolation. The DLSS Frame Generation algorithm takes two rendered frames from the rendering pipeline and generates a new frame that smoothly transitions between them. So for every frame rendered, one additional frame is generated. DLSS 3.0 makes use of a new generation Optical Flow Accelerator (OFA) included in Ada Lovelace generation RTX GPUs. The new OFA is faster and more accurate than the OFA already available in previous Turing and Ampere RTX GPUs. This results in DLSS 3.0 being exclusive for the RTX 40 Series. At release, DLSS 3.0 does not work for VR displays.
DLSS 3.5 adds Ray Reconstruction, replacing multiple denoising algorithms with a single AI model trained on five times more data than DLSS 3. Ray Reconstruction is available on all RTX GPUs and first targeted games with path tracing (aka "full ray tracing"), including Cyberpunk 2077's Phantom Liberty DLC, Portal with RTX, and Alan Wake 2.
The fourth generation of Deep Learning Super Sampling (DLSS) was unveiled alongside the GeForce RTX 50 series. DLSS 4 upscaling uses a new vision transformer-based model for enhanced image quality with reduced ghosting and greater image stability in motion compared to the previous convolutional neural network (CNN) model. DLSS 4 allows a greater number of frames to be generated and interpolated based on a single traditionally rendered frame. This form of frame generation called Multi Frame Generation is exclusive to the GeForce RTX 50 series while the GeForce RTX 40 series is limited to one interpolated frame per traditionally rendered frame. According to Nvidia, this technique will increase performance by up to 800% while retaining low latency with Nvidia Reflex. Nvidia claims that DLSS 4x Frame Generation model uses 30% less video memory with the example of Warhammer 40,000: Darktide using 400MB less memory at 4K resolution with Frame Generation enabled. Nvidia claims that 75 games will integrate DLSS 4 Multi Frame Generation at launch, including Alan Wake 2, Cyberpunk 2077, Indiana Jones and the Great Circle, and Star Wars Outlaws.
Users can manually replace the DLLs in games to support a newer version of DLSS. DLSS Swapper, an open source utility, can automatically do this for all installed games. Replacing DLL files can not add DLSS support or features to games that do not already implement them, though some mods can add frame generation support.
DLSS requires and applies its own anti-aliasing method. Thus, depending on the game and quality setting used, using DLSS may improve image quality even over native resolution rendering. It operates on similar principles to TAA. Like TAA, it uses information from past frames to produce the current frame. Unlike TAA, DLSS does not sample every pixel in every frame. Instead, it samples different pixels in different frames and uses pixels sampled in past frames to fill in the unsampled pixels in the current frame. DLSS uses machine learning to combine samples in the current frame and past frames, and it can be thought of as an advanced and superior TAA implementation made possible by the available tensor cores. Nvidia also offers Deep Learning Anti-Aliasing (DLAA), which provides the same AI-driven anti-aliasing DLSS uses, but without any upscaling or downscaling.
With the exception of the shader-core version implemented in Control, DLSS is only available on GeForce RTX 20, GeForce RTX 30, GeForce RTX 40, GeForce RTX 50, and Quadro RTX series of video cards, using dedicated AI accelerators called Tensor Cores. Tensor Cores are available since the Nvidia Volta GPU microarchitecture, which was first used on the Tesla V100 line of products. They are used for doing fused multiply-add (FMA) operations that are used extensively in neural network calculations for applying a large series of multiplications on weights, followed by the addition of a bias. Tensor cores can operate on FP16, INT8, INT4, and INT1 data types. Each core can do 1024 bits of FMA operations per clock, so 1024 INT1, 256 INT4, 128 INT8, and 64 FP16 operations per clock per tensor core, and most Turing GPUs have a few hundred tensor cores. The Tensor Cores use CUDA Warp-Level Primitives on 32 parallel threads to take advantage of their parallel architecture. A Warp is a set of 32 threads which are configured to execute the same instruction. Since Windows 10 version 1903, Microsoft Windows provided DirectML as one part of DirectX to support Tensor Cores.
Particularly in early versions of DLSS, users reported blurry frames. Andrew Edelsten, an employee at Nvidia, therefore commented on the problem in a blog post in 2019 and promised that they were working on improving the technology and clarified that the DLSS AI algorithm was mainly trained with 4K image material. That the use of DLSS leads to particularly blurred images at lower resolutions, such as Full HD, is due to the fact that the algorithm has far less image information available to calculate an appropriate image compared to higher resolutions like 4K.
The use of DLSS Frame Generation may lead to increased input latency, as well as visual artifacts. It has also been criticized that by implementing DLSS in their games, game developers no longer have an incentive to optimize them so that they also run smoothly in native resolution on modern PC hardware. For example, for the game Alan Wake 2 in 4K resolution at the highest graphics settings with ray tracing enabled, the use of DLSS in Performance mode is recommended even with graphics cards such as the Nvidia GeForce RTX 4080 in order to achieve 60 fps.
The transformer-based AI upscaling model introduced with DLSS 4 received moderate praise for its improved image quality with regard to increased stability, reduced ghosting, better anti-aliasing, and higher level of detail, as well as its backward compatability and higher training scalability regarding future improvements.
FidelityFX Super Resolution – competing technology from AMD
Intel XeSS – competing technology from Intel
PlayStation Spectral Super Resolution – similar technology from PlayStation
DLSS on the Nvidia developer website Mamba is a deep learning architecture focused on sequence modeling. It was developed by researchers from Carnegie Mellon University and Princeton University to address some limitations of transformer models, especially in processing long sequences. It is based on the Structured State Space sequence (S4) model.
To enable handling long data sequences, Mamba incorporates the Structured State Space Sequence model (S4). S4 can effectively and efficiently model long dependencies by combining continuous-time, recurrent, and convolutional models. These enable it to handle irregularly sampled data, unbounded context, and remain computationally efficient during training and inferencing.
Mamba introduces significant enhancements to S4, particularly in its treatment of time-variant operations. It adopts a unique selection mechanism that adapts structured state space model (SSM) parameters based on the input. This enables Mamba to selectively focus on relevant information within sequences, effectively filtering out less pertinent data. The model transitions from a time-invariant to a time-varying framework, which impacts both computation and efficiency.
Mamba employs a hardware-aware algorithm that exploits GPUs, by using kernel fusion, parallel scan, and recomputation. The implementation avoids materializing expanded states in memory-intensive layers, thereby improving performance and memory usage. The result is significantly more efficient in processing long sequences compared to transformers.
Additionally, Mamba simplifies its architecture by integrating the SSM design with MLP blocks, resulting in a homogeneous and streamlined structure, furthering the model's capability for general sequence modeling across data types that include language, audio, and genomics, while maintaining efficiency in both training and inference.
Selective-State-Spaces (SSM): The core of Mamba, SSMs are recurrent models that selectively process information based on the current input. This allows them to focus on relevant information and discard irrelevant data.
Simplified Architecture: Mamba replaces the complex attention and MLP blocks of Transformers with a single, unified SSM block. This aims to reduce computational complexity and improve inference speed.
Hardware-Aware Parallelism: Mamba utilizes a recurrent mode with a parallel algorithm specifically designed for hardware efficiency, potentially further enhancing its performance.
Operating on byte-sized tokens, transformers scale poorly as every token must "attend" to every other token leading to O(n2) scaling laws, as a result, Transformers opt to use subword tokenization to reduce the number of tokens in text, however, this leads to very large vocabulary tables and word embeddings.
This research investigates a novel approach to language modeling, MambaByte, which departs from the standard token-based methods. Unlike traditional models that rely on breaking text into discrete units, MambaByte directly processes raw byte sequences. This eliminates the need for tokenization, potentially offering several advantages:
Language Independence: Tokenization often relies on language-specific rules and vocabulary, limiting applicability across diverse languages. MambaByte's byte-level representation allows it to handle different languages without language-specific adaptations.
Removes the bias of subword tokenisation: where common subwords are overrepresented and rare or new words are underrepresented or split into less meaningful units. This can affect the model's understanding and generation capabilities, particularly for languages with rich morphology or tokens not well-represented in the training data.
Simplicity in Preprocessing: It simplifies the preprocessing pipeline by eliminating the need for complex tokenization and vocabulary management, reducing the preprocessing steps and potential errors.
Subword tokenisation introduces a number of quirks in LLMs, such as failure modes where LLMs can't spell words, reverse certain words, handle rare tokens, which are not present in byte-level tokenisation.
MoE Mamba represents a pioneering integration of the Mixture of Experts (MoE) technique with the Mamba architecture, enhancing the efficiency and scalability of State Space Models (SSMs) in language modeling. This model leverages the strengths of both MoE and SSMs, achieving significant gains in training efficiency—requiring 2.2 times fewer training steps than its predecessor, Mamba, while maintaining competitive performance. MoE Mamba showcases improved efficiency and effectiveness by combining selective state space modeling with expert-based processing, offering a promising avenue for future research in scaling SSMs to handle tens of billions of parameters. The model's design involves alternating Mamba and MoE layers, allowing it to efficiently integrate the entire sequence context and apply the most relevant expert for each token.
Vision Mamba (Vim) integrates SSMs with visual data processing, employing bidirectional Mamba blocks for visual sequence encoding. This method reduces the computational demands typically associated with self-attention in visual tasks. Tested on ImageNet classification, COCO object detection, and ADE20k semantic segmentation, Vim showcases enhanced performance and efficiency and is capable of handling high-resolution images with lower computational resources. This positions Vim as a scalable model for future advancements in visual representation learning.
Jamba is a novel architecture built on a hybrid transformer and mamba SSM architecture developed by AI21 Labs with 52 billion parameters, making it the largest Mamba-variant created so far. It has a context window of 256k tokens.
Mamba LLM represents a significant potential shift in large language model architecture, offering faster, more efficient, and scalable models.
Applications include language translation, content generation, long-form text analysis, audio, and speech processing.
Transformer (machine learning model)
Recurrent neural network  Bidirectional recurrent neural networks (BRNN) connect two hidden layers of opposite directions to the same output. With this form of generative deep learning, the output layer can get information from past (backwards) and future (forward) states simultaneously. Invented in 1997 by Schuster and Paliwal, BRNNs were introduced to increase the amount of input information available to the network. For example, multilayer perceptron (MLPs) and time delay neural network (TDNNs) have limitations on the input data flexibility, as they require their input data to be fixed. Standard recurrent neural network (RNNs) also have restrictions as the future input information cannot be reached from the current state. On the contrary, BRNNs do not require their input data to be fixed. Moreover, their future input information is reachable from the current state.
BRNN are especially useful when the context of the input is needed. For example, in handwriting recognition, the performance can be enhanced by knowledge of the letters located before and after the current letter.
The principle of BRNN is to split the neurons of a regular RNN into two directions, one for positive time direction (forward states), and another for negative time direction (backward states). Those two states' output are not connected to inputs of the opposite direction states. The general structure of RNN and BRNN can be depicted in the right diagram. By using two time directions, input information from the past and future of the current time frame can be used unlike standard RNN which requires the delays for including future information.
BRNNs can be trained using similar algorithms to RNNs, because the two directional neurons do not have any interactions. However, when back-propagation through time is applied, additional processes are needed because updating input and output layers cannot be done at once. General procedures for training are as follows: For forward pass, forward states and backward states are passed first, then output neurons are passed. For backward pass, output neurons are passed first, then forward states and backward states are passed next. After forward and backward passes are done, the weights are updated.
Applications of BRNN include :
Speech Recognition (Combined with Long short-term memory)
Industrial Soft sensor
[1] Implementation of BRNN/LSTM in Python with Theano Long short-term memory (LSTM) is a type of recurrent neural network (RNN) aimed at mitigating the vanishing gradient problem commonly encountered by traditional RNNs. Its relative insensitivity to gap length is its advantage over other RNNs, hidden Markov models, and other sequence learning methods. It aims to provide a short-term memory for RNN that can last thousands of timesteps (thus "long short-term memory"). The name is made in analogy with long-term memory and short-term memory and their relationship, studied by cognitive psychologists since the early 20th century.
An LSTM unit is typically composed of a cell and three gates: an input gate, an output gate, and a forget gate. The cell remembers values over arbitrary time intervals, and the gates regulate the flow of information into and out of the cell. Forget gates decide what information to discard from the previous state, by mapping the previous state and the current input to a value between 0 and 1. A (rounded) value of 1 signifies retention of the information, and a value of 0 represents discarding. Input gates decide which pieces of new information to store in the current cell state, using the same system as forget gates. Output gates control which pieces of information in the current cell state to output, by assigning a value from 0 to 1 to the information, considering the previous and current states. Selectively outputting relevant information from the current state allows the LSTM network to maintain useful, long-term dependencies to make predictions, both in current and future time-steps.
LSTM has wide applications in classification, data processing, time series analysis tasks, speech recognition, machine translation, speech activity detection, robot control, video games, healthcare.
In theory, classic RNNs can keep track of arbitrary long-term dependencies in the input sequences. The problem with classic RNNs is computational (or practical) in nature: when training a classic RNN using back-propagation, the long-term gradients which are back-propagated can "vanish", meaning they can tend to zero due to very small numbers creeping into the computations, causing the model to effectively stop learning. RNNs using LSTM units partially solve the vanishing gradient problem, because LSTM units allow gradients to also flow with little to no attenuation. However, LSTM networks can still suffer from the exploding gradient problem.
The intuition behind the LSTM architecture is to create an additional module in a neural network that learns when to remember and when to forget pertinent information. In other words, the network effectively learns which information might be needed later on in a sequence and when that information is no longer needed. For instance, in the context of natural language processing, the network can learn grammatical dependencies. An LSTM might process the sentence "Dave, as a result of his controversial claims, is now a pariah" by remembering the (statistically likely) grammatical gender and number of the subject Dave, note that this information is pertinent for the pronoun his and note that this information is no longer important after the verb is.
In the equations below, the lowercase variables represent vectors. Matrices
contain, respectively, the weights of the input and recurrent connections, where the subscript
can either be the input gate
, output gate
, the forget gate
or the memory cell
, depending on the activation being calculated. In this section, we are thus using a "vector notation". So, for example,
\in \mathbb  ^
is not just one unit of one LSTM cell, but contains
LSTM cell's units.
See for an empirical study of 8 architectural variants of LSTM.
The compact forms of the equations for the forward pass of an LSTM cell with a forget gate are:
f_&=\sigma _(W_x_+U_h_+b_)\\i_&=\sigma _(W_x_+U_h_+b_)\\o_&=\sigma _(W_x_+U_h_+b_)\\_&=\sigma _(W_x_+U_h_+b_)\\c_&=f_\odot c_+i_\odot _\\h_&=o_\odot \sigma _(c_)\end
where the initial values are
and the operator

denotes the Hadamard product (element-wise product). The subscript
indexes the time step.
Letting the superscripts
refer to the number of input features and number of hidden units, respectively:
\in \mathbb  ^
: input vector to the LSTM unit
\in ^
: forget gate's activation vector
\in ^
: input/update gate's activation vector
\in ^
: output gate's activation vector
\in ^
: hidden state vector also known as output vector of the LSTM unit
_\in ^
: cell input activation vector
\in \mathbb  ^
: cell state vector
 ^
 ^
 ^
: weight matrices and bias vector parameters which need to be learned during training

: sigmoid function.

: hyperbolic tangent function.

: hyperbolic tangent function or, as the peephole LSTM paper suggests,
(x)=x
The figure on the right is a graphical representation of an LSTM unit with peephole connections (i.e. a peephole LSTM). Peephole connections allow the gates to access the constant error carousel (CEC), whose activation is the cell state.
is not used,
is used instead in most places.
f_&=\sigma _(W_x_+U_c_+b_)\\i_&=\sigma _(W_x_+U_c_+b_)\\o_&=\sigma _(W_x_+U_c_+b_)\\c_&=f_\odot c_+i_\odot \sigma _(W_x_+b_)\\h_&=o_\odot \sigma _(c_)\end
Each of the gates can be thought as a "standard" neuron in a feed-forward (or multi-layer) neural network: that is, they compute an activation (using an activation function) of a weighted sum.
represent the activations of respectively the input, output and forget gates, at time step
The 3 exit arrows from the memory cell
to the 3 gates
represent the peephole connections. These peephole connections actually denote the contributions of the activation of the memory cell
at time step
, i.e. the contribution of
, as the picture may suggest). In other words, the gates
calculate their activations at time step
) also considering the activation of the memory cell
at time step
The single left-to-right arrow exiting the memory cell is not a peephole connection and denotes
The little circles containing a

symbol represent an element-wise multiplication between its inputs. The big circles containing an S-like curve represent the application of a differentiable function (like the sigmoid function) to a weighted sum.
Peephole convolutional LSTM. The
denotes the convolution operator.
f_&=\sigma _(W_*x_+U_*h_+V_\odot c_+b_)\\i_&=\sigma _(W_*x_+U_*h_+V_\odot c_+b_)\\c_&=f_\odot c_+i_\odot \sigma _(W_*x_+U_*h_+b_)\\o_&=\sigma _(W_*x_+U_*h_+V_\odot c_+b_)\\h_&=o_\odot \sigma _(c_)\end
An RNN using LSTM units can be trained in a supervised fashion on a set of training sequences, using an optimization algorithm like gradient descent combined with backpropagation through time to compute the gradients needed during the optimization process, in order to change each weight of the LSTM network in proportion to the derivative of the error (at the output layer of the LSTM network) with respect to corresponding weight.
A problem with using gradient descent for standard RNNs is that error gradients vanish exponentially quickly with the size of the time lag between important events. This is due to
W^=0
if the spectral radius of
is smaller than 1.
However, with LSTM units, when error values are back-propagated from the output layer, the error remains in the LSTM unit's cell. This "error carousel" continuously feeds error back to each of the LSTM unit's gates, until they learn to cut off the value.
Many applications use stacks of LSTM RNNs and train them by connectionist temporal classification (CTC) to find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.
Sometimes, it can be advantageous to train (parts of) an LSTM by neuroevolution or by policy gradient methods, especially when there is no "teacher" (that is, training labels).
Applications of LSTM include:
2015: Google started using an LSTM trained by CTC for speech recognition on Google Voice. According to the official blog post, the new model cut transcription errors by 49%.
2016: Google started using an LSTM to suggest messages in the Allo conversation app. In the same year, Google released the Google Neural Machine Translation system for Google Translate which used LSTMs to reduce translation errors by 60%.
Apple announced in its Worldwide Developers Conference that it would start using the LSTM for quicktype in the iPhone and for Siri.
Amazon released Polly, which generates the voices behind Alexa, using a bidirectional LSTM for the text-to-speech technology.
2017: Facebook performed some 4.5 billion automatic translations every day using long short-term memory networks.
Microsoft reported reaching 94.9% recognition accuracy on the Switchboard corpus, incorporating a vocabulary of 165,000 words. The approach used "dialog session-based long-short-term memory".
2018: OpenAI used LSTM trained by policy gradients to beat humans in the complex video game of Dota 2, and to control a human-like robot hand that manipulates physical objects with unprecedented dexterity.
2019: DeepMind used LSTM trained by policy gradients to excel at the complex video game of Starcraft II.
Aspects of LSTM were anticipated by "focused back-propagation" (Mozer, 1989), cited by the LSTM paper.
Sepp Hochreiter's 1991 German diploma thesis analyzed the vanishing gradient problem and developed principles of the method. His supervisor, Jürgen Schmidhuber, considered the thesis highly significant.
An early version of LSTM was published in 1995 in a technical report by Sepp Hochreiter and Jürgen Schmidhuber, then published in the NIPS 1996 conference.
The most commonly used reference point for LSTM was published in 1997 in the journal Neural Computation. By introducing Constant Error Carousel (CEC) units, LSTM deals with the vanishing gradient problem. The initial version of LSTM block included cells, input and output gates.
(Felix Gers, Jürgen Schmidhuber, and Fred Cummins, 1999) introduced the forget gate (also called "keep gate") into the LSTM architecture in 1999, enabling the LSTM to reset its own state. This is the most commonly used version of LSTM nowadays.
(Gers, Schmidhuber, and Cummins, 2000) added peephole connections. Additionally, the output activation function was omitted.
(Graves, Fernandez, Gomez, and Schmidhuber, 2006) introduce a new error function for LSTM: Connectionist Temporal Classification (CTC) for simultaneous alignment and recognition of sequences.
(Graves, Schmidhuber, 2005) published LSTM with full backpropagation through time and bidirectional LSTM.
(Kyunghyun Cho et al., 2014) published a simplified variant of the forget gate LSTM called Gated recurrent unit (GRU).
(Rupesh Kumar Srivastava, Klaus Greff, and Schmidhuber, 2015) used LSTM principles to create the Highway network, a feedforward neural network with hundreds of layers, much deeper than previous networks. Concurrently, the ResNet architecture was developed. It is equivalent to an open-gated or gateless highway network.
A modern upgrade of LSTM called xLSTM is published by a team led by Sepp Hochreiter (Maximilian et al, 2024). One of the 2 blocks (mLSTM) of the architecture are parallelizable like the Transformer architecture, the other ones (sLSTM) allow state tracking.
2001: Gers and Schmidhuber trained LSTM to learn languages unlearnable by traditional models such as Hidden Markov Models.
Hochreiter et al. used LSTM for meta-learning (i.e. learning a learning algorithm).
2004: First successful application of LSTM to speech Alex Graves et al.
2005: Daan Wierstra, Faustino Gomez, and Schmidhuber trained LSTM by neuroevolution without a teacher.
Mayer et al. trained LSTM to control robots.
2007: Wierstra, Foerster, Peters, and Schmidhuber trained LSTM by policy gradients for reinforcement learning without a teacher.
Hochreiter, Heuesel, and Obermayr applied LSTM to protein homology detection the field of biology.
2009: Justin Bayer et al. introduced neural architecture search for LSTM.
2009: An LSTM trained by CTC won the ICDAR connected handwriting recognition competition. Three such models were submitted by a team led by Alex Graves. One was the most accurate model in the competition and another was the fastest. This was the first time an RNN won international competitions.
2013: Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton used LSTM networks as a major component of a network that achieved a record 17.7% phoneme error rate on the classic TIMIT natural speech dataset.
2017: Researchers from Michigan State University, IBM Research, and Cornell University published a study in the Knowledge Discovery and Data Mining (KDD) conference. Their time-aware LSTM (T-LSTM) performs better on certain data sets than standard LSTM.
Monner, Derek D.; Reggia, James A. (2010). "A generalized LSTM-like training algorithm for second-order recurrent neural networks" (PDF). Neural Networks. 25 (1): 70–83. doi:10.1016/j.neunet.2011.07.003. PMC 3217173. PMID 21803542. High-performing extension of LSTM that has been simplified to a single node type and can train arbitrary architectures
Gers, Felix A.; Schraudolph, Nicol N.; Schmidhuber, Jürgen (Aug 2002). "Learning precise timing with LSTM recurrent networks" (PDF). Journal of Machine Learning Research. 3: 115–143.
Gers, Felix (2001). "Long Short-Term Memory in Recurrent Neural Networks" (PDF). PhD thesis.
Abidogun, Olusola Adeniyi (2005). Data Mining, Fraud Detection and Mobile Telecommunications: Call Pattern Analysis with Unsupervised Neural Networks. Master's Thesis (Thesis). University of the Western Cape. hdl:11394/249. Archived (PDF) from the original on May 22, 2012.
original with two chapters devoted to explaining recurrent neural networks, especially LSTM.
Recurrent Neural Networks with over 30 LSTM papers by Jürgen Schmidhuber's group at IDSIA
Zhang, Aston; Lipton, Zachary; Li, Mu; Smola, Alexander J. (2024). "10.1. Long Short-Term Memory (LSTM)". Dive into deep learning. Cambridge New York Port Melbourne New Delhi Singapore: Cambridge University Press. ISBN 978-1-009-38943-3. Time-aware LSTM (T-LSTM) is a long short-term memory (LSTM) unit capable of handling irregular time intervals in longitudinal patient records. T-LSTM was developed by researchers from Michigan State University, IBM Research, and Cornell University and was first presented in the Knowledge Discovery and Data Mining (KDD) conference.
Experiments using real and synthetic data proved that T-LSTM auto-encoder outperformed widely used frameworks including LSTM and MF1-LSTM auto-encoders.  Natural language processing (NLP) is the processing of natural language information by a computer. The study of NLP, a subfield of computer science, is generally associated with artificial intelligence. NLP is related to information retrieval, knowledge representation, computational linguistics, and more broadly with linguistics.
Major processing tasks in an NLP system include: speech recognition, text classification, natural language understanding, and natural language generation.
Natural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled "Computing Machinery and Intelligence" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem. However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe) until the late 1980s when the first statistical machine translation systems were developed.
1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted "blocks worlds" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the "patient" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to "My head hurts" with "Why do you say your head hurts?". Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer memory at the time.
1970s: During the 1970s, many programmers began to write "conceptual ontologies", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, the first chatterbots were written (e.g., PARRY).
1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar), morphology (e.g., two-level morphology), semantics (e.g., Lesk algorithm), reference (e.g., within Centering Theory) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.
Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.
1990s: Many of the notable early successes in statistical methods in NLP occurred in the field of machine translation, due especially to work at IBM Research, such as IBM alignment models. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.
2000s: With the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s. Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the worse efficiency if the algorithm used has a low enough time complexity to be practical.
2003: word n-gram model, at the time the best statistical algorithm, is outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words, trained on up to 14 million words, by Bengio et al.)
2010: Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling, and in the following years he went on to develop Word2vec. In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling and parsing. This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care or protect patient privacy.
Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: such as by writing grammars or devising heuristic rules for stemming.
Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach:
both statistical and neural networks methods can focus more on the most common cases extracted from a corpus of texts, whereas the rule-based approach needs to provide rules for both rare cases and common ones equally.
language models, produced by either statistical or neural networks methods, are more robust to both unfamiliar (e.g. containing words or structures that have not been seen before) and erroneous input (e.g. with misspelled words or words accidentally omitted) in comparison to the rule-based systems, which are also more costly to produce.
the larger such a (probabilistic) language model is, the more accurate it becomes, in contrast to rule-based systems that can gain accuracy only by increasing the amount and complexity of the rules leading to intractability problems.
Rule-based systems are commonly used:
when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system,
for preprocessing in NLP pipelines, e.g., tokenization, or
for postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses.
In the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, which was caused by the inefficiencies of the rule-based approaches.
The earliest decision trees, producing systems of hard if–then rules, were still very similar to the old rule-based approaches.
Only the introduction of hidden Markov models, applied to part-of-speech tagging, announced the end of the old rule-based approach.
A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015, the statistical approach has been replaced by the neural networks approach, using semantic networks and word embeddings to capture semantic properties of words.
Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore.
Neural machine translation, based on then-newly invented sequence-to-sequence transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for statistical machine translation.
The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
Optical character recognition (OCR)
Given an image representing printed text, determine the corresponding text.
Given a sound clip of a person or people speaking, determine the textual representation of the speech. This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed "AI-complete" (see above). In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process. Also, given that words in the same language are spoken by people with different accents, the speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent.
Given a sound clip of a person or people speaking, separate it into words. A subtask of speech recognition and typically grouped with it.
Given a text, transform those units and produce a spoken representation. Text-to-speech can be used to aid the visually impaired.
Word segmentation (Tokenization)
Tokenization is a process used in text analysis that divides text into individual words or word fragments. This technique results in two key components: a word index and tokenized text. The word index is a list that maps unique words to specific numerical identifiers, and the tokenized text replaces each word with its corresponding numerical token. These numerical tokens are then used in various deep learning methods.
For a language like English, this is fairly trivial, since words are usually separated by spaces. However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language. Sometimes this process is also used in cases like bag of words (BOW) creation in data mining.
The task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma. Lemmatization is another technique for reducing words to their normalized form. But in this case, the transformation actually uses a dictionary to map words to their actual form.
Separate words into individual morphemes and identify the class of the morphemes. The difficulty of this task depends greatly on the complexity of the morphology (i.e., the structure of words) of the language being considered. English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g., "open, opens, opened, opening") as separate words. In languages such as Turkish or Meitei, a highly agglutinated Indian language, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms.
Given a sentence, determine the part of speech (POS) for each word. Many words, especially common ones, can serve as multiple parts of speech. For example, "book" can be a noun ("the book on the table") or verb ("to book a flight"); "set" can be a noun, verb or adjective; and "out" can be any of at least five different parts of speech.
The process of reducing inflected (or sometimes derived) words to a base form (e.g., "close" will be the root for "closed", "closing", "close", "closer" etc.). Stemming yields similar results as lemmatization, but does so on grounds of rules, not a dictionary.
Generate a formal grammar that describes a language's syntax.
Sentence breaking (also known as "sentence boundary disambiguation")
Given a chunk of text, find the sentence boundaries. Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g., marking abbreviations).
Determine the parse tree (grammatical analysis) of a given sentence. The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses: perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human). There are two primary types of parsing: dependency parsing and constituency parsing. Dependency parsing focuses on the relationships between words in a sentence (marking things like primary objects and predicates), whereas constituency parsing focuses on building out the parse tree using a probabilistic context-free grammar (PCFG) (see also stochastic grammar).
What is the computational meaning of individual words in context?
How can we learn semantic representations from data?
Named entity recognition (NER)
Given a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization). Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case, is often inaccurate or insufficient. For example, the first letter of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized. Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names. For example, German capitalizes all nouns, regardless of whether they are names, and French and Spanish do not capitalize names that serve as adjectives. Another name for this task is token classification.
Sentiment analysis (see also Multimodal sentiment analysis)
Sentiment analysis is a computational method used to identify and classify the emotional intent behind text. This technique involves analyzing text to determine whether the expressed sentiment is positive, negative, or neutral. Models for sentiment classification typically utilize inputs such as word n-grams, Term Frequency-Inverse Document Frequency (TF-IDF) features, hand-generated features, or employ deep learning models designed to recognize both long-term and short-term dependencies in text sequences. The applications of sentiment analysis are diverse, extending to tasks such as categorizing customer reviews on various online platforms.
The goal of terminology extraction is to automatically extract relevant terms from a given corpus.
Word-sense disambiguation (WSD)
Many words have more than one meaning; we have to select the meaning which makes the most sense in context. For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or an online resource such as WordNet.
Many words—typically proper names—refer to named entities; here we have to select the entity (a famous individual, a location, a company, etc.) which is referred to in context.
Given a chunk of text, identify the relationships among named entities (e.g. who is married to whom).
Given a piece of text (typically a sentence), produce a formal representation of its semantics, either as a graph (e.g., in AMR parsing) or in accordance with a logical formalism (e.g., in DRT parsing). This challenge typically includes aspects of several more elementary NLP tasks from semantics (e.g., semantic role labelling, word-sense disambiguation) and can be extended to include full-fledged discourse analysis (e.g., discourse analysis, coreference; see Natural language understanding below).
Semantic role labelling (see also implicit semantic role labelling below)
Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames), then identify and classify the frame elements (semantic roles).
Given a sentence or larger chunk of text, determine which words ("mentions") refer to the same objects ("entities"). Anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer. The more general task of coreference resolution also includes identifying so-called "bridging relationships" involving referring expressions. For example, in a sentence such as "He entered John's house through the front door", "the front door" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to).
This rubric includes several related tasks. One task is discourse parsing, i.e., identifying the discourse structure of a connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast). Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g. yes–no question, content question, statement, assertion, etc.).
Implicit semantic role labelling
Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames) and their explicit semantic roles in the current sentence (see Semantic role labelling above). Then, identify semantic roles that are not explicitly realized in the current sentence, classify them into arguments that are explicitly realized elsewhere in the text and those that are not specified, and resolve the former against the local text. A closely related task is zero anaphora resolution, i.e., the extension of coreference resolution to pro-drop languages.
Recognizing textual entailment
Given two text fragments, determine if one being true entails the other, entails the other's negation, or allows the other to be either true or false.
Topic segmentation and recognition
Given a chunk of text, separate it into segments each of which is devoted to a topic, and identify the topic of the segment.
The goal of argument mining is the automatic extraction and identification of argumentative structures from natural language text with the aid of computer programs. Such argumentative structures include the premise, conclusions, the argument scheme and the relationship between the main and subsidiary argument, or the main and counter-argument within discourse.
Automatic summarization (text summarization)
Produce a readable summary of a chunk of text. Often used to provide summaries of the text of a known type, such as research papers, articles in the financial section of a newspaper.
Grammatical error correction
Grammatical error detection and correction involves a great band-width of problems on all levels of linguistic analysis (phonology/orthography, morphology, syntax, semantics, pragmatics). Grammatical error correction is impactful since it affects hundreds of millions of people that use or acquire English as a second language. It has thus been subject to a number of shared tasks since 2011. As far as orthography, morphology, syntax and certain aspects of semantics are concerned, and due to the development of powerful neural language models such as GPT-2, this can now (2019) be considered a largely solved problem and is being marketed in various commercial applications.
Translate a text from a natural language into formal logic.
Machine translation (MT)
Automatically translate text from one human language to another. This is one of the most difficult problems, and is a member of a class of problems colloquially termed "AI-complete", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) to solve properly.
Natural language understanding (NLU)
Convert chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural language concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural language semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization.
Natural language generation (NLG):
Convert information from computer databases or semantic intents into readable human language.
Not an NLP task proper but an extension of natural language generation and other NLP tasks is the creation of full-fledged books. The first machine-generated book was created by a rule-based system in 1984 (Racter, The policeman's beard is half-constructed). The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantics-free) language models. The first machine-generated science book was published in 2019 (Beta Writer, Lithium-Ion Batteries, Springer, Cham). Unlike Racter and 1 the Road, this is grounded on factual knowledge and based on text summarization.
A Document AI platform sits on top of the NLP technology enabling users with no prior experience of artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they need from different document types. NLP-powered Document AI enables non-technical teams to quickly access information hidden in documents, for example, lawyers, business analysts and accountants.
Computer systems intended to converse with a human.
Given a human-language question, determine its answer. Typical questions have a specific right answer (such as "What is the capital of Canada?"), but sometimes open-ended questions are also considered (such as "What is the meaning of life?").
Given a description of an image, generate an image that matches the description.
Given a description of a scene, generate a 3D model of the scene.
Given a description of a video, generate a video that matches the description.
Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:
Interest on increasingly abstract, "cognitive" aspects of natural language (1999–2001: shallow parsing, 2002–03: named entity recognition, 2006–09/2017–18: dependency syntax, 2004–05/2008–09 semantic role labelling, 2011–12 coreference, 2015–16: discourse parsing, 2019: semantic parsing).
Increasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, Turkish since 2006; Basque, Catalan, Chinese, Greek, Hungarian, Italian, Turkish since 2007; Czech since 2009; Arabic since 2012; 2017: 40+ languages; 2018: 60+/100+ languages)
Elimination of symbolic representations (rule-based over supervised towards weakly supervised methods, representation learning and end-to-end systems)
Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
Cognition refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." Cognitive science is the interdisciplinary, scientific study of the mind and its processes. Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.
As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining aspects:
Apply the theory of conceptual metaphor, explained by Lakoff as "the understanding of one idea, in terms of another" which provides an idea of the intent of the author. For example, consider the English word big. When used in a comparison ("That is a big tree"), the author's intent is to imply that the tree is physically large relative to other trees or the authors experience. When used metaphorically ("Tomorrow is a big day"), the author's intent to imply importance. The intent behind other usages, like in "She is a big person", will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information.
Assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed, e.g., by means of a probabilistic context-free grammar (PCFG). The mathematical equation for such algorithms is presented in US Patent 9269353:
)=)\times \left(\sum _^)\times ,token_,token_))_\right)
RMM is the relative measure of meaning
token is any block of text, sentence, phrase or word
N is the number of tokens being analyzed
PMM is the probable measure of meaning based on a corpora
d is the non zero location of the token along the sequence of N tokens
PF is the probability function specific to a language
Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, functional grammar, construction grammar, computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of "cognitive AI". Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit) and developments in artificial intelligence, specifically tools and technologies using large language model approaches and new directions in artificial general intelligence based on the free energy principle by British neuroscientist and theoretician at University College London Karl J. Friston.
Media related to Natural language processing at Wikimedia Commons Quantum natural language processing (QNLP) is the application of quantum computing to natural language processing (NLP). It computes word embeddings as parameterised quantum circuits that can solve NLP tasks faster than any classical computer. It is inspired by categorical quantum mechanics and the DisCoCat framework, making use of string diagrams to translate from grammatical structure to quantum processes.
The first quantum algorithm for natural language processing used the DisCoCat framework and Grover's algorithm to show a quadratic quantum speedup for a text classification task. It was later shown that quantum language processing is BQP-Complete, i.e. quantum language models are more expressive than their classical counterpart, unless quantum mechanics can be efficiently simulated by classical computers.
These two theoretical results assume fault-tolerant quantum computation and a QRAM, i.e. an efficient way to load classical data on a quantum computer. Thus, they are not applicable to the noisy intermediate-scale quantum (NISQ) computers available today.
The algorithm of Zeng and Coecke was adapted to the constraints of NISQ computers and implemented on IBM quantum computers to solve binary classification tasks. Instead of loading classical word vectors onto a quantum memory, the word vectors are computed directly as the parameters of quantum circuits. These parameters are optimised using methods from quantum machine learning to solve data-driven tasks such as question answering, machine translation and even algorithmic music composition.
Categorical quantum mechanics
Natural language processing
Quantum machine learning
Applied category theory
DisCoPy, a Python toolkit for computing with string diagrams
lambeq, a Python library for quantum natural language processing The following outline is provided as an overview of and topical guide to natural-language processing:
natural-language processing – computer activity in which computers are entailed to analyze, understand, alter, or generate natural language. This includes the automation of any or all linguistic forms, activities, or methods of communication, such as conversation, correspondence, reading, written composition, dictation, publishing, translation, lip reading, and so on. Natural-language processing is also the name of the branch of computer science, artificial intelligence, and linguistics concerned with enabling computers to engage in communication using natural language(s) in all forms, including but not limited to speech, print, writing, and signing.
Natural-language processing can be described as all of the following:
A field of science – systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.
An applied science – field that applies human knowledge to build or design useful things.
A field of computer science – scientific and practical approach to computation and its applications.
A branch of artificial intelligence – intelligence of machines and robots and the branch of computer science that aims to create it.
A subfield of computational linguistics – interdisciplinary field dealing with the statistical or rule-based modeling of natural language from a computational perspective.
An application of engineering – science, skill, and profession of acquiring and applying scientific, economic, social, and practical knowledge, in order to design and also build structures, machines, devices, systems, materials and processes.
An application of software engineering – application of a systematic, disciplined, quantifiable approach to the design, development, operation, and maintenance of software, and the study of these approaches; that is, the application of engineering to software.
A subfield of computer programming – process of designing, writing, testing, debugging, and maintaining the source code of computer programs. This source code is written in one or more programming languages (such as Java, C++, C#, Python, etc.). The purpose of programming is to create a set of instructions that computers use to perform specific operations or to exhibit desired behaviors.
A subfield of artificial intelligence programming –
A type of system – set of interacting or interdependent components forming an integrated whole or a set of elements (often called 'components' ) and relationships which are different from relationships of the set or its elements to other elements or sets.
A system that includes software – software is a collection of computer programs and related data that provides the instructions for telling a computer what to do and how to do it. Software refers to one or more computer programs and data held in the storage of the computer. In other words, software is a set of programs, procedures, algorithms and its documentation concerned with the operation of a data processing system.
A type of technology – making, modification, usage, and knowledge of tools, machines, techniques, crafts, systems, methods of organization, in order to solve a problem, improve a preexisting solution to a problem, achieve a goal, handle an applied input/output relation or perform a specific function. It can also refer to the collection of such tools, machinery, modifications, arrangements and procedures. Technologies significantly affect human as well as other animal species' ability to control and adapt to their natural environments.
A form of computer technology – computers and their application. NLP makes use of computers, image scanners, microphones, and many types of software programs.
Language technology – consists of natural-language processing (NLP) and computational linguistics (CL) on the one hand, and speech technology on the other. It also includes many application oriented aspects of these. It is often called human language technology (HLT).
The following technologies make natural-language processing possible:
Communication – the activity of a source sending a message to a receiver
Computer programming –
Information extraction –
User interface –
Text editing – program used to edit plain text files
Word processing – piece of software used for composing, editing, formatting, printing documents
Input devices – pieces of hardware for sending data to a computer to be processed
Computer keyboard – typewriter style input device whose input is converted into various data depending on the circumstances
Image scanners –
Information extraction (IE) – field concerned in general with the extraction of semantic information from text. This covers tasks such as named-entity recognition, coreference resolution, relationship extraction, etc.
Ontology engineering – field that studies the methods and methodologies for building ontologies, which are formal representations of a set of concepts within a domain and the relationships between those concepts.
Speech processing – field that covers speech recognition, text-to-speech and related tasks.
Statistical natural-language processing –
Statistical semantics – a subfield of computational semantics that establishes semantic relations between words to examine their contexts.
Distributional semantics – a subfield of statistical semantics that examines the semantic relationship of words across a corpora or in large samples of data.
Natural-language processing contributes to, and makes use of (the theories, tools, and methodologies from), the following fields:
Automated reasoning – area of computer science and mathematical logic dedicated to understanding various aspects of reasoning, and producing software which allows computers to reason completely, or nearly completely, automatically. A sub-field of artificial intelligence, automatic reasoning is also grounded in theoretical computer science and philosophy of mind.
Linguistics – scientific study of human language. Natural-language processing requires understanding of the structure and application of language, and therefore it draws heavily from linguistics.
Applied linguistics – interdisciplinary field of study that identifies, investigates, and offers solutions to language-related real-life problems. Some of the academic fields related to applied linguistics are education, linguistics, psychology, computer science, anthropology, and sociology. Some of the subfields of applied linguistics relevant to natural-language processing are:
Bilingualism / Multilingualism –
Computer-mediated communication (CMC) – any communicative transaction that occurs through the use of two or more networked computers. Research on CMC focuses largely on the social effects of different computer-supported communication technologies. Many recent studies involve Internet-based social networking supported by social software.
Contrastive linguistics – practice-oriented linguistic approach that seeks to describe the differences and similarities between a pair of languages.
Conversation analysis (CA) – approach to the study of social interaction, embracing both verbal and non-verbal conduct, in situations of everyday life. Turn-taking is one aspect of language use that is studied by CA.
Discourse analysis – various approaches to analyzing written, vocal, or sign language use or any significant semiotic event.
Forensic linguistics – application of linguistic knowledge, methods and insights to the forensic context of law, language, crime investigation, trial, and judicial procedure.
Interlinguistics – study of improving communications between people of different first languages with the use of ethnic and auxiliary languages (lingua franca). For instance by use of intentional international auxiliary languages, such as Esperanto or Interlingua, or spontaneous interlanguages known as pidgin languages.
Language assessment – assessment of first, second or other language in the school, college, or university context; assessment of language use in the workplace; and assessment of language in the immigration, citizenship, and asylum contexts. The assessment may include analyses of listening, speaking, reading, writing or cultural understanding, with respect to understanding how the language works theoretically and the ability to use the language practically.
Language pedagogy – science and art of language education, including approaches and methods of language teaching and study. Natural-language processing is used in programs designed to teach language, including first- and second-language training.
Language planning –
Language policy –
Second-language acquisition –
Computational linguistics – interdisciplinary field dealing with the statistical or rule-based modeling of natural language from a computational perspective. The models and tools of computational linguistics are used extensively in the field of natural-language processing, and vice versa.
Computational semantics –
Corpus linguistics – study of language as expressed in samples (corpora) of "real world" text. Corpora is the plural of corpus, and a corpus is a specifically selected collection of texts (or speech segments) composed of natural language. After it is constructed (gathered or composed), a corpus is analyzed with the methods of computational linguistics to infer the meaning and context of its components (words, phrases, and sentences), and the relationships between them. Optionally, a corpus can be annotated ("tagged") with data (manually or automatically) to make the corpus easier to understand (e.g., part-of-speech tagging). This data is then applied to make sense of user input, for example, to make better (automated) guesses of what people are talking about or saying, perhaps to achieve more narrowly focused web searches, or for speech recognition.
Sign linguistics – scientific study and analysis of natural sign languages, their features, their structure (phonology, morphology, syntax, and semantics), their acquisition (as a primary or secondary language), how they develop independently of other languages, their application in communication, their relationships to other languages (including spoken languages), and many other aspects.
Human–computer interaction – the intersection of computer science and behavioral sciences, this field involves the study, planning, and design of the interaction between people (users) and computers. Attention to human-machine interaction is important, because poorly designed human-machine interfaces can lead to many unexpected problems. A classic example of this is the Three Mile Island accident where investigations concluded that the design of the human–machine interface was at least partially responsible for the disaster.
Information retrieval (IR) – field concerned with storing, searching and retrieving information. It is a separate field within computer science (closer to databases), but IR relies on some NLP methods (for example, stemming). Some current research and applications seek to bridge the gap between IR and NLP.
Knowledge representation (KR) – area of artificial intelligence research aimed at representing knowledge in symbols to facilitate inferencing from those knowledge elements, creating new elements of knowledge. Knowledge Representation research involves analysis of how to reason accurately and effectively and how best to use a set of symbols to represent a set of facts within a knowledge domain.
Semantic network – study of semantic relations between concepts.
Semantic Web –
Machine learning – subfield of computer science that examines pattern recognition and computational learning theory in artificial intelligence. There are three broad approaches to machine learning. Supervised learning occurs when the machine is given example inputs and outputs by a teacher so that it can learn a rule that maps inputs to outputs. Unsupervised learning occurs when the machine determines the inputs structure without being provided example inputs or outputs. Reinforcement learning occurs when a machine must perform a goal without teacher feedback.
Pattern recognition – branch of machine learning that examines how machines recognize regularities in data. As with machine learning, teachers can train machines to recognize patterns by providing them with example inputs and outputs (i.e. Supervised Learning), or the machines can recognize patterns without being trained on any example inputs or outputs (i.e. Unsupervised Learning).
Statistical classification –
Anaphora – type of expression whose reference depends upon another referential element. E.g., in the sentence 'Sally preferred the company of herself', 'herself' is an anaphoric expression in that it is coreferential with 'Sally', the sentence's subject.
Context-free language –
Controlled natural language – a natural language with a restriction introduced on its grammar and vocabulary in order to eliminate ambiguity and complexity
Corpus – body of data, optionally tagged (for example, through part-of-speech tagging), providing real world samples for analysis and comparison.
Text corpus – large and structured set of texts, nowadays usually electronically stored and processed. They are used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific subject (or domain).
Speech corpus – database of speech audio files and text transcriptions. In Speech technology, speech corpora are used, among other things, to create acoustic models (which can then be used with a speech recognition engine). In Linguistics, spoken corpora are used to do research into phonetic, conversation analysis, dialectology and other fields.
Context-free grammar (CFG) –
Constraint grammar (CG) –
Definite clause grammar (DCG) –
Functional unification grammar (FUG) –
Generalized phrase structure grammar (GPSG) –
Head-driven phrase structure grammar (HPSG) –
Lexical functional grammar (LFG) –
Probabilistic context-free grammar (PCFG) – another name for stochastic context-free grammar.
Stochastic context-free grammar (SCFG) –
Systemic functional grammar (SFG) –
Tree-adjoining grammar (TAG) –
Natural language –
n-gram – sequence of n number of tokens, where a "token" is a character, syllable, or word. The n is replaced by a number. Therefore, a 5-gram is an n-gram of 5 letters, syllables, or words. "Eat this" is a 2-gram (also known as a bigram).
Bigram – n-gram of 2 tokens. Every sequence of 2 adjacent elements in a string of tokens is a bigram. Bigrams are used for speech recognition, they can be used to solve cryptograms, and bigram frequency is one approach to statistical language identification.
Trigram – special case of the n-gram, where n is 3.
Ontology – formal representation of a set of concepts within a domain and the relationships between those concepts.
Taxonomy – practice and science of classification, including the principles underlying classification, and the methods of classifying things or concepts.
Hyponymy and hypernymy – the linguistics of hyponyms and hypernyms. A hyponym shares a type-of relationship with its hypernym. For example, pigeon, crow, eagle and seagull are all hyponyms of bird (their hypernym); which, in turn, is a hyponym of animal.
Taxonomy for search engines – typically called a "taxonomy of entities". It is a tree in which nodes are labelled with entities which are expected to occur in a web search query. These trees are used to match keywords from a search query with the keywords from relevant answers (or snippets).
Textual entailment – directional relation between text fragments. The relation holds whenever the truth of one text fragment follows from another text. In the TE framework, the entailing and entailed texts are termed text (t) and hypothesis (h), respectively. The relation is directional because even if "t entails h", the reverse "h entails t" is much less certain.
Triphone – sequence of three phonemes. Triphones are useful in models of natural-language processing where they are used to establish the various contexts in which a phoneme can occur in a particular natural language.
Automated essay scoring (AES) – the use of specialized computer programs to assign grades to essays written in an educational setting. It is a method of educational assessment and an application of natural-language processing. Its objective is to classify a large set of textual entities into a small number of discrete categories, corresponding to the possible grades—for example, the numbers 1 to 6. Therefore, it can be considered a problem of statistical classification.
Automatic image annotation – process by which a computer system automatically assigns textual metadata in the form of captioning or keywords to a digital image. The annotations are used in image retrieval systems to organize and locate images of interest from a database.
Automatic summarization – process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document. Often used to provide summaries of text of a known type, such as articles in the financial section of a newspaper.
Keyphrase extraction –
Document summarization –
Multi-document summarization –
Methods and techniques
Extraction-based summarization –
Abstraction-based summarization –
Maximum entropy-based summarization –
Sentence extraction –
Aided summarization –
Human aided machine summarization (HAMS) –
Machine aided human summarization (MAHS) –
Automatic taxonomy induction – automated construction of tree structures from a corpus. This may be applied to building taxonomical classification systems for reading by end users, such as web directories or subject outlines.
Coreference resolution – in order to derive the correct interpretation of text, or even to estimate the relative importance of various mentioned subjects, pronouns and other referring expressions need to be connected to the right individuals or objects. Given a sentence or larger chunk of text, coreference resolution determines which words ("mentions") refer to which objects ("entities") included in the text.
Anaphora resolution – concerned with matching up pronouns with the nouns or names that they refer to. For example, in a sentence such as "He entered John's house through the front door", "the front door" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to).
Dialog system –
Foreign-language reading aid – computer program that assists a non-native language user to read properly in their target language. The proper reading means that the pronunciation should be correct and stress to different parts of the words should be proper.
Foreign-language writing aid – computer program or any other instrument that assists a non-native language user (also referred to as a foreign-language learner) in writing decently in their target language. Assistive operations can be classified into two categories: on-the-fly prompts and post-writing checks.
Grammar checking – the act of verifying the grammatical correctness of written text, especially if this act is performed by a computer program.
Information retrieval –
Cross-language information retrieval –
Machine translation (MT) – aims to automatically translate text from one human language to another. This is one of the most difficult problems, and is a member of a class of problems colloquially termed "AI-complete", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) in order to solve properly.
Classical approach of machine translation – rules-based machine translation.
Computer-assisted translation –
Interactive machine translation –
Translation memory – database that stores so-called "segments", which can be sentences, paragraphs or sentence-like units (headings, titles or elements in a list) that have previously been translated, in order to aid human translators.
Example-based machine translation –
Rule-based machine translation –
Natural-language programming – interpreting and compiling instructions communicated in natural language into computer instructions (machine code).
Natural-language search –
Optical character recognition (OCR) – given an image representing printed text, determine the corresponding text.
Question answering – given a human-language question, determine its answer. Typical questions have a specific right answer (such as "What is the capital of Canada?"), but sometimes open-ended questions are also considered (such as "What is the meaning of life?").
Open domain question answering –
Spam filtering –
Sentiment analysis – extracts subjective information usually from a set of documents, often using online reviews to determine "polarity" about specific objects. It is especially useful for identifying trends of public opinion in the social media, for the purpose of marketing.
Speech recognition – given a sound clip of a person or people speaking, determine the textual representation of the speech. This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed "AI-complete" (see above). In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process.
Speech synthesis (Text-to-speech) –
Text simplification – automated editing a document to include fewer words, or use easier words, while retaining its underlying meaning and information.
Natural-language understanding – converts chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural-language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural-language expression which usually takes the form of organized notations of natural-languages concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural-languages semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization.
Natural-language generation – task of converting information from computer databases into readable human language.
Automatic document classification (text categorization) –
Automatic language identification –
Compound term processing – category of techniques that identify compound terms and match them to their definitions. Compound terms are built by combining two (or more) simple terms, for example "triple" is a single word term but "triple heart bypass" is a compound term.
Automatic taxonomy induction –
Corpus processing –
Automatic acquisition of lexicon –
Text normalization –
Text simplification –
Deep linguistic processing –
Discourse analysis – includes a number of related tasks. One task is identifying the discourse structure of connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast). Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g. yes–no questions, content questions, statements, assertions, orders, suggestions, etc.).
Information extraction –
Text mining – process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning.
Biomedical text mining – (also known as BioNLP), this is text mining applied to texts and literature of the biomedical and molecular biology domain. It is a rather recent research field drawing elements from natural-language processing, bioinformatics, medical informatics and computational linguistics. There is an increasing interest in text mining and information extraction strategies applied to the biomedical and molecular biology literature due to the increasing number of electronically available publications stored in databases such as PubMed.
Decision tree learning –
Sentence extraction –
Terminology extraction –
Latent semantic indexing –
Lemmatisation – groups together all like terms that share a same lemma such that they are classified as a single item.
Morphological segmentation – separates words into individual morphemes and identifies the class of the morphemes. The difficulty of this task depends greatly on the complexity of the morphology (i.e. the structure of words) of the language being considered. English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g. "open, opens, opened, opening") as separate words. In languages such as Turkish, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms.
Named-entity recognition (NER) – given a stream of text, determines which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization). Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case is often inaccurate or insufficient. For example, the first word of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized. Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names. For example, German capitalizes all nouns, regardless of whether they refer to names, and French and Spanish do not capitalize names that serve as adjectives.
Ontology learning – automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between those concepts from a corpus of natural-language text, and encoding them with an ontology language for easy retrieval. Also called "ontology extraction", "ontology generation", and "ontology acquisition".
Parsing – determines the parse tree (grammatical analysis) of a given sentence. The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses. In fact, perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human).
Shallow parsing –
Part-of-speech tagging – given a sentence, determines the part of speech for each word. Many words, especially common ones, can serve as multiple parts of speech. For example, "book" can be a noun ("the book on the table") or verb ("to book a flight"); "set" can be a noun, verb or adjective; and "out" can be any of at least five different parts of speech. Some languages have more such ambiguity than others. Languages with little inflectional morphology, such as English are particularly prone to such ambiguity. Chinese is prone to such ambiguity because it is a tonal language during verbalization. Such inflection is not readily conveyed via the entities employed within the orthography to convey intended meaning.
Query expansion –
Relationship extraction – given a chunk of text, identifies the relationships among named entities (e.g. who is the wife of whom).
Semantic analysis (computational) – formal analysis of meaning, and "computational" refers to approaches that in principle support effective implementation.
Explicit semantic analysis –
Latent semantic analysis –
Semantic analytics –
Sentence breaking (also known as sentence boundary disambiguation and sentence detection) – given a chunk of text, finds the sentence boundaries. Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g. marking abbreviations).
Speech segmentation – given a sound clip of a person or people speaking, separates it into words. A subtask of speech recognition and typically grouped with it.
Stemming – reduces an inflected or derived word into its word stem, base, or root form.
Text chunking –
Tokenization – given a chunk of text, separates it into distinct words, symbols, sentences, or other units
Topic segmentation and recognition – given a chunk of text, separates it into segments each of which is devoted to a topic, and identifies the topic of the segment.
Word segmentation – separates a chunk of continuous text into separate words. For a language like English, this is fairly trivial, since words are usually separated by spaces. However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language.
Word-sense disambiguation (WSD) – because many words have more than one meaning, word-sense disambiguation is used to select the meaning which makes the most sense in context. For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or from an online resource such as WordNet.
Word-sense induction – open problem of natural-language processing, which concerns the automatic identification of the senses of a word (i.e. meanings). Given that the output of word-sense induction is a set of senses for the target word (sense inventory), this task is strictly related to that of word-sense disambiguation (WSD), which relies on a predefined sense inventory and aims to solve the ambiguity of words in context.
Automatic acquisition of sense-tagged corpora –
W-shingling – set of unique "shingles"—contiguous subsequences of tokens in a document—that can be used to gauge the similarity of two documents. The w denotes the number of tokens in each shingle in the set.
Natural-language generation – task of converting information from computer databases into readable human language.
Automatic taxonomy induction (ATI) – automated building of tree structures from a corpus. While ATI is used to construct the core of ontologies (and doing so makes it a component process of natural-language understanding), when the ontologies being constructed are end user readable (such as a subject outline), and these are used for the construction of further documentation (such as using an outline as the basis to construct a report or treatise) this also becomes a component process of natural-language generation.
Document structuring –
History of natural-language processing
History of machine translation
History of automated essay scoring
History of natural-language user interface
History of natural-language understanding
History of optical character recognition
History of question answering
History of speech synthesis
Turing test – test of a machine's ability to exhibit intelligent behavior, equivalent to or indistinguishable from, that of an actual human. In the original illustrative example, a human judge engages in a natural-language conversation with a human and a machine designed to generate performance indistinguishable from that of a human being. All participants are separated from one another. If the judge cannot reliably tell the machine from the human, the machine is said to have passed the test. The test was introduced by Alan Turing in his 1950 paper "Computing Machinery and Intelligence," which opens with the words: "I propose to consider the question, 'Can machines think?'"
Universal grammar – theory in linguistics, usually credited to Noam Chomsky, proposing that the ability to learn grammar is hard-wired into the brain. The theory suggests that linguistic ability manifests itself without being taught (see poverty of the stimulus), and that there are properties that all natural human languages share. It is a matter of observation and experimentation to determine precisely what abilities are innate and what properties are shared by all languages.
ALPAC – was a committee of seven scientists led by John R. Pierce, established in 1964 by the U. S. Government in order to evaluate the progress in computational linguistics in general and machine translation in particular. Its report, issued in 1966, gained notoriety for being very skeptical of research done in machine translation so far, and emphasizing the need for basic research in computational linguistics; this eventually caused the U. S. Government to reduce its funding of the topic dramatically.
Conceptual dependency theory – a model of natural-language understanding used in artificial intelligence systems. Roger Schank at Stanford University introduced the model in 1969, in the early days of artificial intelligence. This model was extensively used by Schank's students at Yale University such as Robert Wilensky, Wendy Lehnert, and Janet Kolodner.
Augmented transition network – type of graph theoretic structure used in the operational definition of formal languages, used especially in parsing relatively complex natural languages, and having wide application in artificial intelligence. Introduced by William A. Woods in 1970.
Distributed Language Translation (project) –
Sukhotin's algorithm – statistical classification algorithm for classifying characters in a text as vowels or consonants. It was initially created by Boris V. Sukhotin.
T9 (predictive text) – stands for "Text on 9 keys", is a USA-patented predictive text technology for mobile phones (specifically those that contain a 3x4 numeric keypad), originally developed by Tegic Communications, now part of Nuance Communications.
Tatoeba – free collaborative online database of example sentences geared towards foreign-language learners.
Teragram Corporation – fully owned subsidiary of SAS Institute, a major producer of statistical analysis software, headquartered in Cary, North Carolina, USA. Teragram is based in Cambridge, Massachusetts and specializes in the application of computational linguistics to multilingual natural-language processing.
TipTop Technologies – company that developed TipTop Search, a real-time web, social search engine with a unique platform for semantic analysis of natural language. TipTop Search provides results capturing individual and group sentiment, opinions, and experiences from content of various sorts including real-time messages from Twitter or consumer product reviews on Amazon.com.
Transderivational search – when a search is being conducted for a fuzzy match across a broad field. In computing the equivalent function can be performed using content-addressable memory.
Vocabulary mismatch – common phenomenon in the usage of natural languages, occurring when different people name the same thing or concept differently.
LRE Map –
Reification (linguistics) –
Semantic Web –
Spoken dialogue system –
Affix grammar over a finite lattice –
Aggregation (linguistics) –
Bag-of-words model – model that represents a text as a bag (multiset) of its words that disregards grammar and word sequence, but maintains multiplicity. This model is a commonly used to train document classifiers
Brill tagger –
Cache language model –
ChaSen, MeCab – provide morphological analysis and word splitting for Japanese
Classic monolingual WSD –
CMU Pronouncing Dictionary – also known as cmudict, is a public domain pronouncing dictionary designed for uses in speech technology, and was created by Carnegie Mellon University (CMU). It defines a mapping from English words to their North American pronunciations, and is commonly used in speech processing applications such as the Festival Speech Synthesis System and the CMU Sphinx speech recognition system.
Concept mining –
Content determination –
DBpedia Spotlight –
Deep linguistic processing –
Discourse relation –
Document-term matrix –
Dragomir R. Radev –
Filtered-popping recursive transition network –
Robby Garner –
Gorn address –
Grammar induction –
Hidden Markov model –
Human language technology –
Information extraction –
International Conference on Language Resources and Evaluation –
Kleene star –
Language Computer Corporation –
Language model –
Latent semantic mapping –
Legal information retrieval –
Lesk algorithm –
Lessac Technologies –
Lexical choice –
Lexical Markup Framework –
Lexical substitution –
Logic form –
LRE Map –
Machine translation software usability –
Maximum entropy –
Message Understanding Conference –
Minimal recursion semantics –
Morphological pattern –
Multi-document summarization –
Multilingual notation –
Naive semantics –
Natural language –
Natural-language interface –
Natural-language user interface –
News analytics –
Nondeterministic polynomial –
Open domain question answering –
Optimality theory –
Paco Nathan –
Phrase structure grammar –
Powerset (company) –
Production (computer science) –
Question answering –
Realization (linguistics) –
Recursive transition network –
Referring expression generation –
Rewrite rule –
Semantic compression –
Semantic neural network –
SPL notation –
Stemming – reduces an inflected or derived word into its word stem, base, or root form.
String kernel –
Google Ngram Viewer – graphs n-gram usage from a corpus of more than 5.2 million books
Text corpus (see list) – large and structured set of texts (nowadays usually electronically stored and processed). They are used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific language territory.
Bank of English
Corpus of Contemporary American English (COCA)
The following natural-language processing toolkits are notable collections of natural-language processing software. They are suites of libraries, frameworks, and applications for symbolic, statistical natural-language and speech processing.
ABNER (A Biomedical Named-Entity Recognizer) – open source text mining program that uses linear-chain conditional random field sequence models. It automatically tags genes, proteins and other entity names in text. Written by Burr Settles of the University of Wisconsin-Madison.
Stanford NER (Named-Entity Recognizer) — Java implementation of a Named-Entity Recognizer that uses linear-chain conditional random field sequence models. It automatically tags persons, organizations, and locations in text in English, German, Chinese, and Spanish languages. Written by Jenny Finkel and other members of the Stanford NLP Group at Stanford University.
Comparison of machine translation applications
Machine translation applications
Linguee – web service that provides an online dictionary for a number of language pairs. Unlike similar services, such as LEO, Linguee incorporates a search engine that provides access to large amounts of bilingual, translated sentence pairs, which come from the World Wide Web. As a translation aid, Linguee therefore differs from machine translation services like Babelfish and is more similar in function to a translation memory.
UNL Universal Networking Language
Yahoo! Babel Fish
CTAKES – open-source natural-language processing system for information extraction from electronic medical record clinical free-text. It processes clinical notes, identifying types of clinical named entities — drugs, diseases/disorders, signs/symptoms, anatomical sites and procedures. Each named entity has attributes for the text span, the ontology mapping code, context (family history of, current, unrelated to patient), and negated/not negated. Also known as Apache cTAKES.
ETAP-3 – proprietary linguistic processing system focusing on English and Russian. It is a rule-based system which uses the Meaning-Text Theory as its theoretical foundation.
JAPE – the Java Annotation Patterns Engine, a component of the open-source General Architecture for Text Engineering (GATE) platform. JAPE is a finite state transducer that operates over annotations based on regular expressions.
LOLITA – "Large-scale, Object-based, Linguistic Interactor, Translator and Analyzer". LOLITA was developed by Roberto Garigliano and colleagues between 1986 and 2000. It was designed as a general-purpose tool for processing unrestricted text that could be the basis of a wide variety of applications. At its core was a semantic network containing some 90,000 interlinked concepts.
Maluuba – intelligent personal assistant for Android devices, that uses a contextual approach to search which takes into account the user's geographic location, contacts, and language.
METAL MT – machine translation system developed in the 1980s at the University of Texas and at Siemens which ran on Lisp Machines.
Never-Ending Language Learning – semantic machine learning system developed by a research team at Carnegie Mellon University, and supported by grants from DARPA, Google, and the NSF, with portions of the system running on a supercomputing cluster provided by Yahoo!. NELL was programmed by its developers to be able to identify a basic set of fundamental semantic relationships between a few hundred predefined categories of data, such as cities, companies, emotions and sports teams. Since the beginning of 2010, the Carnegie Mellon research team has been running NELL around the clock, sifting through hundreds of millions of web pages looking for connections between the information it already knows and what it finds through its search process – to make new connections in a manner that is intended to mimic the way humans learn new information.
Regulus Grammar Compiler – software system for compiling unification grammars into grammars for speech recognition systems.
S Voice –
Siri (software) –
Weka's classification tools –
word2vec – models that were developed by a team of researchers led by Thomas Milkov at Google to generate word embeddings that can reconstruct some of the linguistic context of words using shallow, two dimensional neural nets derived from a much larger vector space.
Festival Speech Synthesis System –
CMU Sphinx speech recognition system –
Language Grid – Open source platform for language web services, which can customize language services by combining existing language services.
Chatterbot – a text-based conversation agent that can interact with human users through some medium, such as an instant message service. Some chatterbots are designed for specific purposes, while others converse with human users on a wide range of topics.
Racter (or Claude Chatterbot)
Mark V Shaney
Albert One – 1998 and 1999 Loebner winner, by Robby Garner.
A.L.I.C.E. – 2001, 2002, and 2004 Loebner Prize winner developed by Richard Wallace.
Cleverbot (winner of the 2010 Mechanical Intelligence Competition)
Elbot – 2008 Loebner Prize winner, by Fred Roberts.
Eugene Goostman – 2012 Turing 100 winner, by Vladimir Veselov.
Fred – an early chatterbot by Robby Garner.
Mitsuku, 2013 and 2016 Loebner Prize winner
Rose - ... 2015 - 3x Loebner Prize winner, by Bruce Wilcox.
SimSimi – A popular artificial intelligence conversation program that was created in 2002 by ISMaker.
Spookitalk – A chatterbot used for NPCs in Douglas Adams' Starship Titanic video game.
Ultra Hal – 2007 Loebner Prize winner, by Robert Medeksza.
GooglyMinotaur, specializing in Radiohead, the first bot released by ActiveBuddy (June 2001-March 2002)
SmarterChild, developed by ActiveBuddy and released in June 2001
Infobot, an assistant on IRC channels such as #perl, primarily to help out with answering Frequently Asked Questions (June 1995-today)
Negobot, a bot designed to catch online pedophiles by posing as a young girl and attempting to elicit personal details from people it speaks to.
AFNLP (Asian Federation of Natural Language Processing Associations) – the organization for coordinating the natural-language processing related activities and events in the Asia-Pacific region.
Australasian Language Technology Association –
Association for Computational Linguistics – international scientific and professional society for people working on problems involving natural-language processing.
Annual Meeting of the Association for Computational Linguistics (ACL)
International Conference on Intelligent Text Processing and Computational Linguistics (CICLing)
International Conference on Language Resources and Evaluation – biennial conference organised by the European Language Resources Association with the support of institutions and organisations involved in natural-language processing
Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)
Text, Speech and Dialogue (TSD) – annual conference
Text Retrieval Conference (TREC) – on-going series of workshops focusing on various information retrieval (IR) research areas, or tracks
AlchemyAPI – service provider of a natural-language processing API.
Google, Inc. – the Google search engine is an example of automatic summarization, utilizing keyphrase extraction.
Calais (Reuters product) – provider of a natural-language processing services.
Wolfram Research, Inc. developer of natural-language processing computation engine Wolfram Alpha.
Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing – Wermter, S., Riloff E. and Scheler, G. (editors). First book that addressed statistical and neural network learning of language.
Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics – by Daniel Jurafsky and James H. Martin. Introductory book on language technology.
Studies in Natural Language Processing – book series of the Association for Computational Linguistics, published by Cambridge University Press.
Computational Linguistics – peer-reviewed academic journal in the field of computational linguistics. It is published quarterly by MIT Press for the Association for Computational Linguistics (ACL)
Daniel Bobrow –
Rollo Carpenter – creator of Jabberwacky and Cleverbot.
Noam Chomsky – author of the seminal work Syntactic Structures, which revolutionized Linguistics with 'universal grammar', a rule based system of syntactic structures.
Kenneth Colby –
David Ferrucci – principal investigator of the team that created Watson, IBM's AI computer that won the quiz show Jeopardy!
Lyn Frazier –
Daniel Jurafsky – Professor of Linguistics and Computer Science at Stanford University. With James H. Martin, he wrote the textbook Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics
Roger Schank – introduced the conceptual dependency theory for natural-language understanding.
Jean E. Fox Tree –
Alan Turing – originator of the Turing Test.
Joseph Weizenbaum – author of the ELIZA chatterbot.
Terry Winograd – professor of computer science at Stanford University, and co-director of the Stanford Human-Computer Interaction Group. He is known within the philosophy of mind and artificial intelligence fields for his work on natural language using the SHRDLU program.
William Aaron Woods –
Maurice Gross – author of the concept of local grammar, taking finite automata as the competence model of language.
Stephen Wolfram – CEO and founder of Wolfram Research, creator of the programming language (natural-language understanding) Wolfram Language, and natural-language processing computation engine Wolfram Alpha.
Victor Yngve –
Crevier, Daniel (1993). AI: The Tumultuous Search for Artificial Intelligence. New York, NY: BasicBooks. ISBN 0-465-02997-3.
McCorduck, Pamela (2004), Machines Who Think (2nd ed.), Natick, MA: A. K. Peters, Ltd., ISBN 978-1-56881-205-2, OCLC 52197627.
Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2. A semantic decomposition is an algorithm that breaks down the meanings of phrases or concepts into less complex concepts. The result of a semantic decomposition is a representation of meaning. This representation can be used for tasks, such as those related to artificial intelligence or machine learning. Semantic decomposition is common in natural language processing applications.
The basic idea of a semantic decomposition is taken from the learning skills of adult humans, where words are explained using other words. It is based on Meaning-text theory. Meaning-text theory is used as a theoretical linguistic framework to describe the meaning of concepts with other concepts.
Given that an AI does not inherently have language, it is unable to think about the meanings behind the words of a language. An artificial notion of meaning needs to be created for a strong AI to emerge.
Creating an artificial representation of meaning requires the analysis of what meaning is. Many terms are associated with meaning, including semantics, pragmatics, knowledge and understanding or word sense. Each term describes a particular aspect of meaning, and contributes to a multitude of theories explaining what meaning is. These theories need to be analyzed further to develop an artificial notion of meaning best fit for our current state of knowledge.
Representing meaning as a graph is one of the two ways that both an AI cognition and a linguistic researcher think about meaning (connectionist view). Logicians utilize a formal representation of meaning to build upon the idea of symbolic representation, whereas description logics describe languages and the meaning of symbols. This contention between 'neat' and 'scruffy' techniques has been discussed since the 1970s.
Research has so far identified semantic measures and with that word-sense disambiguation (WSD) - the differentiation of meaning of words - as the main problem of language understanding. As an AI-complete environment, WSD is a core problem of natural language understanding. AI approaches that use knowledge-given reasoning creates a notion of meaning combining the state of the art knowledge of natural meaning with the symbolic and connectionist formalization of meaning for AI. The abstract approach is shown in Figure. First, a connectionist knowledge representation is created as a semantic network consisting of concepts and their relations to serve as the basis for the representation of meaning.
This graph is built out of different knowledge sources like WordNet, Wiktionary, and BabelNET. The graph is created by lexical decomposition that recursively breaks each concept semantically down into a set of semantic primes. The primes are taken from the theory of Natural Semantic Metalanguage, which has been analyzed for usefulness in formal languages. Upon this graph marker passing is used to create the dynamic part of meaning representing thoughts. The marker passing algorithm, where symbolic information is passed along relations form one concept to another, uses node and edge interpretation to guide its markers. The node and edge interpretation model is the symbolic influence of certain concepts.
Future work uses the created representation of meaning to build heuristics and evaluate them through capability matching and agent planning, chatbots or other applications of natural language understanding.
Principle of compositionality The history of natural language processing describes the advances of natural language processing. There is some overlap with the history of machine translation, the history of speech recognition, and the history of artificial intelligence.
The history of machine translation dates back to the seventeenth century, when philosophers such as Leibniz and Descartes put forward proposals for codes which would relate words between languages. All of these proposals remained theoretical, and none resulted in the development of an actual machine.
The first patents for "translating machines" were applied for in the mid-1930s. One proposal, by Georges Artsrouni was simply an automatic bilingual dictionary using paper tape. The other proposal, by Peter Troyanskii, a Russian, was more detailed. Troyanski proposal included both the bilingual dictionary, and a method for dealing with grammatical roles between languages, based on Esperanto.
In 1950, Alan Turing published his famous article "Computing Machinery and Intelligence" which proposed what is now called the Turing test as a criterion of intelligence. This criterion depends on the ability of a computer program to impersonate a human in a real-time written conversation with a human judge, sufficiently well that the judge is unable to distinguish reliably — on the basis of the conversational content alone — between the program and a real human.
In 1957, Noam Chomsky’s Syntactic Structures revolutionized Linguistics with 'universal grammar', a rule-based system of syntactic structures.
The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem. However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.
Some notably successful NLP systems developed in the 1960s were SHRDLU, a natural language system working in restricted "blocks worlds" with restricted vocabularies.
In 1969 Roger Schank introduced the conceptual dependency theory for natural language understanding. This model, partially influenced by the work of Sydney Lamb, was extensively used by Schank's students at Yale University, such as Robert Wilensky, Wendy Lehnert, and Janet Kolodner.
In 1970, William A. Woods introduced the augmented transition network (ATN) to represent natural language input. Instead of phrase structure rules ATNs used an equivalent set of finite-state automata that were called recursively. ATNs and their more general format called "generalized ATNs" continued to be used for a number of years. During the 1970s many programmers began to write 'conceptual ontologies', which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.
Up to the 1980s, most NLP systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing. This was due both to the steady increase in computational power resulting from Moore's law and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.
The emergence of statistical approaches was aided by both increase in computing power and the availability of large datasets. At that time, large multilingual corpora were starting to emerge. Notably, some were produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.
Many of the notable early successes occurred in the field of machine translation. In 1993, the IBM alignment models were used for statistical machine translation. Compared to previous machine translation systems, which were symbolic systems manually coded by computational linguists, these systems were statistical, which allowed them to automatically learn from large textual corpora. Though these systems do not work well in situations where only small corpora is available, so data-efficient methods continue to be an area of research and development.
In 2001, a one-billion-word large text corpus, scraped from the Internet, referred to as "very very large" at the time, was used for word disambiguation.
To take advantage of large, unlabelled datasets, algorithms were developed for unsupervised and self-supervised learning. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.
Neural language models were developed in 1990s. In 1990, the Elman network, using a recurrent neural network, encoded each word in a training set as a vector, called a word embedding, and the whole vocabulary as a vector database, allowing it to perform such tasks as sequence-predictions that are beyond the power of a simple multilayer perceptron. A shortcoming of the static embeddings was that they didn't differentiate between multiple meanings of homonyms.
Yoshua Bengio developed the first neural probabilistic language model in 2000
In recent years, advancements in deep learning and large language models have significantly enhanced the capabilities of natural language processing, leading to widespread applications in areas such as healthcare, customer service, and content generation.
Crevier, Daniel (1993). AI: The Tumultuous Search for Artificial Intelligence. New York, NY: BasicBooks. ISBN 0-465-02997-3.
McCorduck, Pamela (2004), Machines Who Think (2nd ed.), Natick, MA: A. K. Peters, Ltd., ISBN 978-1-56881-205-2, OCLC 52197627.
Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2. Empirical Methods in Natural Language Processing (EMNLP) is a leading conference in the area of natural language processing and artificial intelligence. Along with the Association for Computational Linguistics (ACL) and the North American Chapter of the Association for Computational Linguistics (NAACL), it is one of the three primary high impact conferences for natural language processing research. EMNLP is organized by the ACL special interest group on linguistic data (SIGDAT) and was started in 1996, based on an earlier conference series called Workshop on Very Large Corpora (WVLC).
As of 2021, according to Microsoft Academic, EMNLP is the 14th most cited conference in computer science, with a citation count of 332,738, between ICML (#13) and ICLR (#15).
EMNLP 2025, Suzhou, China
EMNLP 2024, Miami, Florida, United States
EMNLP 2023, Singapore
EMNLP 2022, Abu Dhabi, United Arab Emirates (Hybrid)
EMNLP 2021, Punta Cana, Dominican Republic or online
EMNLP 2020, Punta Cana, Dominican Republic (Virtual conference due to COVID-19)
EMNLP-IJCNLP 2019, Hong Kong, China
EMNLP 2018, Brussels, Belgium
EMNLP 2017, Copenhagen, Denmark
EMNLP 2016, Austin, Texas, United States
EMNLP 2015, Lisbon, Portugal
EMNLP 2014, Doha, Qatar
EMNLP 2013, Seattle, Washington, United States
EMNLP 2012, Jeju Island, South Korea
EMNLP 2011, Edinburgh, United Kingdom
EMNLP 2010, Cambridge, Massachusetts, United States
EMNLP 2009, Singapore
EMNLP 2008, Honolulu, Hawaii, United States
EMNLP 2007, Prague, Czech Republic
EMNLP 2006, Sydney, Australia
HLT/EMNLP 2005, Vancouver, British Columbia, Canada
EMNLP 2004, Barcelona, Spain
EMNLP 2003, Sapporo, Japan
EMNLP 2002, Philadelphia, Pennsylvania, United States
EMNLP 2001, Pittsburgh, Pennsylvania, United States
EMNLP/VLC 2000, Hong Kong, China
EMNLP/VLC 1999, College Park, Maryland, United States
EMNLP 1998, Granada, Spain
EMNLP 1997, Providence, Rhode Island, United States
EMNLP 1996, Philadelphia, Pennsylvania, United States Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the form of decisions. "Understanding" in this context signifies the transformation of visual images (the input to the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.
The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. Image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, 3D point clouds from LiDaR sensors, or medical scanning devices. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems.
Subdisciplines of computer vision include scene reconstruction, object detection, event detection, activity recognition, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration.
Computer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. "Computer vision is concerned with the automatic extraction, analysis, and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding." As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems. Machine vision refers to a systems engineering discipline, especially in the context of factory automation. In more recent times, the terms computer vision and machine vision have converged to a greater degree.
In the late 1960s, computer vision began at universities that were pioneering artificial intelligence. It was meant to mimic the human visual system as a stepping stone to endowing robots with intelligent behavior. In 1966, it was believed that this could be achieved through an undergraduate summer project, by attaching a camera to a computer and having it "describe what it saw".
What distinguished computer vision from the prevalent field of digital image processing at that time was a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation.
The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields.
By the 1990s, some of the previous research topics became more active than others. Research in projective 3-D reconstructions led to better understanding of camera calibration. With the advent of optimization methods for camera calibration, it was realized that a lot of the ideas were already explored in bundle adjustment theory from the field of photogrammetry. This led to methods for sparse 3-D reconstructions of scenes from multiple images. Progress was made on the dense stereo correspondence problem and further multi-view stereo techniques. At the same time, variations of graph cut were used to solve image segmentation. This decade also marked the first time statistical learning techniques were used in practice to recognize faces in images (see Eigenface). Toward the end of the 1990s, a significant change came about with the increased interaction between the fields of computer graphics and computer vision. This included image-based rendering, image morphing, view interpolation, panoramic image stitching and early light-field rendering.
Recent work has seen the resurgence of feature-based methods used in conjunction with machine learning techniques and complex optimization frameworks.
The advancement of Deep Learning techniques has brought further life to the field of computer vision. The accuracy of deep learning algorithms on several benchmark computer vision data sets for tasks ranging from classification, segmentation and optical flow has surpassed prior methods.
Solid-state physics is another field that is closely related to computer vision. Most computer vision systems rely on image sensors, which detect electromagnetic radiation, which is typically in the form of either visible, infrared or ultraviolet light. The sensors are designed using quantum physics. The process by which light interacts with surfaces is explained using physics. Physics explains the behavior of optics which are a core part of most imaging systems. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Also, various measurement problems in physics can be addressed using computer vision, for example, motion in fluids.
Neurobiology has greatly influenced the development of computer vision algorithms. Over the last century, there has been an extensive study of eyes, neurons, and brain structures devoted to the processing of visual stimuli in both humans and various animals. This has led to a coarse yet convoluted description of how natural vision systems operate in order to solve certain vision-related tasks. These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity. Also, some of the learning-based methods developed within computer vision (e.g. neural net and deep learning based image and feature analysis and classification) have their background in neurobiology. The Neocognitron, a neural network developed in the 1970s by Kunihiko Fukushima, is an early example of computer vision taking direct inspiration from neurobiology, specifically the primary visual cortex.
Some strands of computer vision research are closely related to the study of biological vision—indeed, just as many strands of AI research are closely tied with research into human intelligence and the use of stored knowledge to interpret, integrate, and utilize visual information. The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, develops and describes the algorithms implemented in software and hardware behind artificial vision systems. An interdisciplinary exchange between biological and computer vision has proven fruitful for both fields.
Yet another field related to computer vision is signal processing. Many methods for processing one-variable signals, typically temporal signals, can be extended in a natural way to the processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images, there are many methods developed within computer vision that have no counterpart in the processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision.
Robot navigation sometimes deals with autonomous path planning or deliberation for robotic systems to navigate through an environment. A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot
Besides the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view. For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance. Computer vision is also used in fashion eCommerce, inventory management, patent search, furniture, and the beauty industry.
The fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences, and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented. In image processing, the input and output are both images, whereas in computer vision, the input is an image or video, and the output could be an enhanced image, an analysis of the image's content, or even a system's behavior based on that analysis.
Computer graphics produces image data from 3D models, and computer vision often produces 3D models from image data. There is also a trend towards a combination of the two disciplines, e.g., as explored in augmented reality.
The following characterizations appear relevant but should not be taken as universally accepted:
Image processing and image analysis tend to focus on 2D images, how to transform one image to another, e.g., by pixel-wise operations such as contrast enhancement, local operations such as edge extraction or noise removal, or geometrical transformations such as rotating the image. This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content.
Computer vision includes 3D analysis from 2D images. This analyzes the 3D scene projected onto one or several images, e.g., how to reconstruct structure or other information about the 3D scene from one or several images. Computer vision often relies on more or less complex assumptions about the scene depicted in an image.
Machine vision is the process of applying a range of technologies and methods to provide imaging-based automatic inspection, process control, and robot guidance in industrial applications. Machine vision tends to focus on applications, mainly in manufacturing, e.g., vision-based robots and systems for vision-based inspection, measurement, or picking (such as bin picking). This implies that image sensor technologies and control theory often are integrated with the processing of image data to control a robot and that real-time processing is emphasized by means of efficient implementations in hardware and software. It also implies that external conditions such as lighting can be and are often more controlled in machine vision than they are in general computer vision, which can enable the use of different algorithms.
There is also a field called imaging which primarily focuses on the process of producing images, but sometimes also deals with the processing and analysis of images. For example, medical imaging includes substantial work on the analysis of image data in medical applications. Progress in convolutional neural networks (CNNs) has improved the accurate detection of disease in medical images, particularly in cardiology, pathology, dermatology, and radiology.
Finally, pattern recognition is a field that uses various methods to extract information from signals in general, mainly based on statistical approaches and artificial neural networks. A significant part of this field is devoted to applying these methods to image data.
Photogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. computer stereo vision.
Applications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications. In many computer-vision applications, computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Examples of applications of computer vision include systems for:
Automatic inspection, e.g., in manufacturing applications;
Assisting humans in identification tasks, e.g., a species identification system;
Controlling processes, e.g., an industrial robot;
Detecting events, e.g., for visual surveillance or people counting, e.g., in the restaurant industry;
Interaction, e.g., as the input to a device for computer-human interaction;
monitoring agricultural crops, e.g. an open-source vision transformers model has been developed to help farmers automatically detect strawberry diseases with 98.4% accuracy.
Modeling objects or environments, e.g., medical image analysis or topographical modeling;
Navigation, e.g., by an autonomous vehicle or mobile robot;
Organizing information, e.g., for indexing databases of images and image sequences.
Tracking surfaces or planes in 3D coordinates for allowing Augmented Reality experiences.
Analyzing the condition of facilities in industry or construction.
Automatic real-time lip-reading for devices and apps to assist people with disabilities.
For 2024, the leading areas of computer vision were industry (market size US2.6 billion), military (market size US$996.2 million).
One of the most prominent application fields is medical computer vision, or medical image processing, characterized by the extraction of information from image data to diagnose a patient. An example of this is the detection of tumours, arteriosclerosis or other malign changes, and a variety of dental pathologies; measurements of organ dimensions, blood flow, etc. are another example. It also supports medical research by providing new information: e.g., about the structure of the brain or the quality of medical treatments. Applications of computer vision in the medical area also include enhancement of images interpreted by humans—ultrasonic images or X-ray images, for example—to reduce the influence of noise.
A second application area in computer vision is in industry, sometimes called machine vision, where information is extracted for the purpose of supporting a production process. One example is quality control where details or final products are being automatically inspected in order to find defects. One of the most prevalent fields for such inspection is the Wafer industry in which every single Wafer is being measured and inspected for inaccuracies or defects to prevent a computer chip from coming to market in an unusable manner. Another example is a measurement of the position and orientation of details to be picked up by a robot arm. Machine vision is also heavily used in the agricultural processes to remove undesirable foodstuff from bulk material, a process called optical sorting.
The obvious examples are the detection of enemy soldiers or vehicles and missile guidance. More advanced systems for missile guidance send the missile to an area rather than a specific target, and target selection is made when the missile reaches the area based on locally acquired image data. Modern military concepts, such as "battlefield awareness", imply that various sensors, including image sensors, provide a rich set of information about a combat scene that can be used to support strategic decisions. In this case, automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability.
One of the newer application areas is autonomous vehicles, which include submersibles, land-based vehicles (small robots with wheels, cars, or trucks), aerial vehicles, and unmanned aerial vehicles (UAV). The level of autonomy ranges from fully autonomous (unmanned) vehicles to vehicles where computer-vision-based systems support a driver or a pilot in various situations. Fully autonomous vehicles typically use computer vision for navigation, e.g., for knowing where they are or mapping their environment (SLAM), for detecting obstacles. It can also be used for detecting certain task-specific events, e.g., a UAV looking for forest fires. Examples of supporting systems are obstacle warning systems in cars, cameras and LiDAR sensors in vehicles, and systems for autonomous landing of aircraft. Several car manufacturers have demonstrated systems for autonomous driving of cars. There are ample examples of military autonomous vehicles ranging from advanced missiles to UAVs for recon missions or missile guidance. Space exploration is already being made with autonomous vehicles using computer vision, e.g., NASA's Curiosity and CNSA's Yutu-2 rover.
Materials such as rubber and silicon are being used to create sensors that allow for applications such as detecting microundulations and calibrating robotic hands. Rubber can be used in order to create a mold that can be placed over a finger, inside of this mold would be multiple strain gauges. The finger mold and sensors could then be placed on top of a small sheet of rubber containing an array of rubber pins. A user can then wear the finger mold and trace a surface. A computer can then read the data from the strain gauges and measure if one or more of the pins are being pushed upward. If a pin is being pushed upward then the computer can recognize this as an imperfection in the surface. This sort of technology is useful in order to receive accurate data on imperfections on a very large surface. Another variation of this finger mold sensor are sensors that contain a camera suspended in silicon. The silicon forms a dome around the outside of the camera and embedded in the silicon are point markers that are equally spaced. These cameras can then be placed on devices such as robotic hands in order to allow the computer to receive highly accurate tactile data.
Other application areas include:
Support of visual effects creation for cinema and broadcast, e.g., camera tracking (match moving).
Driver drowsiness detection
Tracking and counting organisms in the biological sciences
Each of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods. Some examples of typical computer vision tasks are presented below.
Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions. Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.
The classical problem in computer vision, image processing, and machine vision is that of determining whether or not the image data contains some specific object, feature, or activity. Different varieties of recognition problem are described in the literature.
Object recognition (also called object classification) – one or several pre-specified or learned objects or object classes can be recognized, usually together with their 2D positions in the image or 3D poses in the scene. Blippar, Google Goggles, and LikeThat provide stand-alone programs that illustrate this functionality.
Identification – an individual instance of an object is recognized. Examples include identification of a specific person's face or fingerprint, identification of handwritten digits, or the identification of a specific vehicle.
Detection – the image data are scanned for specific objects along with their locations. Examples include the detection of an obstacle in the car's field of view and possible abnormal cells or tissues in medical images or the detection of a vehicle in an automatic road toll system. Detection based on relatively simple and fast computations is sometimes used for finding smaller regions of interesting image data which can be further analyzed by more computationally demanding techniques to produce a correct interpretation.
Currently, the best algorithms for such tasks are based on convolutional neural networks. An illustration of their capabilities is given by the ImageNet Large Scale Visual Recognition Challenge; this is a benchmark in object classification and detection, with millions of images and 1000 object classes used in the competition. Performance of convolutional neural networks on the ImageNet tests is now close to that of humans. The best algorithms still struggle with objects that are small or thin, such as a small ant on the stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters (an increasingly common phenomenon with modern digital cameras). By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained classes, such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease.
Several specialized tasks based on recognition exist, such as:
Content-based image retrieval – finding all images in a larger set of images which have a specific content. The content can be specified in different ways, for example in terms of similarity relative to a target image (give me all images similar to image X) by utilizing reverse image search techniques, or in terms of high-level search criteria given as text input (give me all images which contain many houses, are taken during winter and have no cars in them).
Pose estimation – estimating the position or orientation of a specific object relative to the camera. An example application for this technique would be assisting a robot arm in retrieving objects from a conveyor belt in an assembly line situation or picking parts from a bin.
Optical character recognition (OCR) – identifying characters in images of printed or handwritten text, usually with a view to encoding the text in a format more amenable to editing or indexing (e.g. ASCII). A related task is reading of 2D codes such as data matrix and QR codes.
Facial recognition – a technology that enables the matching of faces in digital images or video frames to a face database, which is now widely used for mobile phone facelock, smart door locking, etc.
Emotion recognition – a subset of facial recognition, emotion recognition refers to the process of classifying human emotions. Psychologists caution, however, that internal emotions cannot be reliably detected from faces.
Shape Recognition Technology (SRT) in people counter systems differentiating human beings (head and shoulder patterns) from objects.
Human activity recognition - deals with recognizing the activity from a series of video frames, such as, if the person is picking up an object or walking.
Several tasks relate to motion estimation, where an image sequence is processed to produce an estimate of the velocity either at each points in the image or in the 3D scene or even of the camera that produces the images. Examples of such tasks are:
Egomotion – determining the 3D rigid motion (rotation and translation) of the camera from an image sequence produced by the camera.
Tracking – following the movements of a (usually) smaller set of interest points or objects (e.g., vehicles, objects, humans or other organisms) in the image sequence. This has vast industry applications as most high-running machinery can be monitored in this way.
Optical flow – to determine, for each point in the image, how that point is moving relative to the image plane, i.e., its apparent motion. This motion is a result of both how the corresponding 3D point is moving in the scene and how the camera is moving relative to the scene.
Given one or (typically) more images of a scene, or a video, scene reconstruction aims at computing a 3D model of the scene. In the simplest case, the model can be a set of 3D points. More sophisticated methods produce a complete 3D surface model. The advent of 3D imaging not requiring motion or scanning, and related processing algorithms is enabling rapid advances in this field. Grid-based 3D sensing can be used to acquire 3D images from multiple angles. Algorithms are now available to stitch multiple 3D images together into point clouds and 3D models.
Image restoration comes into the picture when the original image is degraded or damaged due to some external factors like lens wrong positioning, transmission interference, low lighting or motion blurs, etc., which is referred to as noise. When the images are degraded or damaged, the information to be extracted from them also gets damaged. Therefore, we need to recover or restore the image as it was intended to be. The aim of image restoration is the removal of noise (sensor noise, motion blur, etc.) from images. The simplest possible approach for noise removal is various types of filters, such as low-pass filters or median filters. More sophisticated methods assume a model of how the local image structures look to distinguish them from noise. By first analyzing the image data in terms of the local image structures, such as lines or edges, and then controlling the filtering based on local information from the analysis step, a better level of noise removal is usually obtained compared to the simpler approaches.
An example in this field is inpainting.
The organization of a computer vision system is highly application-dependent. Some systems are stand-alone applications that solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a computer vision system also depends on whether its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions that are found in many computer vision systems.
Image acquisition – A digital image is produced by one or several image sensors, which, besides various types of light-sensitive cameras, include range sensors, tomography devices, radar, ultra-sonic cameras, etc. Depending on the type of sensor, the resulting image data is an ordinary 2D image, a 3D volume, or an image sequence. The pixel values typically correspond to light intensity in one or several spectral bands (gray images or colour images) but can also be related to various physical measures, such as depth, absorption or reflectance of sonic or electromagnetic waves, or magnetic resonance imaging.
Pre-processing – Before a computer vision method can be applied to image data in order to extract some specific piece of information, it is usually necessary to process the data in order to ensure that it satisfies certain assumptions implied by the method. Examples are:
Re-sampling to ensure that the image coordinate system is correct.
Noise reduction to ensure that sensor noise does not introduce false information.
Contrast enhancement to ensure that relevant information can be detected.
Scale space representation to enhance image structures at locally appropriate scales.
Feature extraction – Image features at various levels of complexity are extracted from the image data. Typical examples of such features are:
Lines, edges and ridges.
Localized interest points such as corners, blobs or points.
More complex features may be related to texture, shape, or motion.
Detection/segmentation – At some point in the processing, a decision is made about which image points or regions of the image are relevant for further processing. Examples are:
Selection of a specific set of interest points.
Segmentation of one or multiple image regions that contain a specific object of interest.
Segmentation of image into nested scene architecture comprising foreground, object groups, single objects or salient object parts (also referred to as spatial-taxon scene hierarchy), while the visual salience is often implemented as spatial and temporal attention.
Segmentation or co-segmentation of one or multiple videos into a series of per-frame foreground masks while maintaining its temporal semantic continuity.
High-level processing – At this step, the input is typically a small set of data, for example, a set of points or an image region, which is assumed to contain a specific object. The remaining processing deals with, for example:
Verification that the data satisfies model-based and application-specific assumptions.
Estimation of application-specific parameters, such as object pose or object size.
Image recognition – classifying a detected object into different categories.
Image registration – comparing and combining two different views of the same object.
Decision making Making the final decision required for the application, for example:
Pass/fail on automatic inspection applications.
Match/no-match in recognition applications.
Flag for further human review in medical, military, security and recognition applications.
Image-understanding systems (IUS) include three levels of abstraction as follows: low level includes image primitives such as edges, texture elements, or regions; intermediate level includes boundaries, surfaces and volumes; and high level includes objects, scenes, or events. Many of these requirements are entirely topics for further research.
The representational requirements in the designing of IUS for these levels are: representation of prototypical concepts, concept organization, spatial knowledge, temporal knowledge, scaling, and description by comparison and differentiation.
While inference refers to the process of deriving new, not explicitly represented facts from currently known facts, control refers to the process that selects which of the many inference, search, and matching techniques should be applied at a particular stage of processing. Inference and control requirements for IUS are: search and hypothesis activation, matching and hypothesis testing, generation and use of expectations, change and focus of attention, certainty and strength of belief, inference and goal satisfaction.
There are many kinds of computer vision systems; however, all of them contain these basic elements: a power source, at least one image acquisition device (camera, ccd, etc.), a processor, and control and communication cables or some kind of wireless interconnection mechanism. In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones, contain an illumination system and may be placed in a controlled environment. Furthermore, a completed system includes many accessories, such as camera supports, cables, and connectors.
Most computer vision systems use visible-light cameras passively viewing a scene at frame rates of at most 60 frames per second (usually far slower).
A few computer vision systems use image-acquisition hardware with active illumination or something other than visible light or both, such as structured-light 3D scanners, thermographic cameras, hyperspectral imagers, radar imaging, lidar scanners, magnetic resonance images, side-scan sonar, synthetic aperture sonar, etc. Such hardware captures "images" that are then processed often using the same computer vision algorithms used to process visible-light images.
While traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances in digital signal processing and consumer graphics hardware has made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second. For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms. When combined with a high-speed projector, fast image acquisition allows 3D measurement and feature tracking to be realized.
Egocentric vision systems are composed of a wearable camera that automatically take pictures from a first-person perspective.
As of 2016, vision processing units are emerging as a new class of processors to complement CPUs and graphics processing units (GPUs) in this role.
Outline of computer vision
List of emerging technologies
Outline of artificial intelligence
James E. Dobson (2023). The Birth of Computer Vision. University of Minnesota Press. ISBN 978-1-5179-1421-9.
David Marr (1982). Vision. W. H. Freeman and Company. ISBN 978-0-7167-1284-8.
Azriel Rosenfeld; Avinash Kak (1982). Digital Picture Processing. Academic Press. ISBN 978-0-12-597301-4.
Barghout, Lauren; Lawrence W. Lee (2003). Perceptual information processing system. U.S. Patent Application 10/618,543. ISBN 978-0-262-08159-7.
Berthold K.P. Horn (1986). Robot Vision. MIT Press. ISBN 978-0-262-08159-7.
Michael C. Fairhurst (1988). Computer Vision for robotic systems. Prentice Hall. ISBN 978-0-13-166919-2.
Olivier Faugeras (1993). Three-Dimensional Computer Vision, A Geometric Viewpoint. MIT Press. ISBN 978-0-262-06158-2.
Tony Lindeberg (1994). Scale-Space Theory in Computer Vision. Springer. ISBN 978-0-7923-9418-1.
James L. Crowley; Henrik I. Christensen, eds. (1995). Vision as Process. Springer-Verlag. ISBN 978-3-540-58143-7.
Gösta H. Granlund; Hans Knutsson (1995). Signal Processing for Computer Vision. Kluwer Academic Publisher. ISBN 978-0-7923-9530-0.
Reinhard Klette; Karsten Schluens; Andreas Koschan (1998). Computer Vision – Three-Dimensional Data from Images. Springer, Singapore. ISBN 978-981-3083-71-4.
Emanuele Trucco; Alessandro Verri (1998). Introductory Techniques for 3-D Computer Vision. Prentice Hall. ISBN 978-0-13-261108-4.
Bernd Jähne (2002). Digital Image Processing. Springer. ISBN 978-3-540-67754-3.
Richard Hartley and Andrew Zisserman (2003). Multiple View Geometry in Computer Vision. Cambridge University Press. ISBN 978-0-521-54051-3.
Gérard Medioni; Sing Bing Kang (2004). Emerging Topics in Computer Vision. Prentice Hall. ISBN 978-0-13-101366-7.
R. Fisher; K Dawson-Howe; A. Fitzgibbon; C. Robertson; E. Trucco (2005). Dictionary of Computer Vision and Image Processing. John Wiley. ISBN 978-0-470-01526-1.
Nikos Paragios and Yunmei Chen and Olivier Faugeras (2005). Handbook of Mathematical Models in Computer Vision. Springer. ISBN 978-0-387-26371-7.
Wilhelm Burger; Mark J. Burge (2007). Digital Image Processing: An Algorithmic Approach Using Java. Springer. ISBN 978-1-84628-379-6. Archived from the original on 2014-05-17. Retrieved 2007-06-13.
Pedram Azad; Tilo Gockel; Rüdiger Dillmann (2008). Computer Vision – Principles and Practice. Elektor International Media BV. ISBN 978-0-905705-71-2.
Richard Szeliski (2010). Computer Vision: Algorithms and Applications. Springer-Verlag. ISBN 978-1848829343.
J. R. Parker (2011). Algorithms for Image Processing and Computer Vision (2nd ed.). Wiley. ISBN 978-0470643853.
Richard J. Radke (2013). Computer Vision for Visual Effects. Cambridge University Press. ISBN 978-0-521-76687-6.
Nixon, Mark; Aguado, Alberto (2019). Feature Extraction and Image Processing for Computer Vision (4th ed.). Academic Press. ISBN 978-0128149768.
USC Iris computer vision conference list
Computer vision papers on the web – a complete list of papers of the most relevant computer vision conferences.
Computer Vision Online Archived 2011-11-30 at the Wayback Machine – news, source code, datasets and job offers related to computer vision
CVonline – Bob Fisher's Compendium of Computer Vision.
British Machine Vision Association – supporting computer vision research within the UK via the BMVC and MIUA conferences, Annals of the BMVA (open-source journal), BMVA Summer School and one-day meetings
Computer Vision Container, Joe Hoeller GitHub: Widely adopted open-source container for GPU accelerated computer vision applications. Used by researchers, universities, private companies, as well as the U.S. Gov't. In computer vision and image processing, a feature is a piece of information about the content of an image; typically about whether a certain region of the image has certain properties. Features may be specific structures in the image such as points, edges or objects. Features may also be the result of a general neighborhood operation or feature detection applied to the image. Other examples of features are related to motion in image sequences, or to shapes defined in terms of curves or boundaries between different image regions.
More broadly a feature is any piece of information that is relevant for solving the computational task related to a certain application. This is the same sense as feature in machine learning and pattern recognition generally, though image processing has a very sophisticated collection of features. The feature concept is very general and the choice of features in a particular computer vision system may be highly dependent on the specific problem at hand.
There is no universal or exact definition of what constitutes a feature, and the exact definition often depends on the problem or the type of application. Nevertheless, a feature is typically defined as an "interesting" part of an image, and features are used as a starting point for many computer vision algorithms.
Since features are used as the starting point and main primitives for subsequent algorithms, the overall algorithm will often only be as good as its feature detector. Consequently, the desirable property for a feature detector is repeatability: whether or not the same feature will be detected in two or more different images of the same scene.
Feature detection is a low-level image processing operation. That is, it is usually performed as the first operation on an image and examines every pixel to see if there is a feature present at that pixel. If this is part of a larger algorithm, then the algorithm will typically only examine the image in the region of the features. As a built-in pre-requisite to feature detection, the input image is usually smoothed by a Gaussian kernel in a scale-space representation and one or several feature images are computed, often expressed in terms of local image derivative operations.
Occasionally, when feature detection is computationally expensive and there are time constraints, a higher-level algorithm may be used to guide the feature detection stage so that only certain parts of the image are searched for features.
There are many computer vision algorithms that use feature detection as the initial step, so as a result, a very large number of feature detectors have been developed. These vary widely in the kinds of feature detected, the computational complexity and the repeatability.
When features are defined in terms of local neighborhood operations applied to an image, a procedure commonly referred to as feature extraction, one can distinguish between feature detection approaches that produce local decisions whether there is a feature of a given type at a given image point or not, and those who produce non-binary data as result. The distinction becomes relevant when the resulting detected features are relatively sparse. Although local decisions are made, the output from a feature detection step does not need to be a binary image. The result is often represented in terms of sets of (connected or unconnected) coordinates of the image points where features have been detected, sometimes with subpixel accuracy.
When feature extraction is done without local decision making, the result is often referred to as a feature image. Consequently, a feature image can be seen as an image in the sense that it is a function of the same spatial (or temporal) variables as the original image, but where the pixel values hold information about image features instead of intensity or color. This means that a feature image can be processed in a similar way as an ordinary image generated by an image sensor. Feature images are also often computed as integrated step in algorithms for feature detection.
In some applications, it is not sufficient to extract only one type of feature to obtain the relevant information from the image data. Instead, two or more different features are extracted, resulting in two or more feature descriptors at each image point. A common practice is to organize the information provided by all these descriptors as the elements of one single vector, commonly referred to as a feature vector. The set of all possible feature vectors constitutes a feature space.
A common example of feature vectors appears when each image point is to be classified as belonging to a specific class. Assuming that each image point has a corresponding feature vector based on a suitable set of features, meaning that each class is well separated in the corresponding feature space, the classification of each image point can be done using standard classification method.
Another and related example occurs when neural network-based processing is applied to images. The input data fed to the neural network is often given in terms of a feature vector from each image point, where the vector is constructed from several different features extracted from the image data. During a learning phase, the network can itself find which combinations of different features are useful for solving the problem at hand.
Edges are points where there is a boundary (or an edge) between two image regions. In general, an edge can be of almost arbitrary shape, and may include junctions. In practice, edges are usually defined as sets of points in the image that have a strong gradient magnitude. Furthermore, some common algorithms will then chain high gradient points together to form a more complete description of an edge. These algorithms usually place some constraints on the properties of an edge, such as shape, smoothness, and gradient value.
Locally, edges have a one-dimensional structure.
The terms corners and interest points are used somewhat interchangeably and refer to point-like features in an image, which have a local two-dimensional structure. The name "Corner" arose since early algorithms first performed edge detection, and then analyzed the edges to find rapid changes in direction (corners). These algorithms were then developed so that explicit edge detection was no longer required, for instance by looking for high levels of curvature in the image gradient. It was then noticed that the so-called corners were also being detected on parts of the image that were not corners in the traditional sense (for instance a small bright spot on a dark background may be detected). These points are frequently known as interest points, but the term "corner" is used by tradition.
Blobs provide a complementary description of image structures in terms of regions, as opposed to corners that are more point-like. Nevertheless, blob descriptors may often contain a preferred point (a local maximum of an operator response or a center of gravity) which means that many blob detectors may also be regarded as interest point operators. Blob detectors can detect areas in an image that are too smooth to be detected by a corner detector.
Consider shrinking an image and then performing corner detection. The detector will respond to points that are sharp in the shrunk image, but may be smooth in the original image. It is at this point that the difference between a corner detector and a blob detector becomes somewhat vague. To a large extent, this distinction can be remedied by including an appropriate notion of scale. Nevertheless, due to their response properties to different types of image structures at different scales, the LoG and DoH blob detectors are also mentioned in the article on corner detection.
For elongated objects, the notion of ridges is a natural tool. A ridge descriptor computed from a grey-level image can be seen as a generalization of a medial axis. From a practical viewpoint, a ridge can be thought of as a one-dimensional curve that represents an axis of symmetry, and in addition has an attribute of local ridge width associated with each ridge point. Unfortunately, however, it is algorithmically harder to extract ridge features from general classes of grey-level images than edge-, corner- or blob features. Nevertheless, ridge descriptors are frequently used for road extraction in aerial images and for extracting blood vessels in medical images—see ridge detection.
Feature detection includes methods for computing abstractions of image information and making local decisions at every image point whether there is an image feature of a given type at that point or not. The resulting features will be subsets of the image domain, often in the form of isolated points, continuous curves or connected regions.
The extraction of features are sometimes made over several scalings. One of these methods is the scale-invariant feature transform (SIFT).
Once features have been detected, a local image patch around the feature can be extracted. This extraction may involve quite considerable amounts of image processing. The result is known as a feature descriptor or feature vector. Among the approaches that are used to feature description, one can mention N-jets and local histograms (see scale-invariant feature transform for one example of a local histogram descriptor). In addition to such attribute information, the feature detection step by itself may also provide complementary attributes, such as the edge orientation and gradient magnitude in edge detection and the polarity and the strength of the blob in blob detection.
Scale-invariant feature transform
Edge direction, changing intensity, autocorrelation.
Motion detection. Area based, differential approach. Optical flow.
Arbitrary shapes (generalized Hough transform)
Works with any parameterizable feature (class variables, cluster detection, etc..)
Generalised Hough transform
Deformable, parameterized shapes
Active contours (snakes)
A specific image feature, defined in terms of a specific structure in the image data, can often be represented in different ways. For example, an edge can be represented as a Boolean variable in each image point that describes whether an edge is present at that point. Alternatively, we can instead use a representation that provides a certainty measure instead of a Boolean statement of the edge's existence and combine this with information about the orientation of the edge. Similarly, the color of a specific region can either be represented in terms of the average color (three scalars) or a color histogram (three functions).
When a computer vision system or computer vision algorithm is designed the choice of feature representation can be a critical issue. In some cases, a higher level of detail in the description of a feature may be necessary for solving the problem, but this comes at the cost of having to deal with more data and more demanding processing. Below, some of the factors which are relevant for choosing a suitable representation are discussed. In this discussion, an instance of a feature representation is referred to as a feature descriptor, or simply descriptor.
Two examples of image features are local edge orientation and local velocity in an image sequence. In the case of orientation, the value of this feature may be more or less undefined if more than one edge are present in the corresponding neighborhood. Local velocity is undefined if the corresponding image region does not contain any spatial variation. As a consequence of this observation, it may be relevant to use a feature representation that includes a measure of certainty or confidence related to the statement about the feature value. Otherwise, it is a typical situation that the same descriptor is used to represent feature values of low certainty and feature values close to zero, with a resulting ambiguity in the interpretation of this descriptor. Depending on the application, such an ambiguity may or may not be acceptable.
In particular, if a featured image will be used in subsequent processing, it may be a good idea to employ a feature representation that includes information about certainty or confidence. This enables a new feature descriptor to be computed from several descriptors, for example, computed at the same image point but at different scales, or from different but neighboring points, in terms of a weighted average where the weights are derived from the corresponding certainties. In the simplest case, the corresponding computation can be implemented as a low-pass filtering of the featured image. The resulting feature image will, in general, be more stable to noise.
In addition to having certainty measures included in the representation, the representation of the corresponding feature values may itself be suitable for an averaging operation or not. Most feature representations can be averaged in practice, but only in certain cases can the resulting descriptor be given a correct interpretation in terms of a feature value. Such representations are referred to as averageable.
For example, if the orientation of an edge is represented in terms of an angle, this representation must have a discontinuity where the angle wraps from its maximal value to its minimal value. Consequently, it can happen that two similar orientations are represented by angles that have a mean that does not lie close to either of the original angles and, hence, this representation is not averageable. There are other representations of edge orientation, such as the structure tensor, which are averageable.
Another example relates to motion, where in some cases only the normal velocity relative to some edge can be extracted. If two such features have been extracted and they can be assumed to refer to same true velocity, this velocity is not given as the average of the normal velocity vectors. Hence, normal velocity vectors are not averageable. Instead, there are other representations of motions, using matrices or tensors, that give the true velocity in terms of an average operation of the normal velocity descriptors.
Features detected in each image can be matched across multiple images to establish corresponding features such as corresponding points.
The algorithm is based on comparing and analyzing point correspondences between the reference image and the target image. If any part of the cluttered scene shares correspondences greater than the threshold, that part of the cluttered scene image is targeted and considered to include the reference object there.
Automatic image annotation
Vectorization (image tracing)
T. Lindeberg (2009). "Scale-space". In Benjamin Wah (ed.). Encyclopedia of Computer Science and Engineering. Vol. IV. John Wiley and Sons. pp. 2495–2504. doi:10.1002/9780470050118.ecse609. ISBN 978-0470050118. (summary and review of a number of feature detectors formulated based on scale-space operations) In the field of computer vision, any two images of the same planar surface in space are related by a homography (assuming a pinhole camera model). This has many practical applications, such as image rectification, image registration, or camera motion—rotation and translation—between two images. Once camera resectioning has been done from an estimated homography matrix, this information may be used for navigation, or to insert models of 3D objects into an image or video, so that they are rendered with the correct perspective and appear to have been part of the original scene (see Augmented reality).
We have two cameras a and b, looking at points
in a plane.
Passing from the projection
in b to the projection
^p_=^z_^z_K_\cdot H_\cdot K_^\cdot ^p_
are the z coordinates of P in each camera frame and where the homography matrix
is given by
=R-
is the rotation matrix by which b is rotated in relation to a; t is the translation vector from a to b; n and d are the normal vector of the plane and the distance from origin to the plane respectively.
Ka and Kb are the cameras' intrinsic parameter matrices.
The figure shows camera b looking at the plane at distance d.
Note: From above figure, assuming
as plane model,
is the projection of vector
, and equal to
P_\right)
. And we have
=R-
This formula is only valid if camera b has no rotation and no translation. In the general case where
are the respective rotations and translations of camera a and b,
and the homography matrix
=R_R_^-*R_^*t_+t_)n^
where d is the distance of the camera b to the plane.
When the image region in which the homography is computed is small or the image has been acquired with a large focal length, an affine homography is a more appropriate model of image displacements. An affine homography is a special type of a general homography whose last row is fixed to
Direct linear transformation
Feature (computer vision)
Fundamental matrix (computer vision)
Pose (computer vision)
O. Chum and T. Pajdla and P. Sturm (2005). "The Geometric Error for Homographies" (PDF). Computer Vision and Image Understanding. 97 (1): 86–102. doi:10.1016/j.cviu.2004.03.004.
homest is a GPL C/C++ library for robust, non-linear (based on the Levenberg–Marquardt algorithm) homography estimation from matched point pairs (Manolis Lourakis).
OpenCV is a complete (open and free) computer vision software library that has many routines related to homography estimation (cvFindHomography) and re-projection (cvPerspectiveTransform).
Serge Belongie & David Kriegman (2007) Explanation of Homography Estimation from Department of Computer Science and Engineering, University of California, San Diego.
A. Criminisi, I. Reid & A. Zisserman (1997) "A Plane Measuring Device", §3 Computing the Plane to Plane Homography, from Visual Geometry Group, Department of Engineering Science, University of Oxford.
Elan Dubrofsky (2009) Homography Estimation, Master's thesis, from Department of Computer Science, University of British Columbia.
Richard Hartley & Andrew Zisserman (2004) Multiple View Geometry from Visual Geometry Group, Oxford. Includes Matlab Functions for calculating a homography and the fundamental matrix (computer vision).
GIMP Tutorial – using the Perspective Tool by Billy Kerr on YouTube. Shows how to do a perspective transform using GIMP.
Allan Jepson (2010) Planar Homographies from Department of Computer Science, University of Toronto. Includes 2D homography from four pairs of corresponding points, mosaics in image processing, removing perspective distortion in computer vision, rendering textures in computer graphics, and computing planar shadows.
Plane transfer homography Course notes from CSE576 at University of Washington in Seattle.
Etienne Vincent & Robert Laganiere (2000) Detecting Planar Homographies in an Image Pair Archived 2016-03-04 at the Wayback Machine from School of Information Technology and Engineering, University of Ottawa. Describes an algorithm for detecting planes in images, uses random sample consensus (RANSAC) method, describes heuristics and iteration. Computer vision syndrome (CVS) is a condition resulting from focusing the eyes on a computer or other display device for protracted, uninterrupted periods of time and the eye's muscles being unable to recover from the constant tension required to maintain focus on a close object.
Some symptoms of CVS include headaches, blurred vision, neck pain, fatigue, eye strain, dry eyes, irritated eyes, double vision, vertigo/dizziness, polyopia, and difficulty refocusing the eyes. These symptoms can be further aggravated by improper lighting conditions (i.e. glare, strong blue-spectrum backlights, or bright overhead lighting) or air moving past the eyes (e.g. overhead vents, or direct air from a fan).
Asthenopic (eye strain) symptoms in the eye are responsible for much of the severity in CVS. Proper rest to the eye and its muscles is recommended to relieve the associated eye strain. Observations from persons experiencing chronic eye strain have shown that most people who claim to be getting enough sleep are actually not. This, unaware to them, causes the eye strain to build up over a period of time, when if they had obtained seven to eight hours of uninterrupted sleep, their eye muscles would have recovered during the sleep and the strain would not have built up.
Computer workers are often advised to take breaks and look at distant objects. A routinely recommended approach is to consciously blink the eyes every now and then (this helps replenish the tear film) and to look out the window to a distant object or to the sky—doing so provides rest to the ciliary muscles. One of the catch phrases is the "20–20–20 rule": every 20 minutes, focus the eyes on an object 20 feet (6 meters) away for 20 seconds. This basically gives a convenient distance and timeframe for a person to follow the advice from the optometrist and ophthalmologist.
A number of computer and smartphone applications adjust the computer video color temperature, reducing the amount of blue light emitted by the screen, particularly at night.
Dry eye is a symptom that is targeted in the therapy of CVS. The use of over-the-counter artificial-tear solutions can reduce the effects of dry eye in CVS. Prior to using artificial tear solutions, it is necessary to check if dry eye is the actual cause of the problem (measured by a tear meniscus test) or whether there are no actual symptoms of dry eye at all.
Dry eyes because of CVS can also be treated using moisture chamber glasses or humidifier machines. Office spaces with artificially dry air can worsen CVS syndromes, in which case, a desktop or a room humidifier can help the eyes keep a healthy moisture level.
At night, CVS can become worse. It is recommended to use a dark user interface while working at night on the computer. Several browser and OS settings or add-ons exist to darken the user interface.
A 2017 randomized controlled trial evaluated macular carotenoid supplements (lutein, zeaxanthin, and mesozeaxanthin) in people with high screen time usage. The supplement group had statistically significant reduction in self-reported headache, eye strain, eye fatigue and sleep complaints, but no reduction in neck strain or blurry vision.
A 2021 review investigated suggested therapies for CVS and found little supporting evidence for the following: switching to bi- or multi-focal glasses to reduce eye strain, or using glasses that block blue light. The same review reported "low-certainty" in omega-3 supplements as a method to combat CVS.
Decreased focusing capability is mitigated by wearing a small plus-powered (+1.00 to +1.50) over-the-counter pair of eyeglasses. Wearing these eyeglasses helps such patients regain their ability to focus on near objects. People who are engaged in other occupations—such as tailors engaged in embroidery—can experience similar symptoms and can be helped by these glasses.
A Pacific University research study of 36 participants found significant differences in irritation or burning of the eyes, tearing, or watery eyes, dry eyes, and tired eyes, that were each improved by amber colored lenses versus placebo lenses, but in a follow-up study in 2008, the same team was not able to reproduce the results of the first study.
A study sponsored by the lens industry has shown blue light-filtering lenses decrease specific aspects of light emissions. Theoretical reductions in phototoxicity were 10.6% to 23.6%. Additionally, melatonin suppression was reduced by 5.8% to 15.0% and scotopic sensitivity by 2.4% to 9.6%. Over 70% of the participants in this testing were unable to detect these changes. The expansion of technology has led to more individuals utilizing computers and televisions which increase the overall exposure to blue light. Double-blind trials however, have shown no evidence to support the use of blue light filtering lenses for digital eye strain caused by blue light from electronic screens.
Amber-tinted lenses have been shown to affect the circadian rhythm and treat delayed sleep phase disorder.
According to the US National Institute for Occupational Safety and Health, computer vision syndrome affects about 90% of the people who spend three hours or more a day at a computer.
Another study in Malaysia was conducted on 795 university students aged between 18 and 25. The students experienced headaches along with eyestrain, with 89.9% of the students surveyed feeling any type of symptom of CVS.
Asthenopia (eye strain)
Computer-induced medical problems
Effects of blue light technology
Electronic media and sleep
List of repetitive strain injury software (break reminders)
Repetitive strain injury
Visual looming syndrome
"Computer Vision Syndrome (CVS)". American Optometric Association. 2017.
Yan, Zheng; Hu, Liang; Chen, Hao; Lu, Han (September 2008). "Computer Vision Syndrome: A widely spreading but largely unknown epidemic among users". Computers in Human Behavior. 24 (5): 2026–2042. doi:10.1016/j.chb.2007.09.004. Computer vision dazzle, also known as CV dazzle, dazzle makeup, or anti-surveillance makeup, is a type of camouflage used to hamper facial recognition software, inspired by dazzle camouflage used by vehicles such as ships and planes.
CV dazzle combines stylized makeup, asymmetric hair, and sometimes infrared lights built in to glasses or clothing to break up detectable facial patterns recognized by computer vision algorithms in much the same way that warships contrasted color and used sloping lines and curves to distort the structure of a vessel.
It has been shown to be somewhat successful at defeating face detection software in common use, including that employed by Facebook. CV dazzle attempts to block detection by facial recognition technologies such as DeepFace "by creating an 'anti-face'". It uses occlusion, covering certain facial features; transformation, altering the shape or colour of parts of the face; and a combination of the two. Prominent artists employing this technique include Adam Harvey and Jillian Mayer.
Computer vision dazzle makeup has been used by rioters in several different protest movements. Its use as a protesting aid has often been found ineffective. It may be effective to thwart computer technology, but draws human attention, is easy for human monitors to spot on security cameras, and makes it hard for rioters to blend in within a crowd. Advances in facial recognition technology make dazzle makeup increasingly ineffective.
Adversarial machine learning In the fields of computing and computer vision, pose (or spatial pose) represents the position and the orientation of an object, each usually in three dimensions. Poses are often stored internally as transformation matrices. The term “pose” is largely synonymous with the term “transform”, but a transform may often include scale, whereas pose does not.
In computer vision, the pose of an object is often estimated from camera input by the process of pose estimation. This information can then be used, for example, to allow a robot to manipulate an object or to avoid moving into the object based on its perceived position and orientation in the environment. Other applications include skeletal action recognition.
The specific task of determining the pose of an object in an image (or stereo images, image sequence) is referred to as pose estimation. Pose estimation problems can be solved in different ways depending on the image sensor configuration, and choice of methodology. Three classes of methodologies can be distinguished:
Analytic or geometric methods: Given that the image sensor (camera) is calibrated and the mapping from 3D points in the scene and 2D points in the image is known. If also the geometry of the object is known, it means that the projected image of the object on the camera image is a well-known function of the object's pose. Once a set of control points on the object, typically corners or other feature points, has been identified, it is then possible to solve the pose transformation from a set of equations which relate the 3D coordinates of the points with their 2D image coordinates. Algorithms that determine the pose of a point cloud with respect to another point cloud are known as point set registration algorithms, if the correspondences between points are not already known.
Genetic algorithm methods: If the pose of an object does not have to be computed in real-time a genetic algorithm may be used. This approach is robust especially when the images are not perfectly calibrated. In this particular case, the pose represent the genetic representation and the error between the projection of the object control points with the image is the fitness function.
Learning-based methods: These methods use artificial learning-based system which learn the mapping from 2D image features to pose transformation. In short, this means that a sufficiently large set of images of the object, in different poses, must be presented to the system during a learning phase. Once the learning phase is completed, the system should be able to present an estimate of the object's pose given an image of the object.
Homography (computer vision)
Structure from motion
Essential matrix and Trifocal tensor (relative pose) In computer vision, triangulation refers to the process of determining a point in 3D space given its projections onto two, or more, images. In order to solve this problem it is necessary to know the parameters of the camera projection function from 3D to 2D for the cameras involved, in the simplest case represented by the camera matrices. Triangulation is sometimes also referred to as reconstruction or intersection.
The triangulation problem is in principle trivial. Since each point in an image corresponds to a line in 3D space, all points on the line in 3D are projected to the point in the image. If a pair of corresponding points in two, or more images, can be found it must be the case that they are the projection of a common 3D point x. The set of lines generated by the image points must intersect at x (3D point) and the algebraic formulation of the coordinates of x (3D point) can be computed in a variety of ways, as is presented below.
In practice, however, the coordinates of image points cannot be measured with arbitrary accuracy. Instead, various types of noise, such as geometric noise from lens distortion or interest point detection error, lead to inaccuracies in the measured image coordinates. As a consequence, the lines generated by the corresponding image points do not always intersect in 3D space. The problem, then, is to find a 3D point which optimally fits the measured image points. In the literature there are multiple proposals for how to define optimality and how to find the optimal 3D point. Since they are based on different optimality criteria, the various methods produce different estimates of the 3D point x when noise is involved.
In the following, it is assumed that triangulation is made on corresponding image points from two views generated by pinhole cameras.
The image to the left illustrates the epipolar geometry of a pair of stereo cameras of pinhole model. A point x (3D point) in 3D space is projected onto the respective image plane along a line (green) which goes through the camera's focal point,
 _
 _
, resulting in the two corresponding image points
 _
 _
 _
 _
are given and the geometry of the two cameras are known, the two projection lines (green lines) can be determined and it must be the case that they intersect at point x (3D point). Using basic linear algebra that intersection point can be determined in a straightforward way.
The image to the right shows the real case. The position of the image points
 _
 _
cannot be measured exactly. The reason is a combination of factors such as
Geometric distortion, for example lens distortion, which means that the 3D to 2D mapping of the camera deviates from the pinhole camera model. To some extent these errors can be compensated for, leaving a residual geometric error.
A single ray of light from x (3D point) is dispersed in the lens system of the cameras according to a point spread function. The recovery of the corresponding image point from measurements of the dispersed intensity function in the images gives errors.
In a digital camera, the image intensity function is only measured in discrete sensor elements. Inexact interpolation of the discrete intensity function have to be used to recover the true one.
The image points y1' and y2' used for triangulation are often found using various types of feature extractors, for example of corners or interest points in general. There is an inherent localization error for any type of feature extraction based on neighborhood operations.
As a consequence, the measured image points are
 '_
 '_
 _
 _
. However, their projection lines (blue) do not have to intersect in 3D space or come close to x. In fact, these lines intersect if and only if
 '_
 '_
satisfy the epipolar constraint defined by the fundamental matrix. Given the measurement noise in
 '_
 '_
it is rather likely that the epipolar constraint is not satisfied and the projection lines do not intersect.
This observation leads to the problem which is solved in triangulation. Which 3D point xest is the best estimate of x given
 '_
 '_
and the geometry of the cameras? The answer is often found by defining an error measure which depends on xest and then minimizing this error. In the following sections, some of the various methods for computing xest presented in the literature are briefly described.
All triangulation methods produce xest = x in the case that
 _=\mathbf  '_
 _=\mathbf  '_
, that is, when the epipolar constraint is satisfied (except for singular points, see below). It is what happens when the constraint is not satisfied which differs between the methods.
A triangulation method can be described in terms of a function

 \sim \tau (\mathbf  '_,\mathbf  '_,\mathbf  _,\mathbf  _)
 '_,\mathbf  '_
are the homogeneous coordinates of the detected image points and
 _,\mathbf  _
are the camera matrices. x (3D point) is the homogeneous representation of the resulting 3D point. The

sign implies that

is only required to produce a vector which is equal to x up to a multiplication by a non-zero scalar since homogeneous vectors are involved.
Before looking at the specific methods, that is, specific functions

, there are some general concepts related to the methods that need to be explained. Which triangulation method is chosen for a particular problem depends to some extent on these characteristics.
Some of the methods fail to correctly compute an estimate of x (3D point) if it lies in a certain subset of the 3D space, corresponding to some combination of
 '_,\mathbf  '_,\mathbf  _,\mathbf  _
. A point in this subset is then a singularity of the triangulation method. The reason for the failure can be that some equation system to be solved is under-determined or that the projective representation of xest becomes the zero vector for the singular points.
In some applications, it is desirable that the triangulation is independent of the coordinate system used to represent 3D points; if the triangulation problem is formulated in one coordinate system and then transformed into another the resulting estimate xest should transform in the same way. This property is commonly referred to as invariance. Not every triangulation method assures invariance, at least not for general types of coordinate transformations.
For a homogeneous representation of 3D coordinates, the most general transformation is a projective transformation, represented by a

 
. If the homogeneous coordinates are transformed according to
 \sim \mathbf  \,\mathbf  
then the camera matrices must transform as (Ck)
 _\sim \mathbf  _\,\mathbf  ^
to produce the same homogeneous image coordinates (yk)
 _\sim \mathbf  _\,\mathbf  =\mathbf  _\,\mathbf  
If the triangulation function

is invariant to
 
then the following relation must be valid
 _\sim \mathbf  \,\mathbf  _
from which follows that
 '_,\mathbf  '_,\mathbf  _,\mathbf  _)\sim \mathbf  ^\,\tau (\mathbf  '_,\mathbf  '_,\mathbf  _\,\mathbf  ^,\mathbf  _\,\mathbf  ^),
 '_,\mathbf  '_
For each triangulation method, it can be determined if this last relation is valid. If it is, it may be satisfied only for a subset of the projective transformations, for example, rigid or affine transformations.

is only an abstract representation of a computation which, in practice, may be relatively complex. Some methods result in a

which is a closed-form continuous function while others need to be decomposed into a series of computational steps involving, for example, SVD or finding the roots of a polynomial. Yet another class of methods results in

which must rely on iterative estimation of some parameters. This means that both the computation time and the complexity of the operations involved may vary between the different methods.
Each of the two image points
 '_
 '_
has a corresponding projection line (blue in the right image above), here denoted as
 '_
 '_
, which can be determined given the camera matrices
 _,\mathbf  _
be a distance function between a (3D line) L and a x (3D point) such that
 ,\mathbf  )
is the Euclidean distance between
 
 
The midpoint method finds the point xest which minimizes
 '_,\mathbf  )^+d(\mathbf  '_,\mathbf  )^
It turns out that xest lies exactly at the middle of the shortest line segment which joins the two projection lines.
The problem to be solved there is how to compute
given corresponding normalized image coordinates
. If the essential matrix is known and the corresponding rotation and translation transformations have been determined, this algorithm (described in Longuet-Higgins' paper) provides a solution.
 _
denote row k of the rotation matrix
 
 =-\mathbf  _-\\-\mathbf  _-\\-\mathbf  _-\end
Combining the above relations between 3D coordinates in the two coordinate systems and the mapping between 3D and 2D points described earlier gives
== _\cdot ( -\mathbf  ) _\cdot ( -\mathbf  )= _\cdot (\mathbf  -\mathbf  /x_) _\cdot (\mathbf  -\mathbf  /x_)
= _-y'_\,\mathbf  _)\cdot \mathbf   _-y'_\,\mathbf  _)\cdot \mathbf  
is determined, the other two coordinates can be computed as
The above derivation is not unique. It is also possible to start with an expression for
and derive an expression for
= _-y'_\,\mathbf  _)\cdot \mathbf   _-y'_\,\mathbf  _)\cdot \mathbf  
In the ideal case, when the camera maps the 3D points according to a perfect pinhole camera and the resulting 2D points can be detected without any noise, the two expressions for
are equal. In practice, however, they are not and it may be advantageous to combine the two estimates of
, for example, in terms of some sort of average.
There are also other types of extensions of the above computations which are possible. They started with an expression of the primed image coordinates and derived 3D coordinates in the unprimed system. It is also possible to start with unprimed image coordinates and obtain primed 3D coordinates, which finally can be transformed into unprimed 3D coordinates. Again, in the ideal case the result should be equal to the above expressions, but in practice they may deviate.
A final remark relates to the fact that if the essential matrix is determined from corresponding image coordinate, which often is the case when 3D points are determined in this way, the translation vector
 
is known only up to an unknown positive scaling. As a consequence, the reconstructed 3D points, too, are undetermined with respect to a positive scaling.
3D reconstruction from multiple images
Richard Hartley and Andrew Zisserman (2003). Multiple View Geometry in computer vision. Cambridge University Press. ISBN 978-0-521-54051-3.
Two view and multi-view triangulation in Matlab The Conference on Computer Vision and Pattern Recognition is an annual conference on computer vision and pattern recognition.
The conference was first held in 1983 in Washington, DC, organized by Takeo Kanade and Dana H. Ballard. From 1985 to 2010 it was sponsored by the IEEE Computer Society. In 2011 it was also co-sponsored by University of Colorado Colorado Springs. Since 2012 it has been co-sponsored by the IEEE Computer Society and the Computer Vision Foundation, which provides open access to the conference papers.
The conference considers a wide range of topics related to computer vision and pattern recognition—basically any topic that is extracting structures or answers from images or video or applying mathematical methods to data to extract or recognize patterns. Common topics include object recognition, image segmentation, motion estimation, 3D reconstruction, and deep learning.
The conference generally has less than 30% acceptance rates for all papers and less than 5% for oral presentations. It is managed by a rotating group of volunteers who are chosen in a public election at the Pattern Analysis and Machine Intelligence-Technical Community (PAMI-TC) meeting four years before the meeting. The conference uses a multi-tier double-blind peer review process. The program chairs, who cannot submit papers, select area chairs who manage the reviewers for their subset of submissions.
The conference is usually held in June in North America.
These awards are picked by committees delegated by the program chairs of the conference.
The Longuet-Higgins Prize recognizes papers from ten years ago that have made a significant impact on computer vision research.
The Pattern Analysis and Machine Intelligence Young Researcher Award is an award given by the Technical Committee on Pattern Analysis and Machine Intelligence of the IEEE Computer Society to a researcher within 7 years of completing their Ph.D. for outstanding early career research contributions. Candidates are nominated by the computer vision community, with winners selected by a committee of senior researchers in the field. This award was originally instituted in 2012 by the journal Image and Vision Computing, also presented at the conference, and the journal continues to sponsor the award.
The Thomas Huang Memorial Prize was established at the 2020 conference and is awarded annually starting from 2021 to honor researchers who are recognized as examples in research, teaching/mentoring, and service to the computer vision community.
International Conference on Computer Vision
European Conference on Computer Vision
2020 conference website
2019 conference website      Gradient boosting is a machine learning technique based on boosting in a functional space, where the target is pseudo-residuals instead of residuals as in traditional boosting. It gives a prediction model in the form of an ensemble of weak prediction models, i.e., models that make very few assumptions about the data, which are typically simple decision trees. When a decision tree is the weak learner, the resulting algorithm is called gradient-boosted trees; it usually outperforms random forest. As with other boosting methods, a gradient-boosted trees model is built in stages, but it generalizes the other methods by allowing optimization of an arbitrary differentiable loss function.
The idea of gradient boosting originated in the observation by Leo Breiman that boosting can be interpreted as an optimization algorithm on a suitable cost function. Explicit regression gradient boosting algorithms were subsequently developed, by Jerome H. Friedman, (in 1999 and later in 2001) simultaneously with the more general functional gradient boosting perspective of Llew Mason, Jonathan Baxter, Peter Bartlett and Marcus Frean.
The latter two papers introduced the view of boosting algorithms as iterative functional gradient descent algorithms. That is, algorithms that optimize a cost function over function space by iteratively choosing a function (weak hypothesis) that points in the negative gradient direction. This functional gradient view of boosting has led to the development of boosting algorithms in many areas of machine learning and statistics beyond regression and classification.
(This section follows the exposition by Cheng Li.)
Like other boosting methods, gradient boosting combines weak "learners" into a single strong learner iteratively. It is easiest to explain in the least-squares regression setting, where the goal is to teach a model
to predict values of the form
=F(x)
by minimizing the mean squared error
\sum _(_-y_)^
indexes over some training set of size
of actual values of the output variable
_=
the predicted value
the observed value
the number of samples in
If the algorithm has
stages, at each stage

), suppose some imperfect model
, this model may simply predict
_

, the mean of
). In order to improve
, our algorithm should add some new estimator,
Therefore, gradient boosting will fit
to the residual
. As in other boosting variants, each
attempts to correct the errors of its predecessor
. A generalization of this idea to loss functions other than squared error, and to classification and ranking problems, follows from the observation that residuals
for a given model are proportional to the negative gradients of the mean squared error (MSE) loss function (with respect to
=\sum _^\left(y_-F(x_)\right)^
)=(y_-F(x_))=h_(x_)
So, gradient boosting could be generalized to a gradient descent algorithm by plugging in a different loss and its gradient.
Many supervised learning problems involve an output variable y and a vector of input variables x, related to each other with some probabilistic distribution. The goal is to find some function
(x)
that best approximates the output variable from the values of input variables. This is formalized by introducing some loss function
and minimizing it in expectation:
=\,\mathbb  _[L(y,F(x))]
The gradient boosting method assumes a real-valued y. It seeks an approximation
(x)
in the form of a weighted sum of M functions
from some class

, called base (or weak) learners:
(x)=\sum _^\gamma _h_(x)+

is the weight at stage
. We are usually given a training set
,y_),\dots ,(x_,y_)\
of known values of x and corresponding values of y. In accordance with the empirical risk minimization principle, the method tries to find an approximation
(x)
that minimizes the average value of the loss function on the training set, i.e., minimizes the empirical risk. It does so by starting with a model, consisting of a constant function
, and incrementally expands it in a greedy fashion:
(x)=\in ^,h_(x_))
(x)=F_(x)+\left(\in  \left[^,F_(x_)+h_(x_))\right]\right)(x)

\in 
is a base learner function.
Unfortunately, choosing the best function
at each step for an arbitrary loss function L is a computationally infeasible optimization problem in general. Therefore, we restrict our approach to a simplified version of the problem. The idea is to apply a steepest descent step to this minimization problem (functional gradient descent). The basic idea is to find a local minimum of the loss function by iterating on
. In fact, the local maximum-descent direction of the loss function is the negative gradient. Hence, moving a small amount

such that the linear approximation remains valid:
(x)=F_(x)-\gamma \sum _^L(y_,F_(x_))

. For small

, this implies that
,F_(x_))\leq L(y_,F_(x_))
Furthermore, we can optimize

by finding the

value for which the loss function has a minimum:
=^,F_(x_))=^,F_(x_)-\gamma \nabla _L(y_,F_(x_))\right).
If we considered the continuous case, i.e., where

is the set of arbitrary differentiable functions on
 
, we would update the model in accordance with the following equations
(x)=F_(x)-\gamma _\sum _^L(y_,F_(x_))

is the step length, defined as
=^,F_(x_)-\gamma \nabla _L(y_,F_(x_))\right).
In the discrete case however, i.e. when the set

is finite, we choose the candidate function h closest to the gradient of L for which the coefficient γ may then be calculated with the aid of line search on the above equations. Note that this approach is a heuristic and therefore doesn't yield an exact solution to the given problem, but rather an approximation.
In pseudocode, the generic gradient boosting method is:
Gradient boosting is typically used with decision trees (especially CARTs) of a fixed size as base learners. For this special case, Friedman proposes a modification to gradient boosting method which improves the quality of fit of each base learner.
Generic gradient boosting at the m-th step would fit a decision tree
to pseudo-residuals. Let
be the number of its leaves. The tree partitions the input space into
,\ldots ,R_m
and predicts a constant value in each region. Using the indicator notation, the output of
for input x can be written as the sum:
(x)=\sum _^b_\mathbf  _(x),
is the value predicted in the region
Then the coefficients
are multiplied by some value

, chosen using line search so as to minimize the loss function, and the model is updated as follows:
(x)=F_(x)+\gamma _h_(x),\quad \gamma _= \sum _^L(y_,F_(x_)+\gamma h_(x_)).
Friedman proposes to modify this algorithm so that it chooses a separate optimal value

for each of the tree's regions, instead of a single

for the whole tree. He calls the modified algorithm "TreeBoost". The coefficients
from the tree-fitting procedure can be then simply discarded and the model update rule becomes:
(x)=F_(x)+\sum _^\gamma _\mathbf  _(x),\quad \gamma _= \sum _\in R_L(y_,F_(x_)+\gamma ).
When the loss

is mean-squared error (MSE) the coefficients

coincide with the coefficients of the tree-fitting procedure
of terminal nodes in the trees is a parameter which controls the maximum allowed level of interaction between variables in the model. With
(decision stumps), no interaction between variables is allowed. With
the model may include effects of the interaction between up to two variables, and so on.
can be adjusted for a data set at hand.
Hastie et al. comment that typically

work well for boosting and results are fairly insensitive to the choice of
in this range,
is insufficient for many applications, and
is unlikely to be required.
Fitting the training set too closely can lead to degradation of the model's generalization ability, that is, its performance on unseen examples. Several so-called regularization techniques reduce this overfitting effect by constraining the fitting procedure.
One natural regularization parameter is the number of gradient boosting iterations M (i.e. the number of base models). Increasing M reduces the error on training set, but increases risk of overfitting. An optimal value of M is often selected by monitoring prediction error on a separate validation data set.
Another regularization parameter for tree boosting is tree depth. The higher this value the more likely the model will overfit the training data.
An important part of gradient boosting is regularization by shrinkage which uses a modified update rule:
(x)=F_(x)+\nu \cdot \gamma _h_(x),\quad 0<\nu \leq 1,

is called the "learning rate".
Empirically, it has been found that using small learning rates (such as

) yields dramatic improvements in models' generalization ability over gradient boosting without shrinking (

). However, it comes at the price of increasing computational time both during training and querying: lower learning rate requires more iterations.
Soon after the introduction of gradient boosting, Friedman proposed a minor modification to the algorithm, motivated by Breiman's bootstrap aggregation ("bagging") method. Specifically, he proposed that at each iteration of the algorithm, a base learner should be fit on a subsample of the training set drawn at random without replacement. Friedman observed a substantial improvement in gradient boosting's accuracy with this modification.
Subsample size is some constant fraction
of the size of the training set. When
, the algorithm is deterministic and identical to the one described above. Smaller values of
introduce randomness into the algorithm and help prevent overfitting, acting as a kind of regularization. The algorithm also becomes faster, because regression trees have to be fit to smaller datasets at each iteration. Friedman obtained that

leads to good results for small and moderate sized training sets. Therefore,
is typically set to 0.5, meaning that one half of the training set is used to build each base learner.
Also, like in bagging, subsampling allows one to define an out-of-bag error of the prediction performance improvement by evaluating predictions on those observations which were not used in the building of the next base learner. Out-of-bag estimates help avoid the need for an independent validation dataset, but often underestimate actual performance improvement and the optimal number of iterations.
Gradient tree boosting implementations often also use regularization by limiting the minimum number of observations in trees' terminal nodes. It is used in the tree building process by ignoring any splits that lead to nodes containing fewer than this number of training set instances.
Imposing this limit helps to reduce variance in predictions at leaves.
Another useful regularization technique for gradient boosted model is to penalize its complexity. For gradient boosted trees, model complexity can be defined as the proportional number of leaves in the trees. The joint optimization of loss and model complexity corresponds to a post-pruning algorithm to remove branches that fail to reduce the loss by a threshold.
Other kinds of regularization such as an

penalty on the leaf values can also be used to avoid overfitting.
Gradient boosting can be used in the field of learning to rank. The commercial web search engines Yahoo and Yandex use variants of gradient boosting in their machine-learned ranking engines. Gradient boosting is also utilized in High Energy Physics in data analysis. At the Large Hadron Collider (LHC), variants of gradient boosting Deep Neural Networks (DNN) were successful in reproducing the results of non-machine learning methods of analysis on datasets used to discover the Higgs boson. Gradient boosting decision tree was also applied in earth and geological studies – for example quality evaluation of sandstone reservoir.
The method goes by a variety of names. Friedman introduced his regression technique as a "Gradient Boosting Machine" (GBM). Mason, Baxter et al. described the generalized abstract class of algorithms as "functional gradient boosting". Friedman et al. describe an advancement of gradient boosted models as Multiple Additive Regression Trees (MART); Elith et al. describe that approach as "Boosted Regression Trees" (BRT).
A popular open-source implementation for R calls it a "Generalized Boosting Model", however packages expanding this work use BRT. Yet another name is TreeNet, after an early commercial implementation from Salford System's Dan Steinberg, one of researchers who pioneered the use of tree-based methods.
Gradient boosting can be used for feature importance ranking, which is usually based on aggregating importance function of the base learners. For example, if a gradient boosted trees algorithm is developed using entropy-based decision trees, the ensemble algorithm ranks the importance of features based on entropy as well with the caveat that it is averaged out over all base learners.
While boosting can increase the accuracy of a base learner, such as a decision tree or linear regression, it sacrifices intelligibility and interpretability. For example, following the path that a decision tree takes to make its decision is trivial and self-explained, but following the paths of hundreds or thousands of trees is much harder. To achieve both performance and interpretability, some model compression techniques allow transforming an XGBoost into a single "born-again" decision tree that approximates the same decision function. Furthermore, its implementation may be more difficult due to the higher computational demand.
Decision tree learning
Boehmke, Bradley; Greenwell, Brandon (2019). "Gradient Boosting". Hands-On Machine Learning with R. Chapman & Hall. pp. 221–245. ISBN 978-1-138-49568-5.
How to explain gradient boosting XGBoost (eXtreme Gradient Boosting) is an open-source software library which provides a regularizing gradient boosting framework for C++, Java, Python, R, Julia, Perl, and Scala. It works on Linux, Microsoft Windows, and macOS. From the project description, it aims to provide a "Scalable, Portable and Distributed Gradient Boosting (GBM, GBRT, GBDT) Library". It runs on a single machine, as well as the distributed processing frameworks Apache Hadoop, Apache Spark, Apache Flink, and Dask.
XGBoost gained much popularity and attention in the mid-2010s as the algorithm of choice for many winning teams of machine learning competitions.
XGBoost initially started as a research project by Tianqi Chen as part of the Distributed (Deep) Machine Learning Community (DMLC) group at the University of Washington. Initially, it began as a terminal application which could be configured using a libsvm configuration file. It became well known in the ML competition circles after its use in the winning solution of the Higgs Machine Learning Challenge. Soon after, the Python and R packages were built, and XGBoost now has package implementations for Java, Scala, Julia, Perl, and other languages. This brought the library to more developers and contributed to its popularity among the Kaggle community, where it has been used for a large number of competitions.
It was soon integrated with a number of other packages making it easier to use in their respective communities. It has now been integrated with scikit-learn for Python users and with the caret package for R users. It can also be integrated into Data Flow frameworks like Apache Spark, Apache Hadoop, and Apache Flink using the abstracted Rabit and XGBoost4J. XGBoost is also available on OpenCL for FPGAs. An efficient, scalable implementation of XGBoost has been published by Tianqi Chen and Carlos Guestrin.
While the XGBoost model often achieves higher accuracy than a single decision tree, it sacrifices the intrinsic interpretability of decision trees. For example, following the path that a decision tree takes to make its decision is trivial and self-explained, but following the paths of hundreds or thousands of trees is much harder.
Salient features of XGBoost which make it different from other gradient boosting algorithms include:
Clever penalization of trees
A proportional shrinking of leaf nodes
Extra randomization parameter
Implementation on single, distributed systems and out-of-core computation
Automatic feature selection
Theoretically justified weighted quantile sketching for efficient computation
Parallel tree structure boosting with sparsity
Efficient cacheable block structure for decision tree training
XGBoost works as Newton–Raphson in function space unlike gradient boosting that works as gradient descent in function space, a second order Taylor approximation is used in the loss function to make the connection to Newton–Raphson method.
A generic unregularized XGBoost algorithm is:
John Chambers Award (2016)
High Energy Physics meets Machine Learning award (HEP meets ML) (2016) LightGBM, short for Light Gradient-Boosting Machine, is a free and open-source distributed gradient-boosting framework for machine learning, originally developed by Microsoft. It is based on decision tree algorithms and used for ranking, classification and other machine learning tasks. The development focus is on performance and scalability.
The LightGBM framework supports different algorithms including GBT, GBDT, GBRT, GBM, MART and RF. LightGBM has many of XGBoost's advantages, including sparse optimization, parallel training, multiple loss functions, regularization, bagging, and early stopping. A major difference between the two lies in the construction of trees. LightGBM does not grow a tree level-wise — row by row — as most other implementations do. Instead it grows trees leaf-wise. It will choose the leaf with max delta loss to grow. Besides, LightGBM does not use the widely used sorted-based decision tree learning algorithm, which searches the best split point on sorted feature values, as XGBoost or other implementations do. Instead, LightGBM implements a highly optimized histogram-based decision tree learning algorithm, which yields great advantages on both efficiency and memory consumption. The LightGBM algorithm utilizes two novel techniques called Gradient-Based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB) which allow the algorithm to run faster while maintaining a high level of accuracy.
LightGBM works on Linux, Windows, and macOS and supports C++, Python, R, and C#. The source code is licensed under MIT License and available on GitHub.
When using gradient descent, one thinks about the space of possible configurations of the model as a valley, in which the lowest part of the valley is the model which most closely fits the data. In this metaphor, one walks in different directions to learn how much lower the valley becomes.
Typically, in gradient descent, one uses the whole set of data to calculate the valley's slopes. However, this commonly used method assumes that every data point is equally informative.
By contrast, Gradient-Based One-Side Sampling (GOSS), a method first developed for gradient-boosted decision trees, does not rely on the assumption that all data are equally informative. Instead, it treats data points with smaller gradients (shallower slopes) as less informative by randomly dropping them. This is intended to filter out data which may have been influenced by noise, allowing the model to more accurately model the underlying relationships in the data.
Exclusive feature bundling (EFB) is a near-lossless method to reduce the number of effective features. In a sparse feature space many features are nearly exclusive, implying they rarely take nonzero values simultaneously. One-hot encoded features are a perfect example of exclusive features. EFB bundles these features, reducing dimensionality to improve efficiency while maintaining a high level of accuracy. The bundle of exclusive features into a single feature is called an exclusive feature bundle.
Guolin Ke; Qi Meng; Thomas Finely; Taifeng Wang; Wei Chen; Weidong Ma; Qiwei Ye; Tie-Yan Liu (2017). "LightGBM: A Highly Efficient Gradient Boosting Decision Tree" (PDF). Neural Information Processing System.
Quinto, Butch (2020). Next-Generation Machine Learning with Spark – Covers XGBoost, LightGBM, Spark NLP, Distributed Deep Learning with Keras, and More. Apress. ISBN 978-1-4842-5668-8.
van Wyk, Andrich (2023). Machine Learning with LightGBM and Python. Packt Publishing. ISBN 978-1800564749.
GitHub - microsoft/LightGBM
LightGBM - Microsoft Research CatBoost is an open-source software library developed by Yandex. It provides a gradient boosting framework which, among other features, attempts to solve for categorical features using a permutation-driven alternative to the classical algorithm. It works on Linux, Windows, macOS, and is available in
R, and models built using CatBoost can be used for predictions in C++, Java, C#, Rust, Core ML, ONNX, and PMML. The source code is licensed under Apache License and available on GitHub.
InfoWorld magazine awarded the library "The best machine learning tools" in 2017. along with TensorFlow, Pytorch, XGBoost and 8 other libraries.
Kaggle listed CatBoost as one of the most frequently used machine learning (ML) frameworks in the world. It was listed as the top-8 most frequently used ML framework in the 2020 survey and as the top-7 most frequently used ML framework in the 2021 survey.
As of April 2022, CatBoost is installed about 100000 times per day from PyPI repository
CatBoost has gained popularity compared to other gradient boosting algorithms primarily due to the following features
Native handling for categorical features
Fast GPU training
Visualizations and tools for model and feature analysis
Using oblivious trees or symmetric trees for faster execution
Ordered boosting to overcome overfitting
In 2009 Andrey Gulin developed MatrixNet, a proprietary gradient boosting library that was used in Yandex to rank search results.
Since 2009 MatrixNet has been used in different projects in Yandex, including recommendation systems and weather prediction.
In 2014–2015 Andrey Gulin with a team of researchers has started a new project called Tensornet that was aimed at solving the problem of "how to work with categorical data". It resulted in several proprietary Gradient Boosting libraries with different approaches to handling categorical data.
In 2016 Machine Learning Infrastructure team led by Anna Dorogush started working on Gradient Boosting in Yandex, including Matrixnet and Tensornet. They implemented and open-sourced the next version of Gradient Boosting library called CatBoost, which has support of categorical and text data, GPU training, model analysis, visualization tools.
CatBoost was open-sourced in July 2017 and is under active development in Yandex and the open-source community.
JetBrains uses CatBoost for code completion
Cloudflare uses CatBoost for bot detection
Careem uses CatBoost to predict future destinations of the rides
GitHub - catboost/catboost
CatBoost - Yandex Technology Automatic clustering algorithms are algorithms that can perform clustering without prior knowledge of data sets. In contrast with other clustering techniques, automatic clustering algorithms can determine the optimal number of clusters even in the presence of noise and outliers.
Given a set of n objects, centroid-based algorithms create k partitions based on a dissimilarity function, such that k≤n. A major problem in applying this type of algorithm is determining the appropriate number of clusters for unlabeled data. Therefore, most research in clustering analysis has been focused on the automation of the process.
Automated selection of k in a K-means clustering algorithm, one of the most used centroid-based clustering algorithms, is still a major problem in machine learning. The most accepted solution to this problem is the elbow method. It consists of running k-means clustering to the data set with a range of values, calculating the sum of squared errors for each, and plotting them in a line chart. If the chart looks like an arm, the best value of k will be on the "elbow".
Another method that modifies the k-means algorithm for automatically choosing the optimal number of clusters is the G-means algorithm. It was developed from the hypothesis that a subset of the data follows a Gaussian distribution. Thus, k is increased until each k-means center's data is Gaussian. This algorithm only requires the standard statistical significance level as a parameter and does not set limits for the covariance of the data.
Connectivity-based clustering or hierarchical clustering is based on the idea that objects have more similarities to other nearby objects than to those further away. Therefore, the generated clusters from this type of algorithm will be the result of the distance between the analyzed objects.
Hierarchical models can either be divisive, where partitions are built from the entire data set available, or agglomerating, where each partition begins with a single object and additional objects are added to the set. Although hierarchical clustering has the advantage of allowing any valid metric to be used as the defined distance, it is sensitive to noise and fluctuations in the data set and is more difficult to automate.
Methods have been developed to improve and automate existing hierarchical clustering algorithms such as an automated version of single linkage hierarchical cluster analysis (HCA). This computerized method bases its success on a self-consistent outlier reduction approach followed by the building of a descriptive function which permits defining natural clusters. Discarded objects can also be assigned to these clusters. Essentially, one needs not to resort to external parameters to identify natural clusters. Information gathered from HCA, automated and reliable, can be resumed in a dendrogram with the number of natural clusters and the corresponding separation, an option not found in classical HCA. This method includes the two following steps: outliers being removed (this is applied in many filtering applications) and an optional classification allowing expanding clusters with the whole set of objects.
BIRCH (balanced iterative reducing and clustering using hierarchies) is an algorithm used to perform connectivity-based clustering for large data-sets. It is regarded as one of the fastest clustering algorithms, but it is limited because it requires the number of clusters as an input. Therefore, new algorithms based on BIRCH have been developed in which there is no need to provide the cluster count from the beginning, but that preserves the quality and speed of the clusters. The main modification is to remove the final step of BIRCH, where the user had to input the cluster count, and to improve the rest of the algorithm, referred to as tree-BIRCH, by optimizing a threshold parameter from the data. In this resulting algorithm, the threshold parameter is calculated from the maximum cluster radius and the minimum distance between clusters, which are often known. This method proved to be efficient for data sets of tens of thousands of clusters. If going beyond that amount, a supercluster splitting problem is introduced. For this, other algorithms have been developed, like MDB-BIRCH, which reduces super cluster splitting with relatively high speed.
Unlike partitioning and hierarchical methods, density-based clustering algorithms are able to find clusters of any arbitrary shape, not only spheres.
The density-based clustering algorithm uses autonomous machine learning that identifies patterns regarding geographical location and distance to a particular number of neighbors. It is considered autonomous because a priori knowledge on what is a cluster is not required. This type of algorithm provides different methods to find clusters in the data. The fastest method is DBSCAN, which uses a defined distance to differentiate between dense groups of information and sparser noise. Moreover, HDBSCAN can self-adjust by using a range of distances instead of a specified one. Lastly, the method OPTICS creates a reachability plot based on the distance from neighboring features to separate noise from clusters of varying density.
These methods still require the user to provide the cluster center and cannot be considered automatic. The Automatic Local Density Clustering Algorithm (ALDC) is an example of the new research focused on developing automatic density-based clustering. ALDC works out local density and distance deviation of every point, thus expanding the difference between the potential cluster center and other points. This expansion allows the machine to work automatically. The machine identifies cluster centers and assigns the points that are left by their closest neighbor of higher density.
In the automation of data density to identify clusters, research has also been focused on artificially generating the algorithms. For instance, the Estimation of Distribution Algorithms guarantees the generation of valid algorithms by the directed acyclic graph (DAG), in which nodes represent procedures (building block) and edges represent possible execution sequences between two nodes. Building Blocks determine the EDA's alphabet or, in other words, any generated algorithm. Clustering algorithms artificially generated are compared to DBSCAN, a manual algorithm, in experimental results.
Recent advancements in automated machine learning (AutoML) have extended to the domain of clustering, where systems are designed to automatically select preprocessing techniques, feature transformations, clustering algorithms, and validation strategies without human intervention. Unlike traditional clustering methods that rely on fixed pipelines and manual tuning, AutoML-based clustering frameworks dynamically search for the best-performing configurations based on internal clustering validation indices (CVIs) or other unsupervised metrics.
An implementation in this area is TPOT-Clustering, an extension of the Tree-based Pipeline Optimization Tool (TPOT), which automates the process of building clustering pipelines using genetic programming. TPOT-Clustering explores combinations of data transformations, dimensionality reduction methods, clustering algorithms (e.g., K-means, DBSCAN, Agglomerative Clustering), and scoring functions to optimize clustering performance. It leverages an evolutionary algorithm to search the space of possible pipelines, using internal scores such as silhouette or Davies–Bouldin index to guide the selection process.
AutoML for clustering is particularly useful in domains where the structure of the data is unknown and manual tuning is infeasible due to the high dimensionality or complexity of the feature space. These approaches are gaining popularity in areas such as image segmentation, customer segmentation, and bioinformatics, where unsupervised insights are critical. k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid). This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.
The problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation–maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.
The unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.
Given a set of observations (x1, x2, ..., xn), where each observation is a
-dimensional real vector, k-means clustering aims to partition the n observations into k (≤ n) sets S =  so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). Formally, the objective is to find:
  _ \sum _^\sum _ \in S_\left\|\mathbf  -_\right\|^=\mathop   _ \sum _^|S_|\operatorname  S_
where μi is the mean (also called centroid) of points in
=|\sum _ \in S_\mathbf  ,
is the size of

is the usual L2 norm . This is equivalent to minimizing the pairwise squared deviations of points in the same cluster:
  _ \sum _^\,|\,\sum _ ,\mathbf  \in S_\left\|\mathbf  -\mathbf  \right\|^
The equivalence can be deduced from identity
|\sum _ \in S_\left\|\mathbf  -_\right\|^=\sum _ ,\mathbf  \in S_\left\|\mathbf  -\mathbf  \right\|^
. Since the total variance is constant, this is equivalent to maximizing the sum of squared deviations between points in different clusters (between-cluster sum of squares, BCSS). This deterministic relationship is also related to the law of total variance in probability theory.
The term "k-means" was first used by James MacQueen in 1967, though the idea goes back to Hugo Steinhaus in 1956. The standard algorithm was first proposed by Stuart Lloyd of Bell Labs in 1957 as a technique for pulse-code modulation, although it was not published as a journal article until 1982. In 1965, Edward W. Forgy published essentially the same method, which is why it is sometimes referred to as the Lloyd–Forgy algorithm.
The most common algorithm uses an iterative refinement technique. Due to its ubiquity, it is often called "the k-means algorithm"; it is also referred to as Lloyd's algorithm, particularly in the computer science community. It is sometimes also referred to as "naïve k-means", because there exist much faster alternatives.
Given an initial set of k means m1(1), ..., mk(1) (see below), the algorithm proceeds by alternating between two steps:
Assignment step: Assign each observation to the cluster with the nearest mean (centroid): that with the least squared Euclidean distance. (Mathematically, this means partitioning the observations according to the Voronoi diagram generated by the means.)
^=\left\:\left\|x_-m_^\right\|^\leq \left\|x_-m_^\right\|^\ \forall j,1\leq j\leq k\right\,
is assigned to exactly one
, even if it could be assigned to two or more of them.
Update step: Recalculate means (centroids) for observations assigned to each cluster. This is also called refitting.
^=^\right|\sum _\in S_^x_
The objective function in k-means is the WCSS (within cluster sum of squares). After each iteration, the WCSS monotonically decreases, giving a nonnegative monotonically decreasing sequence. This guarantees that the k-means always converges, but not necessarily to the global optimum.
The algorithm has converged when the assignments no longer change or equivalently, when the WCSS has become stable. The algorithm is not guaranteed to find the optimal cluster assignment.
The algorithm is often presented as assigning objects to the nearest cluster by distance. Using a different distance function other than (squared) Euclidean distance may prevent the algorithm from converging. Various modifications of k-means such as spherical k-means and k-medoids have been proposed to allow using other distance measures.
The below pseudocode outlines the implementation of the standard k-means clustering algorithm. Initialization of centroids, distance metric between points and centroids, and the calculation of new centroids are design choices and will vary with different implementations. In this example pseudocode, distance() returns the distance between the specified points.
function kmeans(k, points) is
// Initialize centroids
centroids ← list of k starting centroids
converged ← false
while converged == false do
// Create empty clusters
clusters ← list of k empty lists
// Assign each point to the nearest centroid
for i ← 0 to length(points) - 1 do
point ← points[i]
closestIndex ← 0
minDistance ← distance(point, centroids[0])
for j ← 1 to k - 1 do
d ← distance(point, centroids[j])
if d < minDistance THEN
minDistance ← d
closestIndex ← j
// Recalculate centroids as the mean of each cluster
newCentroids ← empty list
for i ← 0 to k - 1 do
newCentroid ← calculateCentroid(clusters[i])
// Check for convergence
if newCentroids == centroids THEN
converged ← true
centroids ← newCentroids
Commonly used initialization methods are Forgy and Random Partition. The Forgy method randomly chooses k observations from the dataset and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds to the update step, thus computing the initial mean to be the centroid of the cluster's randomly assigned points. The Forgy method tends to spread the initial means out, while Random Partition places all of them close to the center of the data set. According to Hamerly et al., the Random Partition method is generally preferable for algorithms such as the k-harmonic means and fuzzy k-means. For expectation maximization and standard k-means algorithms, the Forgy method of initialization is preferable. A comprehensive study by Celebi et al., however, found that popular initialization methods such as Forgy, Random Partition, and Maximin often perform poorly, whereas Bradley and Fayyad's approach performs "consistently" in "the best group" and k-means++ performs "generally well".
The algorithm does not guarantee convergence to the global optimum. The result may depend on the initial clusters. As the algorithm is usually fast, it is common to run it multiple times with different starting conditions. However, worst-case performance can be slow: in particular certain point sets, even in two dimensions, converge in exponential time, that is 2Ω(n). These point sets do not seem to arise in practice: this is corroborated by the fact that the smoothed running time of k-means is polynomial.
The "assignment" step is referred to as the "expectation step", while the "update step" is a maximization step, making this algorithm a variant of the generalized expectation–maximization algorithm.
Finding the optimal solution to the k-means clustering problem for observations in d dimensions is:
NP-hard in general Euclidean space (of d dimensions) even for two clusters,
NP-hard for a general number of clusters k even in the plane,
if k and d (the dimension) are fixed, the problem can be exactly solved in time
, where n is the number of entities to be clustered.
Thus, a variety of heuristic algorithms such as Lloyd's algorithm given above are generally used.
The running time of Lloyd's algorithm (and most variants) is
n is the number of d-dimensional vectors (to be clustered)
k the number of clusters
i the number of iterations needed until convergence.
On data that does have a clustering structure, the number of iterations until convergence is often small, and results only improve slightly after the first dozen iterations. Lloyd's algorithm is therefore often considered to be of "linear" complexity in practice, although it is in the worst case superpolynomial when performed until convergence.
In the worst-case, Lloyd's algorithm needs
)
iterations, so that the worst-case complexity of Lloyd's algorithm is superpolynomial.
Lloyd's k-means algorithm has polynomial smoothed running time. It is shown that for arbitrary set of n points in
, if each point is independently perturbed by a normal distribution with mean 0 and variance

, then the expected running time of k-means algorithm is bounded by
k^d^\log ^(n)/\sigma ^)
, which is a polynomial in n, k, d and

Better bounds are proven for simple cases. For example, it is shown that the running time of k-means algorithm is bounded by
for n points in an integer lattice
^
Lloyd's algorithm is the standard approach for this problem. However, it spends a lot of processing time computing the distances between each of the k cluster centers and the n data points. Since points usually stay in the same clusters after a few iterations, much of this work is unnecessary, making the naïve implementation very inefficient. Some implementations use caching and the triangle inequality in order to create bounds and accelerate Lloyd's algorithm.
Finding the optimal number of clusters (k) for k-means clustering is a crucial step to ensure that the clustering results are meaningful and useful. Several techniques are available to determine a suitable number of clusters. Here are some of commonly used methods:
Elbow method (clustering): This method involves plotting the explained variation as a function of the number of clusters, and picking the elbow of the curve as the number of clusters to use. However, the notion of an "elbow" is not well-defined and this is known to be unreliable.
Silhouette (clustering): Silhouette analysis measures the quality of clustering and provides an insight into the separation distance between the resulting clusters. A higher silhouette score indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.
Gap statistic: The Gap Statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The optimal k is the value that yields the largest gap statistic.
Davies–Bouldin index: The Davies-Bouldin index is a measure of the how much separation there is between clusters. Lower values of the Davies-Bouldin index indicate a model with better separation.
Calinski-Harabasz index: This Index evaluates clusters based on their compactness and separation. The index is calculated using the ratio of between-cluster variance to within-cluster variance, with higher values indicate better-defined clusters.
Rand index: It calculates the proportion of agreement between the two clusters, considering both the pairs of elements that are correctly assigned to the same or different clusters. Higher values indicate greater similarity and better clustering quality. To provide a more accurate measure, the Adjusted Rand Index (ARI), introduced by Hubert and Arabie in 1985, corrects the Rand Index by adjusting for the expected similarity of all pairings due to chance.
Jenks natural breaks optimization: k-means applied to univariate data
k-medians clustering uses the median in each dimension instead of the mean, and this way minimizes
norm (Taxicab geometry).
k-medoids (also: Partitioning Around Medoids, PAM) uses the medoid instead of the mean, and this way minimizes the sum of distances for arbitrary distance functions.
Fuzzy C-Means Clustering is a soft version of k-means, where each data point has a fuzzy degree of belonging to each cluster.
Gaussian mixture models trained with expectation–maximization algorithm (EM algorithm) maintains probabilistic assignments to clusters, instead of deterministic assignments, and multivariate Gaussian distributions instead of means.
k-means++ chooses initial centers in a way that gives a provable upper bound on the WCSS objective.
The filtering algorithm uses k-d trees to speed up each k-means step.
Some methods attempt to speed up each k-means step using the triangle inequality.
Escape local optima by swapping points between clusters.
The Spherical k-means clustering algorithm is suitable for textual data.
Hierarchical variants such as Bisecting k-means, X-means clustering and G-means clustering repeatedly split clusters to build a hierarchy, and can also try to automatically determine the optimal number of clusters in a dataset.
Internal cluster evaluation measures such as cluster silhouette can be helpful at determining the number of clusters.
Minkowski weighted k-means automatically calculates cluster specific feature weights, supporting the intuitive idea that a feature may have different degrees of relevance at different features. These weights can also be used to re-scale a given data set, increasing the likelihood of a cluster validity index to be optimized at the expected number of clusters.
Mini-batch k-means: k-means variation using "mini batch" samples for data sets that do not fit into memory.
Hartigan and Wong's method provides a variation of k-means algorithm which progresses towards a local minimum of the minimum sum-of-squares problem with different solution updates. The method is a local search that iteratively attempts to relocate a sample into a different cluster as long as this process improves the objective function. When no sample can be relocated into a different cluster with an improvement of the objective, the method stops (in a local minimum). In a similar way as the classical k-means, the approach remains a heuristic since it does not necessarily guarantee that the final solution is globally optimum.
)
be the individual cost of
(x-\mu _)^

the center of the cluster.
Hartigan and Wong's method starts by partitioning the points into random clusters
\_
Next it determines the


for which the following function reaches a maximum
)+\varphi (S_)-\varphi (S_\setminus \)-\varphi (S_\cup \).
that reach this maximum,
moves from the cluster
to the cluster
The algorithm terminates once

is less than zero for all
Different move acceptance strategies can be used. In a first-improvement strategy, any improving relocation can be applied, whereas in a best-improvement strategy, all possible relocations are iteratively tested and only the best is applied at each iteration. The former approach favors speed, whether the latter approach generally favors solution quality at the expense of additional computational time. The function

used to calculate the result of a relocation can also be efficiently evaluated by using equality
\mid \mid -1\cdot \lVert \mu _-x\rVert ^-\mid \mid +1\cdot \lVert \mu _-x\rVert ^.
The classical k-means algorithm and its variations are known to only converge to local minima of the minimum-sum-of-squares clustering problem defined as
  _ \sum _^\sum _ \in S_\left\|\mathbf  -_\right\|^.
Many studies have attempted to improve the convergence behavior of the algorithm and maximize the chances of attaining the global optimum (or at least, local minima of better quality). Initialization and restart techniques discussed in the previous sections are one alternative to find better solutions. More recently, global optimization algorithms based on branch-and-bound and semidefinite programming have produced ‘’provenly optimal’’ solutions for datasets with up to 4,177 entities and 20,531 features. As expected, due to the NP-hardness of the subjacent optimization problem, the computational time of optimal algorithms for k-means quickly increases beyond this size. Optimal solutions for small- and medium-scale still remain valuable as a benchmark tool, to evaluate the quality of other heuristics. To find high-quality local minima within a controlled computational time but without optimality guarantees, other works have explored metaheuristics and other global optimization techniques, e.g., based on incremental approaches and convex optimization, random swaps (i.e., iterated local search), variable neighborhood search and genetic algorithms. It is indeed known that finding better local minima of the minimum sum-of-squares clustering problem can make the difference between failure and success to recover cluster structures in feature spaces of high dimension.
Three key features of k-means that make it efficient are often regarded as its biggest drawbacks:
Euclidean distance is used as a metric and variance is used as a measure of cluster scatter.
The number of clusters k is an input parameter: an inappropriate choice of k may yield poor results. That is why, when performing k-means, it is important to run diagnostic checks for determining the number of clusters in the data set.
Convergence to a local minimum may produce counterintuitive ("wrong") results (see example in Fig.).
A key limitation of k-means is its cluster model. The concept is based on spherical clusters that are separable so that the mean converges towards the cluster center. The clusters are expected to be of similar size, so that the assignment to the nearest cluster center is the correct assignment. When for example applying k-means with a value of
onto the well-known Iris flower data set, the result often fails to separate the three Iris species contained in the data set. With
, the two visible clusters (one containing two species) will be discovered, whereas with
one of the two clusters will be split into two even parts. In fact,
is more appropriate for this data set, despite the data set's containing 3 classes. As with any other clustering algorithm, the k-means result makes assumptions that the data satisfy certain criteria. It works well on some data sets, and fails on others.
The result of k-means can be seen as the Voronoi cells of the cluster means. Since data is split halfway between cluster means, this can lead to suboptimal splits as can be seen in the "mouse" example. The Gaussian models used by the expectation–maximization algorithm (arguably a generalization of k-means) are more flexible by having both variances and covariances. The EM result is thus able to accommodate clusters of variable size much better than k-means as well as correlated clusters (not in this example). In counterpart, EM requires the optimization of a larger number of free parameters and poses some methodological issues due to vanishing clusters or badly-conditioned covariance matrices. k-means is closely related to nonparametric Bayesian modeling.
k-means clustering is rather easy to apply to even large data sets, particularly when using heuristics such as Lloyd's algorithm. It has been successfully used in market segmentation, computer vision, and astronomy among many other domains. It often is used as a preprocessing step for other algorithms, for example to find a starting configuration.
Vector quantization, a technique commonly used in signal processing and computer graphics, involves reducing the color palette of an image to a fixed number of colors, known as k. One popular method for achieving vector quantization is through k-means clustering. In this process, k-means is applied to the color space of an image to partition it into k clusters, with each cluster representing a distinct color in the image. This technique is particularly useful in image segmentation tasks, where it helps identify and group similar colors together.
Example: In the field of computer graphics, k-means clustering is often employed for color quantization in image compression. By reducing the number of colors used to represent an image, file sizes can be significantly reduced without significant loss of visual quality. For instance, consider an image with millions of colors. By applying k-means clustering with k set to a smaller number, the image can be represented using a more limited color palette, resulting in a compressed version that consumes less storage space and bandwidth. Other uses of vector quantization include non-random sampling, as k-means can easily be used to choose k different but prototypical objects from a large data set for further analysis.
Cluster analysis, a fundamental task in data mining and machine learning, involves grouping a set of data points into clusters based on their similarity. k-means clustering is a popular algorithm used for partitioning data into k clusters, where each cluster is represented by its centroid.
However, the pure k-means algorithm is not very flexible, and as such is of limited use (except for when vector quantization as above is actually the desired use case). In particular, the parameter k is known to be hard to choose (as discussed above) when not given by external constraints. Another limitation is that it cannot be used with arbitrary distance functions or on non-numerical data. For these use cases, many other algorithms are superior.
Example: In marketing, k-means clustering is frequently employed for market segmentation, where customers with similar characteristics or behaviors are grouped together. For instance, a retail company may use k-means clustering to segment its customer base into distinct groups based on factors such as purchasing behavior, demographics, and geographic location. These customer segments can then be targeted with tailored marketing strategies and product offerings to maximize sales and customer satisfaction.
k-means clustering has been used as a feature learning (or dictionary learning) step, in either (semi-)supervised learning or unsupervised learning. The basic approach is first to train a k-means clustering representation, using the input training data (which need not be labelled). Then, to project any input datum into the new feature space, an "encoding" function, such as the thresholded matrix-product of the datum with the centroid locations, computes the distance from the datum to each centroid, or simply an indicator function for the nearest centroid, or some smooth transformation of the distance. Alternatively, transforming the sample-cluster distance through a Gaussian RBF, obtains the hidden layer of a radial basis function network.
This use of k-means has been successfully combined with simple, linear classifiers for semi-supervised learning in NLP (specifically for named-entity recognition) and in computer vision. On an object recognition task, it was found to exhibit comparable performance with more sophisticated feature learning approaches such as autoencoders and restricted Boltzmann machines. However, it generally requires more data, for equivalent performance, because each data point only contributes to one "feature".
Example: In natural language processing (NLP), k-means clustering has been integrated with simple linear classifiers for semi-supervised learning tasks such as named-entity recognition (NER). By first clustering unlabeled text data using k-means, meaningful features can be extracted to improve the performance of NER models. For instance, k-means clustering can be applied to identify clusters of words or phrases that frequently co-occur in the input text, which can then be used as features for training the NER model. This approach has been shown to achieve comparable performance with more complex feature learning techniques such as autoencoders and restricted Boltzmann machines, albeit with a greater requirement for labeled data.
Recent advancements in the application of k-means clustering include improvements in initialization techniques, such as the use of k-means++ initialization to select initial cluster centroids in a more effective manner. Additionally, researchers have explored the integration of k-means clustering with deep learning methods, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), to enhance the performance of various tasks in computer vision, natural language processing, and other domains.
The slow "standard algorithm" for k-means clustering, and its associated expectation–maximization algorithm, is a special case of a Gaussian mixture model, specifically, the limiting case when fixing all covariances to be diagonal, equal and have infinitesimal small variance. Instead of small variances, a hard cluster assignment can also be used to show another equivalence of k-means clustering to a special case of "hard" Gaussian mixture modelling. This does not mean that it is efficient to use Gaussian mixture modelling to compute k-means, but just that there is a theoretical relationship, and that Gaussian mixture modelling can be interpreted as a generalization of k-means; on the contrary, it has been suggested to use k-means clustering to find starting points for Gaussian mixture modelling on difficult data.
Another generalization of the k-means algorithm is the k-SVD algorithm, which estimates data points as a sparse linear combination of "codebook vectors". k-means corresponds to the special case of using a single codebook vector, with a weight of 1.
The relaxed solution of k-means clustering, specified by the cluster indicators, is given by principal component analysis (PCA). The intuition is that k-means describe spherically shaped (ball-like) clusters. If the data has 2 clusters, the line connecting the two centroids is the best 1-dimensional projection direction, which is also the first PCA direction. Cutting the line at the center of mass separates the clusters (this is the continuous relaxation of the discrete cluster indicator). If the data have three clusters, the 2-dimensional plane spanned by three cluster centroids is the best 2-D projection. This plane is also defined by the first two PCA dimensions. Well-separated clusters are effectively modelled by ball-shaped clusters and thus discovered by k-means. Non-ball-shaped clusters are hard to separate when they are close. For example, two half-moon shaped clusters intertwined in space do not separate well when projected onto PCA subspace. k-means should not be expected to do well on this data. It is straightforward to produce counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions.
Basic mean shift clustering algorithms maintain a set of data points the same size as the input data set. Initially, this set is copied from the input set. All points are then iteratively moved towards the mean of the points surrounding them. By contrast, k-means restricts the set of clusters to k clusters, usually much less than the number of points in the input data set, using the mean of all points in the prior cluster that are closer to that point than any other for the centroid (e.g. within the Voronoi partition of each updating point). A mean shift algorithm that is similar then to k-means, called likelihood mean shift, replaces the set of points undergoing replacement by the mean of all points in the input set that are within a given distance of the changing set. An advantage of mean shift clustering over k-means is the detection of an arbitrary number of clusters in the data set, as there is not a parameter determining the number of clusters. Mean shift can be much slower than k-means, and still requires selection of a bandwidth parameter.
Under sparsity assumptions and when input data is pre-processed with the whitening transformation, k-means produces the solution to the linear independent component analysis (ICA) task. This aids in explaining the successful application of k-means to feature learning.
k-means implicitly assumes that the ordering of the input data set does not matter. The bilateral filter is similar to k-means and mean shift in that it maintains a set of data points that are iteratively replaced by means. However, the bilateral filter restricts the calculation of the (kernel weighted) mean to include only points that are close in the ordering of the input data. This makes it applicable to problems such as image denoising, where the spatial arrangement of pixels in an image is of critical importance.
The set of squared error minimizing cluster functions also includes the k-medoids algorithm, an approach which forces the center point of each cluster to be one of the actual points, i.e., it uses medoids in place of centroids.
Different implementations of the algorithm exhibit performance differences, with the fastest on a test data set finishing in 10 seconds, the slowest taking 25,988 seconds (~7 hours). The differences can be attributed to implementation quality, language and compiler differences, different termination criteria and precision levels, and the use of indexes for acceleration.
The following implementations are available under Free/Open Source Software licenses, with publicly available source code.
Accord.NET contains C# implementations for k-means, k-means++ and k-modes.
ALGLIB contains parallelized C++ and C# implementations for k-means and k-means++.
AOSP contains a Java implementation for k-means.
CrimeStat implements two spatial k-means algorithms, one of which allows the user to define the starting locations.
ELKI contains k-means (with Lloyd and MacQueen iteration, along with different initializations such as k-means++ initialization) and various more advanced clustering algorithms.
Smile contains k-means and various more other algorithms and results visualization (for java, kotlin and scala).
Julia contains a k-means implementation in the JuliaStats Clustering package.
KNIME contains nodes for k-means and k-medoids.
Mahout contains a MapReduce based k-means.
mlpack contains a C++ implementation of k-means.
Octave contains k-means.
OpenCV contains a k-means implementation.
Orange includes a component for k-means clustering with automatic selection of k and cluster silhouette scoring.
PSPP contains k-means, The QUICK CLUSTER command performs k-means clustering on the dataset.
R contains three k-means variations.
SciPy and scikit-learn contain multiple k-means implementations.
Spark MLlib implements a distributed k-means algorithm.
Torch contains an unsup package that provides k-means clustering.
Weka contains k-means and x-means.
The following implementations are available under proprietary license terms, and may not have publicly available source code.
Centroidal Voronoi tessellation In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis that seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two categories:
Agglomerative: Agglomerative clustering, often referred to as a "bottom-up" approach, begins with each data point as an individual cluster. At each step, the algorithm merges the two most similar clusters based on a chosen distance metric (e.g., Euclidean distance) and linkage criterion (e.g., single-linkage, complete-linkage). This process continues until all data points are combined into a single cluster or a stopping criterion is met. Agglomerative methods are more commonly used due to their simplicity and computational efficiency for small to medium-sized datasets.
Divisive: Divisive clustering, known as a "top-down" approach, starts with all data points in a single cluster and recursively splits the cluster into smaller ones. At each step, the algorithm selects a cluster and divides it into two or more subsets, often using a criterion such as maximizing the distance between resulting clusters. Divisive methods are less common but can be useful when the goal is to identify large, distinct clusters first.
In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented in a dendrogram.
Hierarchical clustering has the distinct advantage that any valid measure of distance can be used. In fact, the observations themselves are not required: all that is used is a matrix of distances. On the other hand, except for the special case of single-linkage distance, none of the algorithms (except exhaustive search in
(2^)
) can be guaranteed to find the optimum solution.
The standard algorithm for hierarchical agglomerative clustering (HAC) has a time complexity of
(n^)
)
memory, which makes it too slow for even medium data sets. However, for some special cases, optimal efficient agglomerative methods (of complexity
(n^)
) are known: SLINK for single-linkage and CLINK for complete-linkage clustering. With a heap, the runtime of the general case can be reduced to
(n^\log n)
, an improvement on the aforementioned bound of
(n^)
, at the cost of further increasing the memory requirements. In many cases, the memory overheads of this approach are too large to make it practically usable. Methods exist which use quadtrees that demonstrate
(n^)
total running time with
(n)
Divisive clustering with an exhaustive search is
(2^)
, but it is common to use faster heuristics to choose splits, such as k-means.
In order to decide which clusters should be combined (for agglomerative), or where a cluster should be split (for divisive), a measure of dissimilarity between sets of observations is required. In most methods of hierarchical clustering, this is achieved by use of an appropriate distance d, such as the Euclidean distance, between single observations of the data set, and a linkage criterion, which specifies the dissimilarity of sets as a function of the pairwise distances of observations in the sets. The choice of metric as well as linkage can have a major impact on the result of the clustering, where the lower level metric determines which objects are most similar, whereas the linkage criterion influences the shape of the clusters. For example, complete-linkage tends to produce more spherical clusters than single-linkage.
The linkage criterion determines the distance between sets of observations as a function of the pairwise distances between observations.
Some commonly used linkage criteria between two sets of observations A and B and a distance d are:
Some of these can only be recomputed recursively (WPGMA, WPGMC), for many a recursive computation with Lance-Williams-equations is more efficient, while for other (Hausdorff, Medoid) the distances have to be computed with the slower full formula. Other linkage criteria include:
The probability that candidate clusters spawn from the same distribution function (V-linkage).
The product of in-degree and out-degree on a k-nearest-neighbour graph (graph degree linkage).
The increment of some cluster descriptor (i.e., a quantity defined for measuring the quality of a cluster) after merging two clusters.
For example, suppose this data is to be clustered, and the Euclidean distance is the distance metric.
The hierarchical clustering dendrogram would be:
Cutting the tree at a given height will give a partitioning clustering at a selected precision. In this example, cutting after the second row (from the top) of the dendrogram will yield clusters    . Cutting after the third row will yield clusters   , which is a coarser clustering, with a smaller number but larger clusters.
This method builds the hierarchy from the individual elements by progressively merging clusters. In our example, we have six elements      and . The first step is to determine which elements to merge in a cluster. Usually, we want to take the two closest elements, according to the chosen distance.
Optionally, one can also construct a distance matrix at this stage, where the number in the i-th row j-th column is the distance between the i-th and j-th elements. Then, as clustering progresses, rows and columns are merged as the clusters are merged and the distances updated. This is a common way to implement this type of clustering, and has the benefit of caching distances between clusters. A simple agglomerative clustering algorithm is described in the single-linkage clustering page; it can easily be adapted to different types of linkage (see below).
Suppose we have merged the two closest elements b and c, we now have the following clusters , , ,  and , and want to merge them further. To do that, we need to take the distance between  and , and therefore define the distance between two clusters.
Usually the distance between two clusters


is one of the following:
The maximum distance between elements of each cluster (also called complete-linkage clustering):
,\,y\in \,\.
The minimum distance between elements of each cluster (also called single-linkage clustering):
,\,y\in \,\.
The mean distance between elements of each cluster (also called average linkage clustering, used e.g. in UPGMA):
|\cdot ||\sum _\sum _d(x,y).
The sum of all intra-cluster variance.
The increase in variance for the cluster being merged (Ward's method)
The probability that candidate clusters spawn from the same distribution function (V-linkage).
In case of tied minimum distances, a pair is randomly chosen, thus being able to generate several structurally different dendrograms. Alternatively, all tied pairs may be joined at the same time, generating a unique dendrogram.
One can always decide to stop clustering when there is a sufficiently small number of clusters (number criterion). Some linkages may also guarantee that agglomeration occurs at a greater distance between clusters than the previous agglomeration, and then one can stop clustering when the clusters are too far apart to be merged (distance criterion). However, this is not the case of, e.g., the centroid linkage where the so-called reversals (inversions, departures from ultrametricity) may occur.
The basic principle of divisive clustering was published as the DIANA (DIvisive ANAlysis clustering) algorithm. Initially, all data is in the same cluster, and the largest cluster is split until every object is separate.
Because there exist
ways of splitting each cluster, heuristics are needed. DIANA chooses the object with the maximum average dissimilarity and then moves all objects to this cluster that are more similar to the new cluster than to the remainder.
Informally, DIANA is not so much a process of "dividing" as it is of "hollowing out": each iteration, an existing cluster (e.g. the initial cluster of the entire dataset) is chosen to form a new cluster inside of it. Objects progressively move to this nested cluster, and hollow out the existing cluster. Eventually, all that's left inside a cluster is nested clusters that grew there, without it owning any loose objects by itself.
Formally, DIANA operates in the following steps:
=\
be the set of all
object indices and
=\\
the set of all formed clusters so far.
Iterate the following until
|=n
Find the current cluster with 2 or more objects that has the largest diameter:
=\arg \max _\max _,i_\in C\delta (i_,i_)
Find the object in this cluster with the most dissimilarity to the rest of the cluster:
=\arg \max _|-1\sum _\setminus \\delta (i,j)
from its old cluster
and put it into a new splinter group
=\\
As long as
isn't empty, keep migrating objects from
to add them to

. To choose which objects to migrate, don't just consider dissimilarity to
, but also adjust for dissimilarity to the splinter group: let
=\arg \max _D(i)
where we define
|-1\sum _\setminus \\delta (i,j)-|\sum _\delta (i,j)
, then either stop iterating when
, or migrate


above measures how strongly an object wants to leave its current cluster, but it is attenuated when the object wouldn't fit in the splinter group either. Such objects will likely start their own splinter group eventually.
The dendrogram of DIANA can be constructed by letting the splinter group

be a child of the hollowed-out cluster
each time. This constructs a tree with
as its root and
unique single-object clusters as its leaves.
ALGLIB implements several hierarchical clustering algorithms (single-link, complete-link, Ward) in C++ and C# with O(n²) memory and O(n³) run time.
ELKI includes multiple hierarchical clustering algorithms, various linkage strategies and also includes the efficient SLINK, CLINK and Anderberg algorithms, flexible cluster extraction from dendrograms and various other cluster analysis algorithms.
Julia has an implementation inside the Clustering.jl package.
Octave, the GNU analog to MATLAB implements hierarchical clustering in function "linkage".
Orange, a data mining software suite, includes hierarchical clustering with interactive dendrogram visualisation.
R has built-in functions and packages that provide functions for hierarchical clustering.
SciPy implements hierarchical clustering in Python, including the efficient SLINK algorithm.
scikit-learn also implements hierarchical clustering in Python.
Weka includes hierarchical cluster analysis.
MATLAB includes hierarchical cluster analysis.
SAS includes hierarchical cluster analysis in PROC CLUSTER.
Mathematica includes a Hierarchical Clustering Package.
NCSS includes hierarchical cluster analysis.
SPSS includes hierarchical cluster analysis.
Qlucore Omics Explorer includes hierarchical cluster analysis.
Stata includes hierarchical cluster analysis.
CrimeStat includes a nearest neighbor hierarchical cluster algorithm with a graphical output for a Geographic Information System.
Kaufman, L.; Rousseeuw, P.J. (1990). Finding Groups in Data: An Introduction to Cluster Analysis (1 ed.). New York: John Wiley. ISBN 0-471-87876-6.
Hastie, Trevor; Tibshirani, Robert; Friedman, Jerome (2009). "14.3.12 Hierarchical clustering". The Elements of Statistical Learning (2nd ed.). New York: Springer. pp. 520–8. ISBN 978-0-387-84857-0. Archived from the original (PDF) on 2009-11-10. Retrieved 2009-10-20. Hierarchical clustering is one method for finding community structures in a network. The technique arranges the network into a hierarchy of groups according to a specified weight function. The data can then be represented in a tree structure known as a dendrogram. Hierarchical clustering can either be agglomerative or divisive depending on whether one proceeds through the algorithm by adding links to or removing links from the network, respectively. One divisive technique is the Girvan–Newman algorithm.
In the hierarchical clustering algorithm, a weight
is first assigned to each pair of vertices
in the network. The weight, which can vary depending on implementation (see section below), is intended to indicate how closely related the vertices are. Then, starting with all the nodes in the network disconnected, begin pairing nodes from highest to lowest weight between the pairs (in the divisive case, start from the original network and remove links from lowest to highest weight). As links are added, connected subsets begin to form. These represent the network's community structures.
The components at each iterative step are always a subset of other structures. Hence, the subsets can be represented using a tree diagram, or dendrogram. Horizontal slices of the tree at a given level indicate the communities that exist above and below a value of the weight.
There are many possible weights for use in hierarchical clustering algorithms. The specific weight used is dictated by the data as well as considerations for computational speed. Additionally, the communities found in the network are highly dependent on the choice of weighting function. Hence, when compared to real-world data with a known community structure, the various weighting techniques have been met with varying degrees of success.
Two weights that have been used previously with varying success are the number of node-independent paths between each pair of vertices and the total number of paths between vertices weighted by the length of the path. One disadvantage of these weights, however, is that both weighting schemes tend to separate single peripheral vertices from their rightful communities because of the small number of paths going to these vertices. For this reason, their use in hierarchical clustering techniques is far from optimal.
Edge betweenness centrality has been used successfully as a weight in the Girvan–Newman algorithm. This technique is similar to a divisive hierarchical clustering algorithm, except the weights are recalculated with each step.
The change in modularity of the network with the addition of a node has also been used successfully as a weight. This method provides a computationally less-costly alternative to the Girvan-Newman algorithm while yielding similar results. Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable. Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics.
Methods are commonly divided into linear and nonlinear approaches. Linear approaches can be further divided into feature selection and feature extraction. Dimensionality reduction can be used for noise reduction, data visualization, cluster analysis, or as an intermediate step to facilitate other analyses.
The process of feature selection aims to find a suitable subset of the input variables (features, or attributes) for the task at hand. The three strategies are: the filter strategy (e.g., information gain), the wrapper strategy (e.g., accuracy-guided search), and the embedded strategy (features are added or removed while building the model based on prediction errors).
Data analysis such as regression or classification can be done in the reduced space more accurately than in the original space.
Feature projection (also called feature extraction) transforms the data from the high-dimensional space to a space of fewer dimensions. The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist. For multidimensional data, tensor representation can be used in dimensionality reduction through multilinear subspace learning.
The main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. In practice, the covariance (and sometimes the correlation) matrix of the data is constructed and the eigenvectors on this matrix are computed. The eigenvectors that correspond to the largest eigenvalues (the principal components) can now be used to reconstruct a large fraction of the variance of the original data. Moreover, the first few eigenvectors can often be interpreted in terms of the large-scale physical behavior of the system, because they often contribute the vast majority of the system's energy, especially in low-dimensional systems. Still, this must be proved on a case-by-case basis as not all systems exhibit this behavior. The original space (with dimension of the number of points) has been reduced (with data loss, but hopefully retaining the most important variance) to the space spanned by a few eigenvectors.
NMF decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist, such as astronomy. NMF is well known since the multiplicative update rule by Lee & Seung, which has been continuously developed: the inclusion of uncertainties, the consideration of missing data and parallel computation, sequential construction which leads to the stability and linearity of NMF, as well as other updates including handling missing data in digital image processing.
With a stable component basis during construction, and a linear modeling process, sequential NMF is able to preserve the flux in direct imaging of circumstellar structures in astronomy, as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar discs. In comparison with PCA, NMF does not remove the mean of the matrices, which leads to physical non-negative fluxes; therefore NMF is able to preserve more information than PCA as demonstrated by Ren et al.
Principal component analysis can be employed in a nonlinear way by means of the kernel trick. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in the data. The resulting technique is called kernel PCA.
Other prominent nonlinear techniques include manifold learning techniques such as Isomap, locally linear embedding (LLE), Hessian LLE, Laplacian eigenmaps, and methods based on tangent space analysis. These techniques construct a low-dimensional data representation using a cost function that retains local properties of the data, and can be viewed as defining a graph-based kernel for Kernel PCA.
More recently, techniques have been proposed that, instead of defining a fixed kernel, try to learn the kernel using semidefinite programming. The most prominent example of such a technique is maximum variance unfolding (MVU). The central idea of MVU is to exactly preserve all pairwise distances between nearest neighbors (in the inner product space) while maximizing the distances between points that are not nearest neighbors.
An alternative approach to neighborhood preservation is through the minimization of a cost function that measures differences between distances in the input and output spaces. Important examples of such techniques include: classical multidimensional scaling, which is identical to PCA; Isomap, which uses geodesic distances in the data space; diffusion maps, which use diffusion distances in the data space; t-distributed stochastic neighbor embedding (t-SNE), which minimizes the divergence between distributions over pairs of points; and curvilinear component analysis.
A different approach to nonlinear dimensionality reduction is through the use of autoencoders, a special kind of feedforward neural networks with a bottleneck hidden layer. The training of deep encoders is typically performed using a greedy layer-wise pre-training (e.g., using a stack of restricted Boltzmann machines) that is followed by a finetuning stage based on backpropagation.
Linear discriminant analysis (LDA) is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition, and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events.
GDA deals with nonlinear discriminant analysis using kernel function operator. The underlying theory is close to the support-vector machines (SVM) insofar as the GDA method provides a mapping of the input vectors into high-dimensional feature space. Similar to LDA, the objective of GDA is to find a projection for the features into a lower dimensional space by maximizing the ratio of between-class scatter to within-class scatter.
Autoencoders can be used to learn nonlinear dimension reduction functions and codings together with an inverse function from the coding to the original representation.
T-distributed Stochastic Neighbor Embedding (t-SNE) is a nonlinear dimensionality reduction technique useful for the visualization of high-dimensional datasets. It is not recommended for use in analysis such as clustering or outlier detection since it does not necessarily preserve densities or distances well.
Uniform manifold approximation and projection (UMAP) is a nonlinear dimensionality reduction technique. Visually, it is similar to t-SNE, but it assumes that the data is uniformly distributed on a locally connected Riemannian manifold and that the Riemannian metric is locally constant or approximately locally constant.
For high-dimensional datasets, dimension reduction is usually performed prior to applying a k-nearest neighbors (k-NN) algorithm in order to mitigate the curse of dimensionality.
Feature extraction and dimension reduction can be combined in one step, using principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA), or non-negative matrix factorization (NMF) techniques to pre-process the data, followed by clustering via k-NN on feature vectors in a reduced-dimension space. In machine learning, this process is also called low-dimensional embedding.
For high-dimensional datasets (e.g., when performing similarity search on live video streams, DNA data, or high-dimensional time series), running a fast approximate k-NN search using locality-sensitive hashing, random projection, "sketches", or other high-dimensional similarity search techniques from the VLDB conference toolbox may be the only feasible option.
A dimensionality reduction technique that is sometimes used in neuroscience is maximally informative dimensions, which finds a lower-dimensional representation of a dataset such that as much information as possible about the original data is preserved.
JMLR Special Issue on Variable and Feature Selection
ELastic MAPs Archived 2011-07-20 at the Wayback Machine
Visual Comparison of various dimensionality reduction methods
A Global Geometric Framework for Nonlinear Dimensionality Reduction Nonlinear dimensionality reduction, also known as manifold learning, is any of various related techniques that aim to project high-dimensional data, potentially existing across non-linear manifolds which cannot be adequately captured by linear decomposition methods, onto lower-dimensional latent manifolds, with the goal of either visualizing the data in the low-dimensional space, or learning the mapping (either from the high-dimensional space to the low-dimensional embedding or vice versa) itself. The techniques described below can be understood as generalizations of linear decomposition methods used for dimensionality reduction, such as singular value decomposition and principal component analysis.
High dimensional data can be hard for machines to work with, requiring significant time and space for analysis. It also presents a challenge for humans, since it's hard to visualize or understand data in more than three dimensions. Reducing the dimensionality of a data set, while keep its essential features relatively intact, can make algorithms more efficient and allow analysts to visualize trends and patterns.
The reduced-dimensional representations of data are often referred to as "intrinsic variables". This description implies that these are the values from which the data was produced. For example, consider a dataset that contains images of a letter 'A', which has been scaled and rotated by varying amounts. Each image has 32×32 pixels. Each image can be represented as a vector of 1024 pixel values. Each row is a sample on a two-dimensional manifold in 1024-dimensional space (a Hamming space). The intrinsic dimensionality is two, because two variables (rotation and scale) were varied in order to produce the data. Information about the shape or look of a letter 'A' is not part of the intrinsic variables because it is the same in every instance. Nonlinear dimensionality reduction will discard the correlated information (the letter 'A') and recover only the varying information (rotation and scale). The image to the right shows sample images from this dataset (to save space, not all input images are shown), and a plot of the two-dimensional points that results from using a NLDR algorithm (in this case, Manifold Sculpting was used) to reduce the data into just two dimensions.
By comparison, if principal component analysis, which is a linear dimensionality reduction algorithm, is used to reduce this same dataset into two dimensions, the resulting values are not so well organized. This demonstrates that the high-dimensional vectors (each representing a letter 'A') that sample this manifold vary in a non-linear manner.
It should be apparent, therefore, that NLDR has several applications in the field of computer-vision. For example, consider a robot that uses a camera to navigate in a closed static environment. The images obtained by that camera can be considered to be samples on a manifold in high-dimensional space, and the intrinsic variables of that manifold will represent the robot's position and orientation.
Invariant manifolds are of general interest for model order reduction in dynamical systems. In particular, if there is an attracting invariant manifold in the phase space, nearby trajectories will converge onto it and stay on it indefinitely, rendering it a candidate for dimensionality reduction of the dynamical system. While such manifolds are not guaranteed to exist in general, the theory of spectral submanifolds (SSM) gives conditions for the existence of unique attracting invariant objects in a broad class of dynamical systems. Active research in NLDR seeks to unfold the observation manifolds associated with dynamical systems to develop modeling techniques.
Some of the more prominent nonlinear dimensionality reduction techniques are listed below.
Sammon's mapping is one of the first and most popular NLDR techniques.
The self-organizing map (SOM, also called Kohonen map) and its probabilistic variant generative topographic mapping (GTM) use a point representation in the embedded space to form a latent variable model based on a non-linear mapping from the embedded space to the high-dimensional space. These techniques are related to work on density networks, which also are based around the same probabilistic model.
Perhaps the most widely used algorithm for dimensional reduction is kernel PCA. PCA begins by computing the covariance matrix of the

 
\sum _^ _\mathbf  _^.
It then projects the data onto the first k eigenvectors of that matrix. By comparison, KPCA begins by computing the covariance matrix of the data after being transformed into a higher-dimensional space,
\sum _^ _)\Phi (\mathbf  _)^.
It then projects the transformed data onto the first k eigenvectors of that matrix, just like PCA. It uses the kernel trick to factor away much of the computation, such that the entire process can be performed without actually computing
 )
. Of course

must be chosen such that it has a known corresponding kernel. Unfortunately, it is not trivial to find a good kernel for a given problem, so KPCA does not yield good results with some problems when using standard kernels. For example, it is known to perform poorly with these kernels on the Swiss roll manifold. However, one can view certain other methods that perform well in such settings (e.g., Laplacian Eigenmaps, LLE) as special cases of kernel PCA by constructing a data-dependent kernel matrix.
KPCA has an internal model, so it can be used to map points onto its embedding that were not available at training time.
Principal curves and manifolds give the natural geometric framework for nonlinear dimensionality reduction and extend the geometric interpretation of PCA by explicitly constructing an embedded manifold, and by encoding using standard geometric projection onto the manifold. This approach was originally proposed by Trevor Hastie in his 1984 thesis, which he formally introduced in 1989. This idea has been explored further by many authors.
How to define the "simplicity" of the manifold is problem-dependent, however, it is commonly measured by the intrinsic dimensionality and/or the smoothness of the manifold. Usually, the principal manifold is defined as a solution to an optimization problem. The objective function includes a quality of data approximation and some penalty terms for the bending of the manifold. The popular initial approximations are generated by linear PCA and Kohonen's SOM.
Laplacian eigenmaps uses spectral techniques to perform dimensionality reduction. This technique relies on the basic assumption that the data lies in a low-dimensional manifold in a high-dimensional space. This algorithm cannot embed out-of-sample points, but techniques based on Reproducing kernel Hilbert space regularization exist for adding this capability. Such techniques can be applied to other nonlinear dimensionality reduction algorithms as well.
Traditional techniques like principal component analysis do not consider the intrinsic geometry of the data. Laplacian eigenmaps builds a graph from neighborhood information of the data set. Each data point serves as a node on the graph and connectivity between nodes is governed by the proximity of neighboring points (using e.g. the k-nearest neighbor algorithm). The graph thus generated can be considered as a discrete approximation of the low-dimensional manifold in the high-dimensional space. Minimization of a cost function based on the graph ensures that points close to each other on the manifold are mapped close to each other in the low-dimensional space, preserving local distances. The eigenfunctions of the Laplace–Beltrami operator on the manifold serve as the embedding dimensions, since under mild conditions this operator has a countable spectrum that is a basis for square integrable functions on the manifold (compare to Fourier series on the unit circle manifold). Attempts to place Laplacian eigenmaps on solid theoretical ground have met with some success, as under certain nonrestrictive assumptions, the graph Laplacian matrix has been shown to converge to the Laplace–Beltrami operator as the number of points goes to infinity.
Isomap is a combination of the Floyd–Warshall algorithm with classic Multidimensional Scaling (MDS). Classic MDS takes a matrix of pair-wise distances between all points and computes a position for each point. Isomap assumes that the pair-wise distances are only known between neighboring points, and uses the Floyd–Warshall algorithm to compute the pair-wise distances between all other points. This effectively estimates the full matrix of pair-wise geodesic distances between all of the points. Isomap then uses classic MDS to compute the reduced-dimensional positions of all the points. Landmark-Isomap is a variant of this algorithm that uses landmarks to increase speed, at the cost of some accuracy.
In manifold learning, the input data is assumed to be sampled from a low dimensional manifold that is embedded inside of a higher-dimensional vector space. The main intuition behind MVU is to exploit the local linearity of manifolds and create a mapping that preserves local neighbourhoods at every point of the underlying manifold.
Locally-linear Embedding (LLE) was presented at approximately the same time as Isomap. It has several advantages over Isomap, including faster optimization when implemented to take advantage of sparse matrix algorithms, and better results with many problems. LLE also begins by finding a set of the nearest neighbors of each point. It then computes a set of weights for each point that best describes the point as a linear combination of its neighbors. Finally, it uses an eigenvector-based optimization technique to find the low-dimensional embedding of points, such that each point is still described with the same linear combination of its neighbors. LLE tends to handle non-uniform sample densities poorly because there is no fixed unit to prevent the weights from drifting as various regions differ in sample densities. LLE has no internal model.
The original data points
\in \mathbb  ^
, and the goal of LLE is to embed each point
to some low-dimensional point
\in \mathbb  ^

LLE has two steps. In the first step, it computes, for each point Xi, the best approximation of Xi based on barycentric coordinates of its neighbors Xj. The original point is approximately reconstructed by a linear combination, given by the weight matrix Wij, of its neighbors. The reconstruction error is:
\left|\mathbf  _-\sum _ _\mathbf  _\right|^
The weights Wij refer to the amount of contribution the point Xj has while reconstructing the point Xi. The cost function is minimized under two constraints:
Each data point Xi is reconstructed only from its neighbors. That is,
if point Xj is not a neighbor of the point Xi.
The sum of every row of the weight matrix equals 1, that is,
 _=1
These two constraints ensure that
is unaffected by rotation and translation.
In the second step, a neighborhood preserving map is created based the weights. Each point
\in \mathbb  ^
is mapped onto a point
\in \mathbb  ^
by minimizing another cost:
\left|\mathbf  _-\sum _ _\mathbf  _\right|^
Unlike in the previous cost function, the weights Wij are kept fixed and the minimization is done on the points Yi to optimize the coordinates. This minimization problem can be solved by solving a sparse N X N eigenvalue problem (N being the number of data points), whose bottom d nonzero eigen vectors provide an orthogonal set of coordinates.
The only hyperparameter in the algorithm is what counts as a "neighbor" of a point. Generally the data points are reconstructed from K nearest neighbors, as measured by Euclidean distance. In this case, the algorithm has only one integer-valued hyperparameter K, which can be chosen by cross validation.
Like LLE, Hessian LLE is also based on sparse matrix techniques. It tends to yield results of a much higher quality than LLE. Unfortunately, it has a very costly computational complexity, so it is not well-suited for heavily sampled manifolds. It has no internal model.
Modified LLE (MLLE) is another LLE variant which uses multiple weights in each neighborhood to address the local weight matrix conditioning problem which leads to distortions in LLE maps. Loosely speaking the multiple weights are the local orthogonal projection of the original weights produced by LLE. The creators of this regularised variant are also the authors of Local Tangent Space Alignment (LTSA), which is implicit in the MLLE formulation when realising that the global optimisation of the orthogonal projections of each weight vector, in-essence, aligns the local tangent spaces of every data point. The theoretical and empirical implications from the correct application of this algorithm are far-reaching.
LTSA is based on the intuition that when a manifold is correctly unfolded, all of the tangent hyperplanes to the manifold will become aligned. It begins by computing the k-nearest neighbors of every point. It computes the tangent space at every point by computing the d-first principal components in each local neighborhood. It then optimizes to find an embedding that aligns the tangent spaces.
Maximum Variance Unfolding, Isomap and Locally Linear Embedding share a common intuition relying on the notion that if a manifold is properly unfolded, then variance over the points is maximized. Its initial step, like Isomap and Locally Linear Embedding, is finding the k-nearest neighbors of every point. It then seeks to solve the problem of maximizing the distance between all non-neighboring points, constrained such that the distances between neighboring points are preserved. The primary contribution of this algorithm is a technique for casting this problem as a semidefinite programming problem. Unfortunately, semidefinite programming solvers have a high computational cost. Like Locally Linear Embedding, it has no internal model.
An autoencoder is a feed-forward neural network which is trained to approximate the identity function. That is, it is trained to map from a vector of values to the same vector. When used for dimensionality reduction purposes, one of the hidden layers in the network is limited to contain only a small number of network units. Thus, the network must learn to encode the vector into a small number of dimensions and then decode it back into the original space. Thus, the first half of the network is a model which maps from high to low-dimensional space, and the second half maps from low to high-dimensional space. Although the idea of autoencoders is quite old, training of deep autoencoders has only recently become possible through the use of restricted Boltzmann machines and stacked denoising autoencoders. Related to autoencoders is the NeuroScale algorithm, which uses stress functions inspired by multidimensional scaling and Sammon mappings (see above) to learn a non-linear mapping from the high-dimensional to the embedded space. The mappings in NeuroScale are based on radial basis function networks.
Gaussian process latent variable models (GPLVM) are probabilistic dimensionality reduction methods that use Gaussian Processes (GPs) to find a lower dimensional non-linear embedding of high dimensional data. They are an extension of the Probabilistic formulation of PCA. The model is defined probabilistically and the latent variables are then marginalized and parameters are obtained by maximizing the likelihood. Like kernel PCA they use a kernel function to form a non linear mapping (in the form of a Gaussian process). However, in the GPLVM the mapping is from the embedded(latent) space to the data space (like density networks and GTM) whereas in kernel PCA it is in the opposite direction. It was originally proposed for visualization of high dimensional data but has been extended to construct a shared manifold model between two observation spaces.
GPLVM and its many variants have been proposed specially for human motion modeling, e.g., back constrained GPLVM, GP dynamic model (GPDM), balanced GPDM (B-GPDM) and topologically constrained GPDM. To capture the coupling effect of the pose and gait manifolds in the gait analysis, a multi-layer joint gait-pose manifolds was proposed.
t-distributed stochastic neighbor embedding (t-SNE) is widely used. It is one of a family of stochastic neighbor embedding methods. The algorithm computes the probability that pairs of datapoints in the high-dimensional space are related, and then chooses low-dimensional embeddings which produce a similar distribution.
Relational perspective map is a multidimensional scaling algorithm. The algorithm finds a configuration of data points on a manifold by simulating a multi-particle dynamic system on a closed manifold, where data points are mapped to particles and distances (or dissimilarity) between data points represent a repulsive force. As the manifold gradually grows in size the multi-particle system cools down gradually and converges to a configuration that reflects the distance information of the data points.
Relational perspective map was inspired by a physical model in which positively charged particles move freely on the surface of a ball. Guided by the Coulomb force between particles, the minimal energy configuration of the particles will reflect the strength of repulsive forces between the particles.
The Relational perspective map was introduced in.
The algorithm firstly used the flat torus as the image manifold, then it has been extended (in the software VisuMap to use other types of closed manifolds, like the sphere, projective space, and Klein bottle, as image manifolds.
Contagion maps use multiple contagions on a network to map the nodes as a point cloud. In the case of the Global cascades model the speed of the spread can be adjusted with the threshold parameter

the contagion map is equivalent to the Isomap algorithm.
Curvilinear component analysis (CCA) looks for the configuration of points in the output space that preserves original distances as much as possible while focusing on small distances in the output space (conversely to Sammon's mapping which focus on small distances in original space).
It should be noticed that CCA, as an iterative learning algorithm, actually starts with focus on large distances (like the Sammon algorithm), then gradually change focus to small distances. The small distance information will overwrite the large distance information, if compromises between the two have to be made.
The stress function of CCA is related to a sum of right Bregman divergences.
CDA trains a self-organizing neural network to fit the manifold and seeks to preserve geodesic distances in its embedding. It is based on Curvilinear Component Analysis (which extended Sammon's mapping), but uses geodesic distances instead.
Diffeomorphic Dimensionality Reduction or Diffeomap learns a smooth diffeomorphic mapping which transports the data onto a lower-dimensional linear subspace. The methods solves for a smooth time indexed vector field such that flows along the field which start at the data points will end at a lower-dimensional linear subspace, thereby attempting to preserve pairwise differences under both the forward and inverse mapping.
Manifold alignment takes advantage of the assumption that disparate data sets produced by similar generating processes will share a similar underlying manifold representation. By learning projections from each original space to the shared manifold, correspondences are recovered and knowledge from one domain can be transferred to another. Most manifold alignment techniques consider only two data sets, but the concept extends to arbitrarily many initial data sets.
Diffusion maps leverages the relationship between heat diffusion and a random walk (Markov Chain); an analogy is drawn between the diffusion operator on a manifold and a Markov transition matrix operating on functions defined on the graph whose nodes were sampled from the manifold. In particular, let a data set be represented by
 =[x_,x_,\ldots ,x_]\in \Omega \subset \mathbf  
. The underlying assumption of diffusion map is that the high-dimensional data lies on a low-dimensional manifold of dimension
 
. Let X represent the data set and

represent the distribution of the data points on X. Further, define a kernel which represents some notion of affinity of the points in X. The kernel

has the following properties
k is symmetric

k is positivity preserving
Thus one can think of the individual data points as the nodes of a graph and the kernel k as defining some sort of affinity on that graph. The graph is symmetric by construction since the kernel is symmetric. It is easy to see here that from the tuple (X,k) one can construct a reversible Markov Chain. This technique is common to a variety of fields and is known as the graph Laplacian.
For example, the graph K = (X,E) can be constructed using a Gaussian kernel.
=e^-x_\|_^/\sigma ^&x_\sim x_\\0&\end
In the above equation,
\sim x_
is a nearest neighbor of
. Properly, Geodesic distance should be used to actually measure distances on the manifold. Since the exact structure of the manifold is not available, for the nearest neighbors the geodesic distance is approximated by euclidean distance. The choice

modulates our notion of proximity in the sense that if
-x_\|_\gg \sigma 
-x_\|_\ll \sigma 
. The former means that very little diffusion has taken place while the latter implies that the diffusion process is nearly complete. Different strategies to choose

can be found in.
In order to faithfully represent a Markov matrix,
must be normalized by the corresponding degree matrix
now represents a Markov chain.
is the probability of transitioning from
in one time step. Similarly the probability of transitioning from
in t time steps is given by
is the matrix
multiplied by itself t times.
The Markov matrix
constitutes some notion of local geometry of the data set X. The major difference between diffusion maps and principal component analysis is that only local features of the data are considered in diffusion maps as opposed to taking correlations of the entire data set.
defines a random walk on the data set which means that the kernel captures some local geometry of data set. The Markov chain defines fast and slow directions of propagation through the kernel values. As the walk propagates forward in time, the local geometry information aggregates in the same way as local transitions (defined by differential equations) of the dynamical system. The metaphor of diffusion arises from the definition of a family diffusion distance
\_
^(x,y)=\|p_(x,\cdot )-p_(y,\cdot )\|^
For fixed t,
defines a distance between any two points of the data set based on path connectivity: the value of
will be smaller the more paths that connect x to y and vice versa. Because the quantity
involves a sum over of all paths of length t,
is much more robust to noise in the data than geodesic distance.
takes into account all the relation between points x and y while calculating the distance and serves as a better notion of proximity than just Euclidean distance or even geodesic distance.
Local Multidimensional Scaling performs multidimensional scaling in local regions, and then uses convex optimization to fit all the pieces together.
Nonlinear PCA (NLPCA) uses backpropagation to train a multi-layer perceptron (MLP) to fit to a manifold. Unlike typical MLP training, which only updates the weights, NLPCA updates both the weights and the inputs. That is, both the weights and inputs are treated as latent values. After training, the latent inputs are a low-dimensional representation of the observed vectors, and the MLP maps from that low-dimensional representation to the high-dimensional observation space.
Data-driven high-dimensional scaling (DD-HDS) is closely related to Sammon's mapping and curvilinear component analysis except that (1) it simultaneously penalizes false neighborhoods and tears by focusing on small distances in both original and output space, and that (2) it accounts for concentration of measure phenomenon by adapting the weighting function to the distance distribution.
Manifold Sculpting uses graduated optimization to find an embedding. Like other algorithms, it computes the k-nearest neighbors and tries to seek an embedding that preserves relationships in local neighborhoods. It slowly scales variance out of higher dimensions, while simultaneously adjusting points in lower dimensions to preserve those relationships. If the rate of scaling is small, it can find very precise embeddings. It boasts higher empirical accuracy than other algorithms with several problems. It can also be used to refine the results from other manifold learning algorithms. It struggles to unfold some manifolds, however, unless a very slow scaling rate is used. It has no model.
RankVisu is designed to preserve rank of neighborhood rather than distance. RankVisu is especially useful on difficult tasks (when the preservation of distance cannot be achieved satisfyingly). Indeed, the rank of neighborhood is less informative than distance (ranks can be deduced from distances but distances cannot be deduced from ranks) and its preservation is thus easier.
Topologically constrained isometric embedding (TCIE) is an algorithm based on approximating geodesic distances after filtering geodesics inconsistent with the Euclidean metric. Aimed at correcting the distortions caused when Isomap is used to map intrinsically non-convex data, TCIE uses weight least-squares MDS in order to obtain a more accurate mapping. The TCIE algorithm first detects possible boundary points in the data, and during computation of the geodesic length marks inconsistent geodesics, to be given a small weight in the weighted stress majorization that follows.
Uniform manifold approximation and projection (UMAP) is a nonlinear dimensionality reduction technique. It is similar to t-SNE.
A method based on proximity matrices is one where the data is presented to the algorithm in the form of a similarity matrix or a distance matrix. These methods all fall under the broader class of metric multidimensional scaling. The variations tend to be differences in how the proximity data is computed; for example, isomap, locally linear embeddings, maximum variance unfolding, and Sammon mapping (which is not in fact a mapping) are examples of metric multidimensional scaling methods.
Waffles is an open source C++ library containing implementations of LLE, Manifold Sculpting, and some other manifold learning algorithms.
UMAP.jl implements the method for the programming language Julia.
The method has also been implemented in Python (code available on GitHub)
Whitney embedding theorem
Growing self-organizing map (GSOM)
Self-organizing map (SOM)
Murphy, Kevin P. (2022). "Manifold Learning". Probabilistic Machine Learning. MIT Press. pp. 682–699. ISBN 978-0-262-04682-4.
Mike Tipping's Thesis
Short review of Diffusion Maps
Nonlinear PCA by autoencoder neural networks Multifactor dimensionality reduction (MDR) is a statistical approach, also used in machine learning automatic approaches, for detecting and characterizing combinations of attributes or independent variables that interact to influence a dependent or class variable. MDR was designed specifically to identify nonadditive interactions among discrete variables that influence a binary outcome and is considered a nonparametric and model-free alternative to traditional statistical methods such as logistic regression.
The basis of the MDR method is a constructive induction or feature engineering algorithm that converts two or more variables or attributes to a single attribute. This process of constructing a new attribute changes the representation space of the data. The end goal is to create or discover a representation that facilitates the detection of nonlinear or nonadditive interactions among the attributes such that prediction of the class variable is improved over that of the original representation of the data.
Consider the following simple example using the exclusive OR (XOR) function. XOR is a logical operator that is commonly used in data mining and machine learning as an example of a function that is not linearly separable. The table below represents a simple dataset where the relationship between the attributes (X1 and X2) and the class variable (Y) is defined by the XOR function such that Y = X1 XOR X2.
A machine learning algorithm would need to discover or approximate the XOR function in order to accurately predict Y using information about X1 and X2. An alternative strategy would be to first change the representation of the data using constructive induction to facilitate predictive modeling. The MDR algorithm would change the representation of the data (X1 and X2) in the following manner. MDR starts by selecting two attributes. In this simple example, X1 and X2 are selected. Each combination of values for X1 and X2 are examined and the number of times Y=1 and/or Y=0 is counted. In this simple example, Y=1 occurs zero times and Y=0 occurs once for the combination of X1=0 and X2=0. With MDR, the ratio of these counts is computed and compared to a fixed threshold. Here, the ratio of counts is 0/1 which is less than our fixed threshold of 1. Since 0/1 < 1 we encode a new attribute (Z) as a 0. When the ratio is greater than one we encode Z as a 1. This process is repeated for all unique combinations of values for X1 and X2. Table 2 illustrates our new transformation of the data.
The machine learning algorithm now has much less work to do to find a good predictive function. In fact, in this very simple example, the function Y = Z has a classification accuracy of 1. A nice feature of constructive induction methods such as MDR is the ability to use any data mining or machine learning method to analyze the new representation of the data. Decision trees, neural networks, or a naive Bayes classifier could be used in combination with measures of model quality such as balanced accuracy and mutual information.
As illustrated above, the basic constructive induction algorithm in MDR is very simple. However, its implementation for mining patterns from real data can be computationally complex. As with any machine learning algorithm there is always concern about overfitting. That is, machine learning algorithms are good at finding patterns in completely random data. It is often difficult to determine whether a reported pattern is an important signal or just chance. One approach is to estimate the generalizability of a model to independent datasets using methods such as cross-validation. Models that describe random data typically don't generalize. Another approach is to generate many random permutations of the data to see what the data mining algorithm finds when given the chance to overfit. Permutation testing makes it possible to generate an empirical p-value for the result. Replication in independent data may also provide evidence for an MDR model but can be sensitive to difference in the data sets. These approaches have all been shown to be useful for choosing and evaluating MDR models. An important step in a machine learning exercise is interpretation. Several approaches have been used with MDR including entropy analysis and pathway analysis. Tips and approaches for using MDR to model gene-gene interactions have been reviewed.
Numerous extensions to MDR have been introduced. These include family-based methods, fuzzy methods, covariate adjustment, odds ratios, risk scores, survival methods, robust methods, methods for quantitative traits, and many others.
MDR has mostly been applied to detecting gene-gene interactions or epistasis in genetic studies of common human diseases such as atrial fibrillation, autism, bladder cancer, breast cancer, cardiovascular disease, hypertension, obesity, pancreatic cancer, prostate cancer and tuberculosis. It has also been applied to other biomedical problems such as the genetic analysis of pharmacology outcomes. A central challenge is the scaling of MDR to big data such as that from genome-wide association studies (GWAS). Several approaches have been used. One approach is to filter the features prior to MDR analysis. This can be done using biological knowledge through tools such as BioFilter. It can also be done using computational tools such as ReliefF. Another approach is to use stochastic search algorithms such as genetic programming to explore the search space of feature combinations. Yet another approach is a brute-force search using high-performance computing.
www.epistasis.org provides an open-source and freely-available MDR software package.
An R package for MDR.
An sklearn-compatible Python implementation.
An R package for Model-Based MDR.
MDR in Weka.
Multilinear subspace learning
Michalski, R. S., "Pattern Recognition as Knowledge-Guided Computer Induction," Department of Computer Science Reports, No. 927, University of Illinois, Urbana, June 1978. Principal component analysis (PCA) is a linear dimensionality reduction technique with applications in exploratory data analysis, visualization and data preprocessing.
The data is linearly transformed onto a new coordinate system such that the directions (principal components) capturing the largest variation in the data can be easily identified.
The principal components of a collection of points in a real coordinate space are a sequence of
unit vectors, where the
-th vector is the direction of a line that best fits the data while being orthogonal to the first
vectors. Here, a best-fitting line is defined as one that minimizes the average squared perpendicular distance from the points to the line. These directions (i.e., principal components) constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated. Many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points.
Principal component analysis has applications in many fields such as population genetics, microbiome studies, and atmospheric science.
When performing PCA, the first principal component of a set of
variables is the derived variable formed as a linear combination of the original variables that explains the most variance. The second principal component explains the most variance in what is left once the effect of the first component is removed, and we may proceed through
iterations until all the variance is explained. PCA is most commonly used when many of the variables are highly correlated with each other and it is desirable to reduce their number to an independent set.
The first principal component can equivalently be defined as a direction that maximizes the variance of the projected data. The
-th principal component can be taken as a direction orthogonal to the first
principal components that maximizes the variance of the projected data.
For either objective, it can be shown that the principal components are eigenvectors of the data's covariance matrix. Thus, the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix. PCA is the simplest of the true eigenvector-based multivariate analyses and is closely related to factor analysis. Factor analysis typically incorporates more domain-specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix. PCA is also related to canonical correlation analysis (CCA). CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new orthogonal coordinate system that optimally describes variance in a single dataset. Robust and L1-norm-based variants of standard PCA have also been proposed.
PCA was invented in 1901 by Karl Pearson, as an analogue of the principal axis theorem in mechanics; it was later independently developed and named by Harold Hotelling in the 1930s. Depending on the field of application, it is also named the discrete Karhunen–Loève transform (KLT) in signal processing, the Hotelling transform in multivariate quality control, proper orthogonal decomposition (POD) in mechanical engineering, singular value decomposition (SVD) of X (invented in the last quarter of the 19th century), eigenvalue decomposition (EVD) of XTX in linear algebra, factor analysis (for a discussion of the differences between PCA and factor analysis see Ch. 7 of Jolliffe's Principal Component Analysis), Eckart–Young theorem (Harman, 1960), or empirical orthogonal functions (EOF) in meteorological science (Lorenz, 1956), empirical eigenfunction decomposition (Sirovich, 1987), quasiharmonic modes (Brooks et al., 1988), spectral decomposition in noise and vibration, and empirical modal analysis in structural dynamics.
PCA can be thought of as fitting a p-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small, then the variance along that axis is also small.
To find the axes of the ellipsoid, we must first center the values of each variable in the dataset on 0 by subtracting the mean of the variable's observed values from each of those values. These transformed values are used instead of the original observed values for each of the variables. Then, we compute the covariance matrix of the data and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix. Then we must normalize each of the orthogonal eigenvectors to turn them into unit vectors. Once this is done, each of the mutually-orthogonal unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data. This choice of basis will transform the covariance matrix into a diagonalized form, in which the diagonal elements represent the variance of each axis. The proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues.
Biplots and scree plots (degree of explained variance) are used to interpret findings of the PCA.
PCA is defined as an orthogonal linear transformation on a real inner product space that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.

data matrix, X, with column-wise zero empirical mean (the sample mean of each column has been shifted to zero), where each of the n rows represents a different repetition of the experiment, and each of the p columns gives a particular kind of feature (say, the results from a particular sensor).
Mathematically, the transformation is defined by a set of size
of p-dimensional vectors of weights or coefficients
 _=(w_,\dots ,w_)_
that map each row vector
 _=(x_,\dots ,x_)_
of X to a new vector of principal component scores
 _=(t_,\dots ,t_)_
, given by
_=\mathbf  _\cdot \mathbf  _\qquad \mathrm  \qquad i=1,\dots ,n\qquad k=1,\dots ,l
in such a way that the individual variables
,\dots ,t_
of t considered over the data set successively inherit the maximum possible variance from X, with each coefficient vector w constrained to be a unit vector (where
is usually selected to be strictly less than
to reduce dimensionality).
The above may equivalently be written in matrix form as
 =\mathbf  \mathbf  
 _=_
 _=_
 _=_
In order to maximize variance, the first weight vector w(1) thus has to satisfy
 _=\arg \max _ \Vert =1\,\left\(t_)_^\right\=\arg \max _ \Vert =1\,\left\\left(\mathbf  _\cdot \mathbf  \right)^\right\
Equivalently, writing this in matrix form gives
 _=\arg \max _ \right\|=1\left\ \right\|^\right\=\arg \max _ \right\|=1\left\ ^\mathbf  ^\mathbf  \right\
Since w(1) has been defined to be a unit vector, it equivalently also satisfies
 _=\arg \max \left\ ^\mathbf  ^\mathbf   ^\mathbf  \right\
The quantity to be maximised can be recognised as a Rayleigh quotient. A standard result for a positive semidefinite matrix such as XTX is that the quotient's maximum possible value is the largest eigenvalue of the matrix, which occurs when w is the corresponding eigenvector.
With w(1) found, the first principal component of a data vector x(i) can then be given as a score t1(i) = x(i) ⋅ w(1) in the transformed co-ordinates, or as the corresponding vector in the original variables,  w(1).
The k-th component can be found by subtracting the first k − 1 principal components from X:
 _=\mathbf  -\sum _^\mathbf  \mathbf  _\mathbf  _^
and then finding the weight vector which extracts the maximum variance from this new data matrix
 _=\mathop   _ \right\|=1\left\ _\mathbf  \right\|^\right\=\arg \max \left\ ^\mathbf  _^\mathbf  _\mathbf   ^\mathbf  \right\
It turns out that this gives the remaining eigenvectors of XTX, with the maximum values for the quantity in brackets given by their corresponding eigenvalues. Thus the weight vectors are eigenvectors of XTX.
The k-th principal component of a data vector x(i) can therefore be given as a score tk(i) = x(i) ⋅ w(k) in the transformed coordinates, or as the corresponding vector in the space of the original variables,  w(k), where w(k) is the kth eigenvector of XTX.
The full principal components decomposition of X can therefore be given as
 =\mathbf  \mathbf  
where W is a p-by-p matrix of weights whose columns are the eigenvectors of XTX. The transpose of W is sometimes called the whitening or sphering transformation. Columns of W multiplied by the square root of corresponding eigenvalues, that is, eigenvectors scaled up by the variances, are called loadings in PCA or in Factor analysis.
XTX itself can be recognized as proportional to the empirical sample covariance matrix of the dataset XT.
The sample covariance Q between two of the different principal components over the dataset is given by:
Q(\mathrm  _,\mathrm  _)&\propto (\mathbf  \mathbf  _)^(\mathbf  \mathbf  _)\\&=\mathbf  _^\mathbf  ^\mathbf  \mathbf  _\\&=\mathbf  _^\lambda _\mathbf  _\\&=\lambda _\mathbf  _^\mathbf  _\end
where the eigenvalue property of w(k) has been used to move from line 2 to line 3. However eigenvectors w(j) and w(k) corresponding to eigenvalues of a symmetric matrix are orthogonal (if the eigenvalues are different), or can be orthogonalised (if the vectors happen to share an equal repeated value). The product in the final line is therefore zero; there is no sample covariance between different principal components over the dataset.
Another way to characterise the principal components transformation is therefore as the transformation to coordinates which diagonalise the empirical sample covariance matrix.
In matrix form, the empirical covariance matrix for the original variables can be written
 \propto \mathbf  ^\mathbf  =\mathbf  \mathbf  \mathbf  ^
The empirical covariance matrix between the principal components becomes
 ^\mathbf  \mathbf  \propto \mathbf  ^\mathbf  \,\mathbf  \,\mathbf  ^\mathbf  =\mathbf  
where Λ is the diagonal matrix of eigenvalues λ(k) of XTX. λ(k) is equal to the sum of the squares over the dataset associated with each component k, that is, λ(k) = Σi tk2(i) = Σi (x(i) ⋅ w(k))2.
The transformation P = X W maps a data vector x(i) from an original space of x variables to a new space of p variables which are uncorrelated over the dataset.
To non-dimensionalize the centered data, let Xc represent the characteristic values of data vectors Xi, given by:

\|X\|_
(mean absolute value), or
\|X\|_
(normalized Euclidean norm),
for a dataset of size n. These norms are used to transform the original space of variables x, y to a new space of uncorrelated variables p, q (given Yc with same meaning), such that
=,\quad q_=
and the new variables are linearly related as:

To find the optimal linear relationship, we minimize the total squared reconstruction error:
\sum _^(\alpha p_-q_)^
; such that setting the derivative of the error function to zero

\left(-\lambda \pm +4\right)

Such dimensionality reduction can be a very useful step for visualising and processing high-dimensional datasets, while still retaining as much of the variance in the dataset as possible. For example, selecting L = 2 and keeping only the first two principal components finds the two-dimensional plane through the high-dimensional dataset in which the data is most spread out, so if the data contains clusters these too may be most spread out, and therefore most visible to be plotted out in a two-dimensional diagram; whereas if two directions through the data (or two of the original variables) are chosen at random, the clusters may be much less spread apart from each other, and may in fact be much more likely to substantially overlay each other, making them indistinguishable.
Similarly, in regression analysis, the larger the number of explanatory variables allowed, the greater is the chance of overfitting the model, producing conclusions that fail to generalise to other datasets. One approach, especially when there are strong correlations between different possible explanatory variables, is to reduce them to a few principal components and then run the regression against them, a method called principal component regression.
Dimensionality reduction may also be appropriate when the variables in a dataset are noisy. If each column of the dataset contains independent identically distributed Gaussian noise, then the columns of T will also contain similarly identically distributed Gaussian noise (such a distribution is invariant under the effects of the matrix W, which can be thought of as a high-dimensional rotation of the co-ordinate axes). However, with more of the total variance concentrated in the first few principal components compared to the same noise variance, the proportionate effect of the noise is less—the first few components achieve a higher signal-to-noise ratio. PCA thus can have the effect of concentrating much of the signal into the first few principal components, which can usefully be captured by dimensionality reduction; while the later principal components may be dominated by noise, and so disposed of without great loss. If the dataset is not too large, the significance of the principal components can be tested using parametric bootstrap, as an aid in determining how many principal components to retain.
The principal components transformation can also be associated with another matrix factorization, the singular value decomposition (SVD) of X,
 =\mathbf  \mathbf  \mathbf  ^
Here Σ is an n-by-p rectangular diagonal matrix of positive numbers σ(k), called the singular values of X; U is an n-by-n matrix, the columns of which are orthogonal unit vectors of length n called the left singular vectors of X; and W is a p-by-p matrix whose columns are orthogonal unit vectors of length p and called the right singular vectors of X.
In terms of this factorization, the matrix XTX can be written
\mathbf  ^\mathbf  &=\mathbf  \mathbf  ^\mathbf  ^\mathbf  \mathbf  \mathbf  ^\\&=\mathbf  \mathbf  ^\mathbf  \mathbf  ^\\&=\mathbf  \mathbf  ^\mathbf  ^\end
 
is the square diagonal matrix with the singular values of X and the excess zeros chopped off that satisfies
^ =\mathbf  ^\mathbf  
. Comparison with the eigenvector factorization of XTX establishes that the right singular vectors W of X are equivalent to the eigenvectors of XTX, while the singular values σ(k) of
 
are equal to the square-root of the eigenvalues λ(k) of XTX.
Using the singular value decomposition the score matrix T can be written
\mathbf  &=\mathbf  \mathbf  \\&=\mathbf  \mathbf  \mathbf  ^\mathbf  \\&=\mathbf  \mathbf  \end
so each column of T is given by one of the left singular vectors of X multiplied by the corresponding singular value. This form is also the polar decomposition of T.
Efficient algorithms exist to calculate the SVD of X without having to form the matrix XTX, so computing the SVD is now the standard way to calculate a principal components analysis from a data matrix, unless only a handful of components are required.
As with the eigen-decomposition, a truncated n × L score matrix TL can be obtained by considering only the first L largest singular values and their singular vectors:
 _=\mathbf  _\mathbf  _=\mathbf  \mathbf  _
The truncation of a matrix M or T using a truncated singular value decomposition in this way produces a truncated matrix that is the nearest possible matrix of rank L to the original matrix, in the sense of the difference between the two having the smallest possible Frobenius norm, a result known as the Eckart–Young theorem [1936].
Theorem (Optimal k‑dimensional fit).
Let P be an n×m data matrix whose columns have been mean‑centered and scaled, and let

be its singular value decomposition. Then the best rank‑k approximation to P in the least‑squares (Frobenius‑norm) sense is
=U_\,\Sigma _\,V_^
where Vk consists of the first k columns of V. Moreover, the relative residual variance is
^\sigma _^^\sigma _^
The singular values (in Σ) are the square roots of the eigenvalues of the matrix XTX. Each eigenvalue is proportional to the portion of the "variance" (more correctly of the sum of the squared distances of the points from their multidimensional mean) that is associated with each eigenvector. The sum of all the eigenvalues is equal to the sum of the squared distances of the points from their multidimensional mean. PCA essentially rotates the set of points around their mean in order to align with the principal components. This moves as much of the variance as possible (using an orthogonal transformation) into the first few dimensions. The values in the remaining dimensions, therefore, tend to be small and may be dropped with minimal loss of information (see below). PCA is often used in this manner for dimensionality reduction. PCA has the distinction of being the optimal orthogonal transformation for keeping the subspace that has largest "variance" (as defined above). This advantage, however, comes at the price of greater computational requirements if compared, for example, and when applicable, to the discrete cosine transform, and in particular to the DCT-II which is simply known as the "DCT". Nonlinear dimensionality reduction techniques tend to be more computationally demanding than PCA.
PCA is sensitive to the scaling of the variables. Mathematically this sensitivity comes from the way a rescaling changes the sample‑covariance matrix that PCA diagonalises.
 _
be the *centered* data matrix (n rows, p columns) and define the covariance
\,\mathbf  _^\mathbf  _.
‑th variable is multiplied by a factor

 _^=\mathbf  _D,\qquad D=\operatorname  (\alpha _,\ldots ,\alpha _).
Hence the new covariance is
=D^\,\Sigma \,D.
Because the eigenvalues and eigenvectors of

are those of

, the principal axes rotate toward any column whose variance has been inflated, exactly as the 2‑D example below illustrates.
If we have just two variables and they have the same sample variance and are completely correlated, then the PCA will entail a rotation by 45° and the "weights" (they are the cosines of rotation) for the two variables with respect to the principal component will be equal. But if we multiply all values of the first variable by 100, then the first principal component will be almost the same as that variable, with a small contribution from the other variable, whereas the second component will be almost aligned with the second original variable. This means that whenever the different variables have different units (like temperature and mass), PCA is a somewhat arbitrary method of analysis. (Different results would be obtained if one used Fahrenheit rather than Celsius for example.) Pearson's original paper was entitled "On Lines and Planes of Closest Fit to Systems of Points in Space" – "in space" implies physical Euclidean space where such concerns do not arise. One way of making the PCA less arbitrary is to use variables scaled so as to have unit variance, by standardizing the data and hence use the autocorrelation matrix instead of the autocovariance matrix as a basis for PCA. However, this compresses (or expands) the fluctuations in all dimensions of the signal space to unit variance.
Classical PCA assumes the cloud of points has already been translated so its centroid is at the origin.
Write each observation as
 _=+\mathbf  _,\qquad =\sum _^\mathbf  _.

we are in effect diagonalising
\;=\;n\,^\;+\;\,\mathbf  ^\mathbf  ,
 
is the centered matrix.
The rank‑one term
^
often dominates, forcing the leading eigenvector to point almost exactly toward the mean and obliterating any structure in the centred part
 
After mean subtraction that term vanishes and the principal axes align with the true directions of maximal variance.
Mean-centering is unnecessary if performing a principal components analysis on a correlation matrix, as the data are already centered after calculating correlations. Correlations are derived from the cross-product of two standard scores (Z-scores) or statistical moments (hence the name: Pearson Product-Moment Correlation). Also see the article by Kromrey & Foster-Johnson (1998) on "Mean-centering in Moderated Regression: Much Ado About Nothing". Since covariances are correlations of normalized variables (Z- or standard-scores) a PCA based on the correlation matrix of X is equal to a PCA based on the covariance matrix of Z, the standardized version of X.
PCA is a popular primary technique in pattern recognition. It is not, however, optimized for class separability. However, it has been used to quantify the distance between two or more classes by calculating center of mass for each class in principal component space and reporting Euclidean distance between center of mass of two or more classes. The linear discriminant analysis is an alternative which is optimized for class separability.
Some properties of PCA include:
Property 1: For any integer q, 1 ≤ q ≤ p, consider the orthogonal linear transformation
 x
is a q-element vector and
 
is a (q × p) matrix, and let
 _=\mathbf  \mathbf  \mathbf  
be the variance-covariance matrix for
. Then the trace of
 _
 (\mathbf  _)
, is maximized by taking
 =\mathbf  _
 _
consists of the first q columns of
 
 
is the transpose of
 )
 
is not defined here)
Property 2: Consider again the orthonormal transformation
 x
 ,\mathbf  
 _
defined as before. Then
 (\mathbf  _)
is minimized by taking
 =\mathbf  _^,
 _^
consists of the last q columns of
 
The statistical implication of this property is that the last few PCs are not simply unstructured left-overs after removing the important PCs. Because these last PCs have variances as small as possible they are useful in their own right. They can help to detect unsuspected near-constant linear relationships between the elements of x, and they may also be useful in regression, in selecting a subset of variables from x, and in outlier detection.
Property 3: (Spectral decomposition of Σ)
 =\lambda _\alpha _\alpha _'+\cdots +\lambda _\alpha _\alpha _'
Before we look at its usage, we first look at diagonal elements,
 (x_)=\sum _^\lambda _\alpha _^
Then, perhaps the main statistical implication of the result is that not only can we decompose the combined variances of all the elements of x into decreasing contributions due to each PC, but we can also decompose the whole covariance matrix into contributions
\alpha _\alpha _'
from each PC. Although not strictly decreasing, the elements of
\alpha _\alpha _'
will tend to become smaller as
\alpha _\alpha _'
is nonincreasing for increasing
, whereas the elements of

tend to stay about the same size because of the normalization constraints:
'\alpha _=1,k=1,\dots ,p
As noted above, the results of PCA depend on the scaling of the variables. This can be cured by scaling each feature by its standard deviation, so that one ends up with dimensionless features with unital variance.
The applicability of PCA as described above is limited by certain (tacit) assumptions made in its derivation. In particular, PCA can capture linear correlations between the features but fails when this assumption is violated (see Figure 6a in the reference). In some cases, coordinate transformations can restore the linearity assumption and PCA can then be applied (see kernel PCA).
Another limitation is the mean-removal process before constructing the covariance matrix for PCA. In fields such as astronomy, all the signals are non-negative, and the mean-removal process will force the mean of some astrophysical exposures to be zero, which consequently creates unphysical negative fluxes, and forward modeling has to be performed to recover the true magnitude of the signals. As an alternative method, non-negative matrix factorization focusing only on the non-negative elements in the matrices is well-suited for astrophysical observations. See more at the relation between PCA and non-negative matrix factorization.
PCA is at a disadvantage if the data has not been standardized before applying the algorithm to it. PCA transforms the original data into data that is relevant to the principal components of that data, which means that the new data variables cannot be interpreted in the same ways that the originals were. They are linear interpretations of the original variables. Also, if PCA is not performed properly, there is a high likelihood of information loss.
PCA relies on a linear model. If a dataset has a pattern hidden inside it that is nonlinear, then PCA can actually steer the analysis in the complete opposite direction of progress. Researchers at Kansas State University discovered that the sampling error in their experiments impacted the bias of PCA results. "If the number of subjects or blocks is smaller than 30, and/or the researcher is interested in PC's beyond the first, it may be better to first correct for the serial correlation, before PCA is conducted". The researchers at Kansas State also found that PCA could be "seriously biased if the autocorrelation structure of the data is not correctly handled".
Dimensionality reduction results in a loss of information, in general. PCA-based dimensionality reduction tends to minimize that information loss, under certain signal and noise models.
Under the assumption that
 =\mathbf  +\mathbf  ,
that is, that the data vector
 
is the sum of the desired information-bearing signal
 
and a noise signal
 
one can show that PCA can be optimal for dimensionality reduction, from an information-theoretic point-of-view.
In particular, Linsker showed that if
 
is Gaussian and
 
is Gaussian noise with a covariance matrix proportional to the identity matrix, the PCA maximizes the mutual information
 ;\mathbf  )
between the desired information
 
and the dimensionality-reduced output
 =\mathbf  _^\mathbf  
If the noise is still Gaussian and has a covariance matrix proportional to the identity matrix (that is, the components of the vector
 
are iid), but the information-bearing signal
 
is non-Gaussian (which is a common scenario), PCA at least minimizes an upper bound on the information loss, which is defined as
 ;\mathbf  )-I(\mathbf  ;\mathbf  ).
The optimality of PCA is also preserved if the noise
 
is iid and at least more Gaussian (in terms of the Kullback–Leibler divergence) than the information-bearing signal
 
. In general, even if the above signal model holds, PCA loses its information-theoretic optimality as soon as the noise
 
The following is a detailed description of PCA using the covariance method as opposed to the correlation method.
The goal is to transform a given data set X of dimension p to an alternative data set Y of smaller dimension L. Equivalently, we are seeking to find the matrix Y, where Y is the Karhunen–Loève transform (KLT) of matrix X:
 =\mathbb  \ \
Organize the data set
Suppose you have data comprising a set of observations of p variables, and you want to reduce the data so that each observation can be described with only L variables, L < p. Suppose further, that the data are arranged as a set of n data vectors
 _\ldots \mathbf  _
 _
representing a single grouped observation of the p variables.
 _\ldots \mathbf  _
as row vectors, each with p elements.
Place the row vectors into a single matrix X of dimensions n × p.
Calculate the empirical mean
Find the empirical mean along each column j = 1, ..., p.
Place the calculated mean values into an empirical mean vector u of dimensions p × 1.
=\sum _^X_
Calculate the deviations from the mean
Mean subtraction is an integral part of the solution towards finding a principal component basis that minimizes the mean square error of approximating the data. Hence we proceed by centering the data as follows:
Subtract the empirical mean vector
 ^
from each row of the data matrix X.
Store mean-subtracted data in the n × p matrix B.
 =\mathbf  -\mathbf  \mathbf  ^
where h is an n × 1 column vector of all 1s:
=1\,\qquad \qquad i=1,\ldots ,n
In some applications, each variable (column of B) may also be scaled to have a variance equal to 1 (see Z-score). This step affects the calculated principal components, but makes them independent of the units used to measure the different variables.
Find the covariance matrix
Find the p × p empirical covariance matrix C from matrix B:
 =\mathbf  ^\mathbf  
is the conjugate transpose operator. If B consists entirely of real numbers, which is the case in many applications, the "conjugate transpose" is the same as the regular transpose.
The reasoning behind using n − 1 instead of n to calculate the covariance is Bessel's correction.
Find the eigenvectors and eigenvalues of the covariance matrix
Compute the matrix V of eigenvectors which diagonalizes the covariance matrix C:
 ^\mathbf  \mathbf  =\mathbf  
where D is the diagonal matrix of eigenvalues of C. This step will typically involve the use of a computer-based algorithm for computing eigenvectors and eigenvalues. These algorithms are readily available as sub-components of most matrix algebra systems, such as SAS, R, MATLAB, Mathematica, SciPy, IDL (Interactive Data Language), or GNU Octave as well as OpenCV.
Matrix D will take the form of an p × p diagonal matrix, where
=\lambda _\qquad k=\ell 
is the jth eigenvalue of the covariance matrix C, and
=0\qquad k\neq \ell .
Matrix V, also of dimension p × p, contains p column vectors, each of length p, which represent the p eigenvectors of the covariance matrix C.
The eigenvalues and eigenvectors are ordered and paired. The jth eigenvalue corresponds to the jth eigenvector.
Matrix V denotes the matrix of right eigenvectors (as opposed to left eigenvectors). In general, the matrix of right eigenvectors need not be the (conjugate) transpose of the matrix of left eigenvectors.
Rearrange the eigenvectors and eigenvalues
Sort the columns of the eigenvector matrix V and eigenvalue matrix D in order of decreasing eigenvalue.
Make sure to maintain the correct pairings between the columns in each matrix.
Compute the cumulative energy content for each eigenvector
The eigenvalues represent the distribution of the source data's energy among each of the eigenvectors, where the eigenvectors form a basis for the data. The cumulative energy content g for the jth eigenvector is the sum of the energy content across all of the eigenvalues from 1 through j:
=\sum _^D_\qquad j=1,\dots ,p
Select a subset of the eigenvectors as basis vectors
Save the first L columns of V as the p × L matrix W:
=V_\qquad k=1,\dots ,p\qquad \ell =1,\dots ,L

Use the vector g as a guide in choosing an appropriate value for L. The goal is to choose a value of L as small as possible while achieving a reasonably high value of g on a percentage basis. For example, you may want to choose L so that the cumulative energy g is above a certain threshold, like 90 percent. In this case, choose the smallest value of L such that
\geq 0.9
Project the data onto the new basis
The projected data points are the rows of the matrix
 =\mathbf  \cdot \mathbf  
That is, the first column of
 
is the projection of the data points onto the first principal component, the second column is the projection onto the second principal component, etc.
Let X be a d-dimensional random vector expressed as column vector. Without loss of generality, assume X has zero mean.
We want to find

a d × d orthonormal transformation matrix P so that PX has a diagonal covariance matrix (that is, PX is a random vector with all its distinct components pairwise uncorrelated).
A quick computation assuming
were unitary yields:
\operatorname  (PX)&=\operatorname  [PX~(PX)^]\\&=\operatorname  [PX~X^P^]\\&=P\operatorname  [XX^]P^\\&=P\operatorname  (X)P^\\\end

holds if and only if
 (X)
were diagonalisable by
This is very constructive, as cov(X) is guaranteed to be a non-negative definite matrix and thus is guaranteed to be diagonalisable by some unitary matrix.
In practical implementations, especially with high dimensional data (large p), the naive covariance method is rarely used because it is not efficient due to high computational and memory costs of explicitly determining the covariance matrix. The covariance-free approach avoids the np2 operations of explicitly calculating and storing the covariance matrix XTX, instead utilizing one of matrix-free methods, for example, based on the function evaluating the product XT(X r) at the cost of 2np operations.
One way to compute the first principal component efficiently is shown in the following pseudo-code, for a data matrix X with zero mean, without ever computing its covariance matrix.
r = a random vector of length p
r = r / norm(r)
do c times:
s = 0 (a vector of length p)
for each row x in X
s = s + (x ⋅ r) x
λ = rTs // λ is the eigenvalue
error = |λ ⋅ r − s|
r = s / norm(s)
exit if error < tolerance
return λ, r
This power iteration algorithm simply calculates the vector XT(X r), normalizes, and places the result back in r. The eigenvalue is approximated by rT (XTX) r, which is the Rayleigh quotient on the unit vector r for the covariance matrix XTX . If the largest singular value is well separated from the next largest one, the vector r gets close to the first principal component of X within the number of iterations c, which is small relative to p, at the total cost 2cnp. The power iteration convergence can be accelerated without noticeably sacrificing the small cost per iteration using more advanced matrix-free methods, such as the Lanczos algorithm or the Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) method.
Subsequent principal components can be computed one-by-one via deflation or simultaneously as a block. In the former approach, imprecisions in already computed approximate principal components additively affect the accuracy of the subsequently computed principal components, thus increasing the error with every new computation. The latter approach in the block power method replaces single-vectors r and s with block-vectors, matrices R and S. Every column of R approximates one of the leading principal components, while all columns are iterated simultaneously. The main calculation is evaluation of the product XT(X R). Implemented, for example, in LOBPCG, efficient blocking eliminates the accumulation of the errors, allows using high-level BLAS matrix-matrix product functions, and typically leads to faster convergence, compared to the single-vector one-by-one technique.
Non-linear iterative partial least squares (NIPALS) is a variant the classical power iteration with matrix deflation by subtraction implemented for computing the first few components in a principal component or partial least squares analysis. For very-high-dimensional datasets, such as those generated in the *omics sciences (for example, genomics, metabolomics) it is usually only necessary to compute the first few PCs. The non-linear iterative partial least squares (NIPALS) algorithm updates iterative approximations to the leading scores and loadings t1 and r1T by the power iteration multiplying on every iteration by X on the left and on the right, that is, calculation of the covariance matrix is avoided, just as in the matrix-free implementation of the power iterations to XTX, based on the function evaluating the product XT(X r) = ((X r)TX)T.
The matrix deflation by subtraction is performed by subtracting the outer product, t1r1T from X leaving the deflated residual matrix used to calculate the subsequent leading PCs.
For large data matrices, or matrices that have a high degree of column collinearity, NIPALS suffers from loss of orthogonality of PCs due to machine precision round-off errors accumulated in each iteration and matrix deflation by subtraction. A Gram–Schmidt re-orthogonalization algorithm is applied to both the scores and the loadings at each iteration step to eliminate this loss of orthogonality. NIPALS reliance on single-vector multiplications cannot take advantage of high-level BLAS and results in slow convergence for clustered leading singular values—both these deficiencies are resolved in more sophisticated matrix-free block solvers, such as the Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) method.
In an "online" or "streaming" situation with data arriving piece by piece rather than being stored in a single batch, it is useful to make an estimate of the PCA projection that can be updated sequentially. This can be done efficiently, but requires different algorithms.
In PCA, it is common that we want to introduce qualitative variables as supplementary elements. For example, many quantitative variables have been measured on plants. For these plants, some qualitative variables are available as, for example, the species to which the plant belongs. These data were subjected to PCA for quantitative variables. When analyzing the results, it is natural to connect the principal components to the qualitative variable species.
For this, the following results are produced.
Identification, on the factorial planes, of the different species, for example, using different colors.
Representation, on the factorial planes, of the centers of gravity of plants belonging to the same species.
For each center of gravity and each axis, p-value to judge the significance of the difference between the center of gravity and origin.
These results are what is called introducing a qualitative variable as supplementary element. This procedure is detailed in and Husson, Lê, & Pagès (2009) and Pagès (2013).
Few software offer this option in an "automatic" way. This is the case of SPAD that historically, following the work of Ludovic Lebart, was the first to propose this option, and the R package FactoMineR.
The earliest application of factor analysis was in locating and measuring components of human intelligence. It was believed that intelligence had various uncorrelated components such as spatial intelligence, verbal intelligence, induction, deduction etc and that scores on these could be adduced by factor analysis from results on various tests, to give a single index known as the Intelligence Quotient (IQ). The pioneering statistical psychologist Spearman actually developed factor analysis in 1904 for his two-factor theory of intelligence, adding a formal technique to the science of psychometrics. In 1924 Thurstone looked for 56 factors of intelligence, developing the notion of Mental Age. Standard IQ tests today are based on this early work.
In 1949, Shevky and Williams introduced the theory of factorial ecology, which dominated studies of residential differentiation from the 1950s to the 1970s. Neighbourhoods in a city were recognizable or could be distinguished from one another by various characteristics which could be reduced to three by factor analysis. These were known as 'social rank' (an index of occupational status), 'familism' or family size, and 'ethnicity'; Cluster analysis could then be applied to divide the city into clusters or precincts according to values of the three key factor variables. An extensive literature developed around factorial ecology in urban geography, but the approach went out of fashion after 1980 as being methodologically primitive and having little place in postmodern geographical paradigms.
One of the problems with factor analysis has always been finding convincing names for the various artificial factors. In 2000, Flood revived the factorial ecology approach to show that principal components analysis actually gave meaningful answers directly, without resorting to factor rotation. The principal components were actually dual variables or shadow prices of 'forces' pushing people together or apart in cities. The first component was 'accessibility', the classic trade-off between demand for travel and demand for space, around which classical urban economics is based. The next two components were 'disadvantage', which keeps people of similar status in separate neighbourhoods (mediated by planning), and ethnicity, where people of similar ethnic backgrounds try to co-locate.
About the same time, the Australian Bureau of Statistics defined distinct indexes of advantage and disadvantage taking the first principal component of sets of key variables that were thought to be important. These SEIFA indexes are regularly published for various jurisdictions, and are used frequently in spatial analysis.
PCA can be used as a formal method for the development of indexes. As an alternative confirmatory composite analysis has been proposed to develop and assess indexes.
The City Development Index was developed by PCA from about 200 indicators of city outcomes in a 1996 survey of 254 global cities. The first principal component was subject to iterative regression, adding the original variables singly until about 90% of its variation was accounted for. The index ultimately used about 15 indicators but was a good predictor of many more variables. Its comparative value agreed very well with a subjective assessment of the condition of each city. The coefficients on items of infrastructure were roughly proportional to the average costs of providing the underlying services, suggesting the Index was actually a measure of effective physical and social investment in the city.
The country-level Human Development Index (HDI) from UNDP, which has been published since 1990 and is very extensively used in development studies, has very similar coefficients on similar indicators, strongly suggesting it was originally constructed using PCA.
In 1978 Cavalli-Sforza and others pioneered the use of principal components analysis (PCA) to summarise data on variation in human gene frequencies across regions. The components showed distinctive patterns, including gradients and sinusoidal waves. They interpreted these patterns as resulting from specific ancient migration events.
Since then, PCA has been ubiquitous in population genetics, with thousands of papers using PCA as a display mechanism. Genetics varies largely according to proximity, so the first two principal components actually show spatial distribution and may be used to map the relative geographical location of different population groups, thereby showing individuals who have wandered from their original locations.
PCA in genetics has been technically controversial, in that the technique has been performed on discrete non-normal variables and often on binary allele markers. The lack of any measures of standard error in PCA are also an impediment to more consistent usage. In August 2022, the molecular biologist Eran Elhaik published a theoretical paper in Scientific Reports analyzing 12 PCA applications. He concluded that it was easy to manipulate the method, which, in his view, generated results that were 'erroneous, contradictory, and absurd.' Specifically, he argued, the results achieved in population genetics were characterized by cherry-picking and circular reasoning.
Market research has been an extensive user of PCA. It is used to develop customer satisfaction or customer loyalty scores for products, and with clustering, to develop market segments that may be targeted with advertising campaigns, in much the same way as factorial ecology will locate geographical areas with similar characteristics.
PCA rapidly transforms large amounts of data into smaller, easier-to-digest variables that can be more rapidly and readily analyzed. In any consumer questionnaire, there are series of questions designed to elicit consumer attitudes, and principal components seek out latent variables underlying these attitudes. For example, the Oxford Internet Survey in 2013 asked 2000 people about their attitudes and beliefs, and from these analysts extracted four principal component dimensions, which they identified as 'escape', 'social networking', 'efficiency', and 'problem creating'.
Another example from Joe Flood in 2008 extracted an attitudinal index toward housing from 28 attitude questions in a national survey of 2697 households in Australia. The first principal component represented a general attitude toward property and home ownership. The index, or the attitude questions it embodied, could be fed into a General Linear Model of tenure choice. The strongest determinant of private renting by far was the attitude index, rather than income, marital status or household type.
In quantitative finance, PCA is used
in financial risk management, and has been applied to other problems such as portfolio optimization.
PCA is commonly used in problems involving fixed income securities and portfolios, and interest rate derivatives.
Valuations here depend on the entire yield curve, comprising numerous highly correlated instruments, and PCA is used to define a set of components or factors that explain rate movements,
thereby facilitating the modelling.
One common risk management application is to calculating value at risk, VaR, applying PCA to the Monte Carlo simulation.
Here, for each simulation-sample, the components are stressed, and rates, and in turn option values, are then reconstructed;
with VaR calculated, finally, over the entire run.
PCA is also used in hedging exposure to interest rate risk, given partial durations and other sensitivities.
Under both, the first three, typically, principal components of the system are of interest (representing "shift", "twist", and "curvature").
These principal components are derived from an eigen-decomposition of the covariance matrix of yield at predefined maturities;
and where the variance of each component is its eigenvalue (and as the components are orthogonal, no correlation need be incorporated in subsequent modelling).
For equity, an optimal portfolio is one where the expected return is maximized for a given level of risk, or alternatively, where risk is minimized for a given return; see Markowitz model for discussion.
Thus, one approach is to reduce portfolio risk, where allocation strategies are applied to the "principal portfolios" instead of the underlying stocks.
A second approach is to enhance portfolio return, using the principal components to select companies' stocks with upside potential.
PCA has also been used to understand relationships between international equity markets, and within markets between groups of companies in industries or sectors.
PCA may also be applied to stress testing, essentially an analysis of a bank's ability to endure a hypothetical adverse economic scenario. Its utility is in "distilling the information contained in [several] macroeconomic variables into a more manageable data set, which can then [be used] for analysis." Here, the resulting factors are linked to e.g. interest rates – based on the largest elements of the factor's eigenvector – and it is then observed how a "shock" to each of the factors affects the implied assets of each of the banks.
A variant of principal components analysis is used in neuroscience to identify the specific properties of a stimulus that increases a neuron's probability of generating an action potential. This technique is known as spike-triggered covariance analysis. In a typical application an experimenter presents a white noise process as a stimulus (usually either as a sensory input to a test subject, or as a current injected directly into the neuron) and records a train of action potentials, or spikes, produced by the neuron as a result. Presumably, certain features of the stimulus make the neuron more likely to spike. In order to extract these features, the experimenter calculates the covariance matrix of the spike-triggered ensemble, the set of all stimuli (defined and discretized over a finite time window, typically on the order of 100 ms) that immediately preceded a spike. The eigenvectors of the difference between the spike-triggered covariance matrix and the covariance matrix of the prior stimulus ensemble (the set of all stimuli, defined over the same length time window) then indicate the directions in the space of stimuli along which the variance of the spike-triggered ensemble differed the most from that of the prior stimulus ensemble. Specifically, the eigenvectors with the largest positive eigenvalues correspond to the directions along which the variance of the spike-triggered ensemble showed the largest positive change compared to the variance of the prior. Since these were the directions in which varying the stimulus led to a spike, they are often good approximations of the sought after relevant stimulus features.
In neuroscience, PCA is also used to discern the identity of a neuron from the shape of its action potential. Spike sorting is an important procedure because extracellular recording techniques often pick up signals from more than one neuron. In spike sorting, one first uses PCA to reduce the dimensionality of the space of action potential waveforms, and then performs clustering analysis to associate specific action potentials with individual neurons.
PCA as a dimension reduction technique is particularly suited to detect coordinated activities of large neuronal ensembles. It has been used in determining collective variables, that is, order parameters, during phase transitions in the brain.
Correspondence analysis (CA)
was developed by Jean-Paul Benzécri
and is conceptually similar to PCA, but scales the data (which should be non-negative) so that rows and columns are treated equivalently. It is traditionally applied to contingency tables.
CA decomposes the chi-squared statistic associated to this table into orthogonal factors.
Because CA is a descriptive technique, it can be applied to tables for which the chi-squared statistic is appropriate or not.
Several variants of CA are available including detrended correspondence analysis and canonical correspondence analysis. One special extension is multiple correspondence analysis, which may be seen as the counterpart of principal component analysis for categorical data.
Principal component analysis creates variables that are linear combinations of the original variables. The new variables have the property that the variables are all orthogonal. The PCA transformation can be helpful as a pre-processing step before clustering. PCA is a variance-focused approach seeking to reproduce the total variable variance, in which components reflect both common and unique variance of the variable. PCA is generally preferred for purposes of data reduction (that is, translating variable space into optimal factor space) but not when the goal is to detect the latent construct or factors.
Factor analysis is similar to principal component analysis, in that factor analysis also involves linear combinations of variables. Different from PCA, factor analysis is a correlation-focused approach seeking to reproduce the inter-correlations among variables, in which the factors "represent the common variance of variables, excluding unique variance". In terms of the correlation matrix, this corresponds with focusing on explaining the off-diagonal terms (that is, shared co-variance), while PCA focuses on explaining the terms that sit on the diagonal. However, as a side result, when trying to reproduce the on-diagonal terms, PCA also tends to fit relatively well the off-diagonal correlations. Results given by PCA and factor analysis are very similar in most situations, but this is not always the case, and there are some problems where the results are significantly different. Factor analysis is generally used when the research purpose is detecting data structure (that is, latent constructs or factors) or causal modeling. If the factor model is incorrectly formulated or the assumptions are not met, then factor analysis will give erroneous results.
It has been asserted that the relaxed solution of k-means clustering, specified by the cluster indicators, is given by the principal components, and the PCA subspace spanned by the principal directions is identical to the cluster centroid subspace. However, that PCA is a useful relaxation of k-means clustering was not a new result, and it is straightforward to uncover counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions.
Non-negative matrix factorization (NMF) is a dimension reduction method where only non-negative elements in the matrices are used, which is therefore a promising method in astronomy, in the sense that astrophysical signals are non-negative. The PCA components are orthogonal to each other, while the NMF components are all non-negative and therefore constructs a non-orthogonal basis.
In PCA, the contribution of each component is ranked based on the magnitude of its corresponding eigenvalue, which is equivalent to the fractional residual variance (FRV) in analyzing empirical data. For NMF, its components are ranked based only on the empirical FRV curves. The residual fractional eigenvalue plots, that is,
^\lambda _\sum _^\lambda _
as a function of component number
given a total of
components, for PCA have a flat plateau, where no data is captured to remove the quasi-static noise, then the curves drop quickly as an indication of over-fitting (random noise). The FRV curves for NMF is decreasing continuously when the NMF components are constructed sequentially, indicating the continuous capturing of quasi-static noise; then converge to higher levels than PCA, indicating the less over-fitting property of NMF.
It is often difficult to interpret the principal components when the data include many variables of various origins, or when some variables are qualitative. This leads the PCA user to a delicate elimination of several variables. If observations or variables have an excessive impact on the direction of the axes, they should be removed and then projected as supplementary elements. In addition, it is necessary to avoid interpreting the proximities between the points close to the center of the factorial plane.
The iconography of correlations, on the contrary, which is not a projection on a system of axes, does not have these drawbacks. We can therefore keep all the variables.
The principle of the diagram is to underline the "remarkable" correlations of the correlation matrix, by a solid line (positive correlation) or dotted line (negative correlation).
A strong correlation is not "remarkable" if it is not direct, but caused by the effect of a third variable. Conversely, weak correlations can be "remarkable". For example, if a variable Y depends on several independent variables, the correlations of Y with each of them are weak and yet "remarkable".
A particular disadvantage of PCA is that the principal components are usually linear combinations of all input variables. Sparse PCA overcomes this disadvantage by finding linear combinations that contain just a few input variables. It extends the classic method of principal component analysis (PCA) for the reduction of dimensionality of data by adding sparsity constraint on the input variables.
Several approaches have been proposed, including
a regression framework,
a convex relaxation/semidefinite programming framework,
a generalized power method framework
an alternating maximization framework
forward-backward greedy search and exact methods using branch-and-bound techniques,
Bayesian formulation framework.
The methodological and theoretical developments of Sparse PCA as well as its applications in scientific studies were recently reviewed in a survey paper.
Most of the modern methods for nonlinear dimensionality reduction find their theoretical and algorithmic roots in PCA or K-means. Pearson's original idea was to take a straight line (or plane) which will be "the best fit" to a set of data points. Trevor Hastie expanded on this concept by proposing Principal curves as the natural extension for the geometric interpretation of PCA, which explicitly constructs a manifold for data approximation followed by projecting the points onto it. See also the elastic map algorithm and principal geodesic analysis. Another popular generalization is kernel PCA, which corresponds to PCA performed in a reproducing kernel Hilbert space associated with a positive definite kernel.
In multilinear subspace learning, PCA is generalized to multilinear PCA (MPCA) that extracts features directly from tensor representations. MPCA is solved by performing PCA in each mode of the tensor iteratively. MPCA has been applied to face recognition, gait recognition, etc. MPCA is further extended to uncorrelated MPCA, non-negative MPCA and robust MPCA.
N-way principal component analysis may be performed with models such as Tucker decomposition, PARAFAC, multiple factor analysis, co-inertia analysis, STATIS, and DISTATIS.
While PCA finds the mathematically optimal method (as in minimizing the squared error), it is still sensitive to outliers in the data that produce large errors, something that the method tries to avoid in the first place. It is therefore common practice to remove outliers before computing PCA. However, in some contexts, outliers can be difficult to identify.
For example, in data mining algorithms like correlation clustering, the assignment of points to clusters and outliers is not known beforehand.
A recently proposed generalization of PCA based on a weighted PCA increases robustness by assigning different weights to data objects based on their estimated relevancy.
Outlier-resistant variants of PCA have also been proposed, based on L1-norm formulations (L1-PCA).
Robust principal component analysis (RPCA) via decomposition in low-rank and sparse matrices is a modification of PCA that works well with respect to grossly corrupted observations.
Independent component analysis (ICA) is directed to similar problems as principal component analysis, but finds additively separable components rather than successive approximations.
Given a matrix
, it tries to decompose it into two matrices such that
. A key difference from techniques such as PCA and ICA is that some of the entries of
are constrained to be 0. Here
is termed the regulatory layer. While in general such a decomposition can have multiple solutions, they prove that if the following conditions are satisfied :
has full column rank
Each column of
must have at least
is the number of columns of
(or alternatively the number of rows of
). The justification for this criterion is that if a node is removed from the regulatory layer along with all the output nodes connected to it, the result must still be characterized by a connectivity matrix with full column rank.
must have full row rank.
then the decomposition is unique up to multiplication by a scalar.
Discriminant analysis of principal components (DAPC) is a multivariate method used to identify and describe clusters of genetically related individuals. Genetic variation is partitioned into two components: variation between groups and within groups, and it maximizes the former. Linear discriminants are linear combinations of alleles which best separate the clusters. Alleles that most contribute to this discrimination are therefore those that are the most markedly different across groups. The contributions of alleles to the groupings identified by DAPC can allow identifying regions of the genome driving the genetic divergence among groups
In DAPC, data is first transformed using a principal components analysis (PCA) and subsequently clusters are identified using discriminant analysis (DA).
A DAPC can be realized on R using the package Adegenet. (more info: adegenet on the web)
Directional component analysis (DCA) is a method used in the atmospheric sciences for analysing multivariate datasets.
Like PCA, it allows for dimension reduction, improved visualization and improved interpretability of large data-sets.
Also like PCA, it is based on a covariance matrix derived from the input dataset.
The difference between PCA and DCA is that DCA additionally requires the input of a vector direction, referred to as the impact.
Whereas PCA maximises explained variance, DCA maximises probability density given impact.
The motivation for DCA is to find components of a multivariate dataset that are both likely (measured using probability density) and important (measured using the impact).
DCA has been used to find the most likely and most serious heat-wave patterns in weather prediction ensembles
, and the most likely and most impactful changes in rainfall due to climate change
ALGLIB – a C++ and C# library that implements PCA and truncated PCA
Analytica – The built-in EigenDecomp function computes principal components.
ELKI – includes PCA for projection, including robust variants of PCA, as well as PCA-based clustering algorithms.
Gretl – principal component analysis can be performed either via the pca command or via the princomp() function.
Julia – Supports PCA with the pca function in the MultivariateStats package
KNIME – A java based nodal arranging software for Analysis, in this the nodes called PCA, PCA compute, PCA Apply, PCA inverse make it easily.
Maple (software) – The PCA command is used to perform a principal component analysis on a set of data.
Mathematica – Implements principal component analysis with the PrincipalComponents command using both covariance and correlation methods.
MathPHP – PHP mathematics library with support for PCA.
MATLAB – The SVD function is part of the basic system. In the Statistics Toolbox, the functions princomp and pca (R2012b) give the principal components, while the function pcares gives the residuals and reconstructed matrix for a low-rank PCA approximation.
Matplotlib – Python library have a PCA package in the .mlab module.
mlpack – Provides an implementation of principal component analysis in C++.
mrmath – A high performance math library for Delphi and FreePascal can perform PCA; including robust variants.
NAG Library – Principal components analysis is implemented via the g03aa routine (available in both the Fortran versions of the Library).
NMath – Proprietary numerical library containing PCA for the .NET Framework.
GNU Octave – Free software computational environment mostly compatible with MATLAB, the function princomp gives the principal component.
Oracle Database 12c – Implemented via DBMS_DATA_MINING.SVDS_SCORING_MODE by specifying setting value SVDS_SCORING_PCA
Orange (software) – Integrates PCA in its visual programming environment. PCA displays a scree plot (degree of explained variance) where user can interactively select the number of principal components.
Origin – Contains PCA in its Pro version.
Qlucore – Commercial software for analyzing multivariate data with instant response using PCA.
R – Free statistical package, the functions princomp and prcomp can be used for principal component analysis; prcomp uses singular value decomposition which generally gives better numerical accuracy. Some packages that implement PCA in R, include, but are not limited to: ade4, vegan, ExPosition, dimRed, and FactoMineR.
SAS – Proprietary software; for example, see
scikit-learn – Python library for machine learning which contains PCA, Probabilistic PCA, Kernel PCA, Sparse PCA and other techniques in the decomposition module.
Scilab – Free and open-source, cross-platform numerical computational package, the function princomp computes principal component analysis, the function pca computes principal component analysis with standardized variables.
SPSS – Proprietary software most commonly used by social scientists for PCA, factor analysis and associated cluster analysis.
Weka – Java library for machine learning which contains modules for computing principal components.
Jackson, J.E. (1991). A User's Guide to Principal Components (Wiley).
Jolliffe, I. T. (1986). Principal Component Analysis. Springer Series in Statistics. Springer-Verlag. pp. 487. CiteSeerX 10.1.1.149.8828. doi:10.1007/b98835. ISBN 978-0-387-95442-4.
Jolliffe, I. T. (2002). Principal Component Analysis. Springer Series in Statistics. New York: Springer-Verlag. doi:10.1007/b98835. ISBN 978-0-387-95442-4.
Husson François, Lê Sébastien & Pagès Jérôme (2009). Exploratory Multivariate Analysis by Example Using R. Chapman & Hall/CRC The R Series, London. 224p. ISBN 978-2-7535-0938-2
Pagès Jérôme (2014). Multiple Factor Analysis by Example Using R. Chapman & Hall/CRC The R Series London 272 p
University of Copenhagen video by Rasmus Bro on YouTube
Stanford University video by Andrew Ng on YouTube
A Tutorial on Principal Component Analysis
A layman's introduction to principal component analysis on YouTube (a video of less than 100 seconds.)
StatQuest: StatQuest: Principal Component Analysis (PCA), Step-by-Step on YouTube
Layman's explanation in making sense of principal component analysis, eigenvectors & eigenvalues on Stack Overflow
See also the list of Software implementations Functional principal component analysis (FPCA) is a statistical method for investigating the dominant modes of variation of functional data. Using this method, a random function is represented in the eigenbasis, which is an orthonormal basis of the Hilbert space L2 that consists of the eigenfunctions of the autocovariance operator. FPCA represents functional data in the most parsimonious way, in the sense that when using a fixed number of basis functions, the eigenfunction basis explains more variation than any other basis expansion. FPCA can be applied for representing random functions, or in functional regression and classification.
For a square-integrable stochastic process X(t), t ∈ 𝒯, let
(X(t))
(X(s),X(t))=\sum _^\lambda _\varphi _(s)\varphi _(t),
\geq \lambda _\geq ...\geq 0
are the eigenvalues and


, ... are the orthonormal eigenfunctions of the linear Hilbert–Schmidt operator
()\rightarrow L^(),\,G(f)=\int _G(s,t)f(s)ds.
By the Karhunen–Loève theorem, one can express the centered process in the eigenbasis,
^\xi _\varphi _(t),
=\int _(X(t)-\mu (t))\varphi _(t)dt
is the principal component associated with the k-th eigenfunction

, with the properties
(\xi _)=0,(\xi _)=\lambda _(\xi _\xi _)=0k\neq l.
The centered process is then equivalent to ξ1, ξ2, .... A common assumption is that X can be represented by only the first few eigenfunctions (after subtracting the mean function), i.e.
(t)=\mu (t)+\sum _^\xi _\varphi _(t),
 \left(\int _\left(X(t)-X_(t)\right)^dt\right)=\sum _\lambda _\rightarrow 0m\rightarrow \infty .
The first eigenfunction

depicts the dominant mode of variation of X.
= \Vert =1 \left\ \left(\int _(X(t)-\mu (t))\varphi (t)dt\right)\right\,
 \Vert =\left(\int _\varphi (t)^dt\right)^.
The k-th eigenfunction

is the dominant mode of variation orthogonal to


, ... ,

= \Vert =1,\langle \varphi ,\varphi _\rangle =0j=1,\dots ,k-1 \left\ \left(\int _(X(t)-\mu (t))\varphi (t)dt\right)\right\,
\rangle =\int _\varphi (t)\varphi _(t)dt,j=1,\dots ,k-1.
Let Yij = Xi(tij) + εij be the observations made at locations (usually time points) tij, where Xi is the i-th realization of the smooth stochastic process that generates the data, and εij are identically and independently distributed normal random variable with mean 0 and variance σ2, j = 1, 2, ..., mi. To obtain an estimate of the mean function μ(tij), if a dense sample on a regular grid is available, one may take the average at each location tij:
(t_)=\sum _^Y_.
If the observations are sparse, one needs to smooth the data pooled from all observations to obtain the mean estimate, using smoothing methods like local linear smoothing or spline smoothing.
Then the estimate of the covariance function
(s,t)
is obtained by averaging (in the dense case) or smoothing (in the sparse case) the raw covariances
(t_,t_)=(Y_-(t_))(Y_-(t_)),j\neq l,i=1,\dots ,n.
Note that the diagonal elements of Gi should be removed because they contain measurement error.
(s,t)
is discretized to an equal-spaced dense grid, and the estimation of eigenvalues λk and eigenvectors vk is carried out by numerical linear algebra. The eigenfunction estimates
_
can then be obtained by interpolating the eigenvectors
.
The fitted covariance should be positive definite and symmetric and is then obtained as
(s,t)=\sum _>0__(s)_(t).
(t)
be a smoothed version of the diagonal elements Gi(tij, tij) of the raw covariance matrices. Then
(t)
is an estimate of (G(t, t) + σ2). An estimate of σ2 is obtained by
^=|\int _((t)-(t,t))dt,
^>0;
^=0.
If the observations Xij, j=1, 2, ..., mi are dense in 𝒯, then the k-th FPC ξk can be estimated by numerical integration, implementing
_=\langle X-,_\rangle .
However, if the observations are sparse, this method will not work. Instead, one can use best linear unbiased predictors, yielding
_=__^_^(Y_-),
_=+^\mathbf  _

is evaluated at the grid points generated by tij, j = 1, 2, ..., mi. The algorithm, PACE, has an available Matlab package and R package
Asymptotic convergence properties of these estimates have been investigated.
FPCA can be applied for displaying the modes of functional variation, in scatterplots of FPCs against each other or of responses against FPCs, for modeling sparse longitudinal data, or for functional regression and classification (e.g., functional linear regression). Scree plots and other methods can be used to determine the number of components included. Functional Principal component analysis has varied applications in time series analysis. At present, this method is being adapted from traditional multivariate techniques to analyze financial data sets such as stock market indices and generate implied volatility graphs. A good example of advantages of the functional approach is the Smoothed FPCA (SPCA), developed by Silverman [1996] and studied by Pezzulli and Silverman [1993], that enables direct combination of FPCA along with a general smoothing approach that makes using the information stored in some linear differential operators possible. An important application of the FPCA already known from multivariate PCA is motivated by the Karhunen-Loève decomposition of a random function to the set of functional parameters – factor functions and corresponding factor loadings (scalar random variables). This application is much more important than in the standard multivariate PCA since the distribution of the random function is in general too complex to be directly analyzed and the Karhunen-Loève decomposition reduces the analysis to the interpretation of the factor functions and the distribution of scalar random variables. Due to dimensionality reduction as well as its accuracy to represent data, there is a wide scope for further developments of functional principal component techniques in the financial field.
Applications of PCA in automotive engineering.
The following table shows a comparison of various elements of principal component analysis (PCA) and FPCA. The two methods are both used for dimensionality reduction. In implementations, FPCA uses a PCA step.
However, PCA and FPCA differ in some critical aspects. First, the order of multivariate data in PCA can be permuted, which has no effect on the analysis, but the order of functional data carries time or space information and cannot be reordered. Second, the spacing of observations in FPCA matters, while there is no spacing issue in PCA. Third, regular PCA does not work for high-dimensional data without regularization, while FPCA has a built-in regularization due to the smoothness of the functional data and the truncation to a finite number of included components.
Principal component analysis
James O. Ramsay; B. W. Silverman (8 June 2005). Functional Data Analysis. Springer. ISBN 978-0-387-40080-8. In the field of multivariate statistics, kernel principal component analysis (kernel PCA)
is an extension of principal component analysis (PCA) using techniques of kernel methods. Using a kernel, the originally linear operations of PCA are performed in a reproducing kernel Hilbert space.
Recall that conventional PCA operates on zero-centered data; that is,
\sum _^\mathbf  _=\mathbf  
 _
is one of the
It operates by diagonalizing the covariance matrix,
\sum _^\mathbf  _\mathbf  _^
in other words, it gives an eigendecomposition of the covariance matrix:
 =C\mathbf  
which can be rewritten as
 _^\mathbf  =\mathbf  _^C\mathbf  \quad ~i=1,\ldots ,N
(See also: Covariance matrix as a linear operator)
To understand the utility of kernel PCA, particularly for clustering, observe that, while N points cannot, in general, be linearly separated in
dimensions, they can almost always be linearly separated in

dimensions. That is, given N points,
 _
, if we map them to an N-dimensional space with
 _)
 ^\to \mathbb  ^
it is easy to construct a hyperplane that divides the points into arbitrary clusters. Of course, this

creates linearly independent vectors, so there is no covariance on which to perform eigendecomposition explicitly as we would in linear PCA.
Instead, in kernel PCA, a non-trivial, arbitrary

function is 'chosen' that is never calculated explicitly, allowing the possibility to use very-high-dimensional

's if we never have to actually evaluate the data in that space. Since we generally try to avoid working in the

-space, which we will call the 'feature space', we can create the N-by-N kernel
 ,\mathbf  )=(\Phi (\mathbf  ),\Phi (\mathbf  ))=\Phi (\mathbf  )^\Phi (\mathbf  )
which represents the inner product space (see Gramian matrix) of the otherwise intractable feature space. The dual form that arises in the creation of a kernel allows us to mathematically formulate a version of PCA in which we never actually solve the eigenvectors and eigenvalues of the covariance matrix in the
 )
-space (see Kernel trick). The N-elements in each column of K represent the dot product of one point of the transformed data with respect to all the transformed points (N points). Some well-known kernels are shown in the example below.
Because we are never working directly in the feature space, the kernel-formulation of PCA is restricted in that it computes not the principal components themselves, but the projections of our data onto those components. To evaluate the projection from a point in the feature space
 )
onto the kth principal component
(where superscript k means the component k, not powers of k)
^\Phi (\mathbf  )=\left(\sum _^\mathbf  _^\Phi (\mathbf  _)\right)^\Phi (\mathbf  )
We note that
 _)^\Phi (\mathbf  )
denotes dot product, which is simply the elements of the kernel
. It seems all that's left is to calculate and normalize the
 _^
, which can be done by solving the eigenvector equation
 =K\mathbf  
is the number of data points in the set, and

 
are the eigenvalues and eigenvectors of
. Then to normalize the eigenvectors
 ^
, we require that
Care must be taken regarding the fact that, whether or not
has zero-mean in its original space, it is not guaranteed to be centered in the feature space (which we never compute explicitly). Since centered data is required to perform an effective principal component analysis, we 'centralize'
 K-K\mathbf  +\mathbf  K\mathbf  
 
denotes a N-by-N matrix for which each element takes value
. We use
to perform the kernel PCA algorithm described above.
One caveat of kernel PCA should be illustrated here. In linear PCA, we can use the eigenvalues to rank the eigenvectors based on how much of the variation of the data is captured by each principal component. This is useful for data dimensionality reduction and it could also be applied to KPCA. However, in practice there are cases that all variations of the data are same. This is typically caused by a wrong choice of kernel scale.
In practice, a large data set leads to a large K, and storing K may become a problem. One way to deal with this is to perform clustering on the dataset, and populate the kernel with the means of those clusters. Since even this method may yield a relatively large K, it is common to compute only the top P eigenvalues and eigenvectors of the eigenvalues are calculated in this way.
Consider three concentric clouds of points (shown); we wish to use kernel PCA to identify these groups. The color of the points does not represent information involved in the algorithm, but only shows how the transformation relocates the data points.
First, consider the kernel
,)=(^ +1)^
Applying this to kernel PCA yields the next image.
Now consider a Gaussian kernel:
,)=e^-||^,
That is, this kernel is a measure of closeness, equal to 1 when the points coincide and equal to 0 at infinity.
Note in particular that the first principal component is enough to distinguish the three different groups, which is impossible using only linear PCA, because linear PCA operates only in the given (in this case two-dimensional) space, in which these concentric point clouds are not linearly separable.
Kernel PCA has been demonstrated to be useful for novelty detection and image de-noising.
Nonlinear dimensionality reduction Robust Principal Component Analysis (RPCA) is a modification of the widely used statistical procedure of principal component analysis (PCA) which works well with respect to grossly corrupted observations. A number of different approaches exist for Robust PCA, including an idealized version of Robust PCA, which aims to recover a low-rank matrix L0 from highly corrupted measurements M = L0 +S0. This decomposition in low-rank and sparse matrices can be achieved by techniques such as Principal Component Pursuit method (PCP), Stable PCP, Quantized PCP, Block based PCP, and Local PCP. Then, optimization methods are used such as the Augmented Lagrange Multiplier Method (ALM), Alternating Direction Method (ADM), Fast Alternating Minimization (FAM), Iteratively Reweighted Least Squares (IRLS )
or alternating projections (AP).
The 2014 guaranteed algorithm for the robust PCA problem (with the input matrix being
) is an alternating minimization type algorithm. The computational complexity is
\log \right)
where the input is the superposition of a low-rank (of rank
) and a sparse matrix of dimension


is the desired accuracy of the recovered solution, i.e.,
-L\|_\leq \epsilon 
is the true low-rank component and

is the estimated or recovered low-rank component. Intuitively, this algorithm performs projections of the residual onto the set of low-rank matrices (via the SVD operation) and sparse matrices (via entry-wise hard thresholding) in an alternating manner - that is, low-rank projection of the difference the input matrix and the sparse matrix obtained at a given iteration followed by sparse projection of the difference of the input matrix and the low-rank matrix obtained in the previous step, and iterating the two steps until convergence.
This alternating projections algorithm is later improved by an accelerated version, coined AccAltProj. The acceleration is achieved by applying a tangent space projection before projecting the residue onto the set of low-rank matrices. This trick improves the computational complexity to
\right)
with a much smaller constant in front while it maintains the theoretically guaranteed linear convergence.
Another fast version of accelerated alternating projections algorithm is IRCUR. It uses the structure of CUR decomposition in alternating projections framework to dramatically reduces the computational complexity of RPCA to
r^\log(m)\log(n)\log \right)
This method consists of relaxing the rank constraint
in the optimization problem to the nuclear norm
and the sparsity constraint

. The resulting program can be solved using methods such as the method of Augmented Lagrange Multipliers.
Some recent works propose RPCA algorithms with learnable/training parameters. Such a learnable/trainable algorithm can be unfolded as a deep neural network whose parameters can be learned via machine learning techniques from a given dataset or problem distribution. The learned algorithm will have superior performance on the corresponding problem distribution.
RPCA has many real life important applications particularly when the data under study can naturally be modeled as a low-rank plus a sparse contribution. Following examples are inspired by contemporary challenges in computer science, and depending on the applications, either the low-rank component or the sparse component could be the object of interest:
Given a sequence of surveillance video frames, it is often required to identify the activities that stand out from the background. If we stack the video frames as columns of a matrix M, then the low-rank component L0 naturally corresponds to the stationary background and the sparse component S0 captures the moving objects in the foreground.
Images of a convex, Lambertian surface under varying illuminations span a low-dimensional subspace. This is one of the reasons for effectiveness of low-dimensional models for imagery data. In particular, it is easy to approximate images of a human's face by a low-dimensional subspace. To be able to correctly retrieve this subspace is crucial in many applications such as face recognition and alignment. It turns out that RPCA can be applied successfully to this problem to exactly recover the face.
L1-norm principal component analysis
Decomposition into Low-rank plus Additive Matrices
T. Bouwmans, N. Aybat, and E. Zahzah. Handbook on Robust Low-Rank and Sparse Matrix Decomposition: Applications in Image and Video Processing, CRC Press, Taylor and Francis Group, May 2016. (more information: http://www.crcpress.com/product/isbn/9781498724623)
Z. Lin, H. Zhang, "Low-Rank Models in Visual Analysis: Theories, Algorithms, and Applications", Academic Press, Elsevier, June 2017. (more information: https://www.elsevier.com/books/low-rank-models-in-visual-analysis/lin/978-0-12-812731-5)
N. Vaswani, Y. Chi, T. Bouwmans, Special Issue on “Rethinking PCA for Modern Datasets: Theory, Algorithms, and Applications”, Proceedings of the IEEE, 2018.
T. Bouwmans, N. Vaswani, P. Rodriguez, R. Vidal, Z. Lin, Special Issue on “Robust Subspace Learning and Tracking: Theory, Algorithms, and Applications”, IEEE Journal of Selected Topics in Signal Processing, December 2018.
RSL-CV 2015: Workshop on Robust Subspace Learning and Computer Vision in conjunction with ICCV 2015 (For more information: http://rsl-cv2015.univ-lr.fr/workshop/)
RSL-CV 2017: Workshop on Robust Subspace Learning and Computer Vision in conjunction with ICCV 2017 (For more information: http://rsl-cv.univ-lr.fr/2017/)
RSL-CV 2021: Workshop on Robust Subspace Learning and Computer Vision in conjunction with ICCV 2021 (For more information: https://rsl-cv.univ-lr.fr/2021/)
Special Session on "Online Algorithms for Static and Dynamic Robust PCA and Compressive Sensing" in conjunction with SSP 2018. (More information: https://ssp2018.org/)
Documentation from the University of Illinois - Archive Link
The LRS Library (developed by Andrews Sobral) provides a collection of low-rank and sparse decomposition algorithms in MATLAB. The library was designed for moving object detection in videos, but it can be also used for other computer vision / machine learning tasks. Currently the LRSLibrary offers more than 100 algorithms based on matrix and tensor methods. L1-norm principal component analysis (L1-PCA) is a general method for multivariate data analysis.
L1-PCA is often preferred over standard L2-norm principal component analysis (PCA) when the analyzed data may contain outliers (faulty values or corruptions), as it is believed to be robust.
Both L1-PCA and standard PCA seek a collection of orthogonal directions (principal components) that define a subspace wherein data representation is maximized according to the selected criterion.
Standard PCA quantifies data representation as the aggregate of the L2-norm of the data point projections into the subspace, or equivalently the aggregate Euclidean distance of the original points from their subspace-projected representations.
L1-PCA uses instead the aggregate of the L1-norm of the data point projections into the subspace. In PCA and L1-PCA, the number of principal components (PCs) is lower than the rank of the analyzed matrix, which coincides with the dimensionality of the space defined by the original data points.
Therefore, PCA or L1-PCA are commonly employed for dimensionality reduction for the purpose of data denoising or compression.
Among the advantages of standard PCA that contributed to its high popularity are low-cost computational implementation by means of singular-value decomposition (SVD) and statistical optimality when the data set is generated by a true multivariate normal data source.
However, in modern big data sets, data often include corrupted, faulty points, commonly referred to as outliers.
Standard PCA is known to be sensitive to outliers, even when they appear as a small fraction of the processed data.
The reason is that the L2-norm formulation of L2-PCA places squared emphasis on the magnitude of each coordinate of each data point, ultimately overemphasizing peripheral points, such as outliers.
On the other hand, following an L1-norm formulation, L1-PCA places linear emphasis on the coordinates of each data point, effectively restraining outliers.
Consider any matrix
 =[\mathbf  _,\mathbf  _,\ldots ,\mathbf  _]\in \mathbb  ^
-dimensional data points. Define
 )
. For integer

, L1-PCA is formulated as:
, (1) simplifies to finding the L1-norm principal component (L1-PC) of
 
In (1)-(2), L1-norm

returns the sum of the absolute entries of its argument and L2-norm

returns the sum of the squared entries of its argument. If one substitutes

in (2) by the Frobenius/L2-norm

, then the problem becomes standard PCA and it is solved by the matrix
 
that contains the
dominant singular vectors of
 
(i.e., the singular vectors that correspond to the
highest singular values).
The maximization metric in (2) can be expanded as
For any matrix
 \in \mathbb  ^

 )
as the nearest (in the L2-norm sense) matrix to
 
that has orthonormal columns. That is, define
Procrustes Theorem states that if
 
 __\mathbf  _^
 )=\mathbf  \mathbf  ^
Markopoulos, Karystinos, and Pados showed that, if
 _
is the exact solution to the binary nuclear-norm maximization (BNM) problem
is the exact solution to L1-PCA in (2). The nuclear-norm

in (2) returns the summation of the singular values of its matrix argument and can be calculated by means of standard SVD. Moreover, it holds that, given the solution to L1-PCA,
 _
, the solution to BNM can be obtained as
(\cdot )

-sign matrix of its matrix argument (with no loss of generality, we can consider
). In addition, it follows that
 ^\mathbf  _\|_=\|\mathbf  \mathbf  _\|_
. BNM in (5) is a combinatorial problem over antipodal binary variables. Therefore, its exact solution can be found through exhaustive evaluation of all
elements of its feasibility set, with asymptotic cost
(2^)
. Therefore, L1-PCA can also be solved, through BNM, with cost
(2^)
(exponential in the product of the number of data points with the number of the sought-after components). It turns out that L1-PCA can be solved optimally (exactly) with polynomial complexity in
for fixed data dimension
(N^)
For the special case of
(single L1-PC of
 
), BNM takes the binary-quadratic-maximization (BQM) form
The transition from (5) to (8) for
holds true, since the unique singular value of
 \mathbf  
is equal to
 \mathbf  \|_= ^\mathbf  ^\mathbf  \mathbf  
, for every
 
. Then, if
 _
is the solution to BQM in (7), it holds that
is the exact L1-PC of
 
, as defined in (1). In addition, it holds that
 _=(\mathbf  ^\mathbf  _)
 ^\mathbf  _\|_=\|\mathbf  \mathbf  _\|_
As shown above, the exact solution to L1-PCA can be obtained by the following two-step process:
1. Solve the problem in (5) to obtain
 _
2. Apply SVD on
 \mathbf  _
 _
BNM in (5) can be solved by exhaustive search over the domain of
 
(2^)
Also, L1-PCA can be solved optimally with cost
(N^)
 )
is constant with respect to
(always true for finite data dimension
In 2008, Kwak proposed an iterative algorithm for the approximate solution of L1-PCA for
. This iterative method was later generalized for
components. Another approximate efficient solver was proposed by McCoy and Tropp by means of semi-definite programming (SDP). Most recently, L1-PCA (and BNM in (5)) were solved efficiently by means of bit-flipping iterations (L1-BF algorithm).
1 function L1BF(
 
 ^\in \^
\leftarrow \

 \mathbf  ^\|_
4 Until termination (or
 \leftarrow \mathbf  ^


\rceil 

 ]_\leftarrow -[\mathbf  ]_
// flip bit
 \mathbf  \|_
// calculated by SVD or faster (see)

 ^\leftarrow \mathbf  


// no bit was flipped
=\
\leftarrow \
The computational cost of L1-BF is
(NDmin\+N^K^(K^+r))
L1-PCA has also been generalized to process complex data. For complex L1-PCA, two efficient algorithms were proposed in 2018.
L1-PCA has also been extended for the analysis of tensor data, in the form of L1-Tucker, the L1-norm robust analogous of standard Tucker decomposition. Two algorithms for the solution of L1-Tucker are L1-HOSVD and L1-HOOI.
MATLAB code for L1-PCA is available at MathWorks.  Feature engineering is a preprocessing step in supervised machine learning and statistical modeling which transforms raw data into a more effective set of inputs. Each input comprises several attributes, known as features. By providing models with relevant information, feature engineering significantly enhances their predictive accuracy and decision-making capability.
Beyond machine learning, the principles of feature engineering are applied in various scientific fields, including physics. For example, physicists construct dimensionless numbers such as the Reynolds number in fluid dynamics, the Nusselt number in heat transfer, and the Archimedes number in sedimentation. They also develop first approximations of solutions, such as analytical solutions for the strength of materials in mechanics.
One of the applications of feature engineering has been clustering of feature-objects or sample-objects in a dataset. Especially, feature engineering based on matrix decomposition has been extensively used for data clustering under non-negativity constraints on the feature coefficients. These include Non-Negative Matrix Factorization (NMF), Non-Negative Matrix-Tri Factorization (NMTF), Non-Negative Tensor Decomposition/Factorization (NTF/NTD), etc. The non-negativity constraints on coefficients of the feature vectors mined by the above-stated algorithms yields a part-based representation, and different factor matrices exhibit natural clustering properties. Several extensions of the above-stated feature engineering methods have been reported in literature, including orthogonality-constrained factorization for hard clustering, and manifold learning to overcome inherent issues with these algorithms.
Other classes of feature engineering algorithms include leveraging a common hidden structure across multiple inter-related datasets to obtain a consensus (common) clustering scheme. An example is Multi-view Classification based on Consensus Matrix Decomposition (MCMD), which mines a common clustering scheme across multiple datasets. MCMD is designed to output two types of class labels (scale-variant and scale-invariant clustering), and:
is computationally robust to missing information,
can obtain shape- and scale-based outliers,
and can handle high-dimensional data effectively.
Coupled matrix and tensor decompositions are popular in multi-view feature engineering.
Feature engineering in machine learning and statistical modeling involves selecting, creating, transforming, and extracting data features. Key components include feature creation from existing data, transforming and imputing missing or invalid features, reducing data dimensionality through methods like Principal Components Analysis (PCA), Independent Component Analysis (ICA), and Linear Discriminant Analysis (LDA), and selecting the most relevant features for model training based on importance scores and correlation matrices.
Features vary in significance. Even relatively insignificant features may contribute to a model. Feature selection can reduce the number of features to prevent a model from becoming too specific to the training data set (overfitting).
Feature explosion occurs when the number of identified features is too large for effective model estimation or optimization. Common causes include:
Feature templates - implementing feature templates instead of coding new features
Feature combinations - combinations that cannot be represented by a linear system
Feature explosion can be limited via techniques such as: regularization, kernel methods, and feature selection.
Automation of feature engineering is a research topic that dates back to the 1990s. Machine learning software that incorporates automated feature engineering has been commercially available since 2016. Related academic literature can be roughly separated into two types:
Multi-relational decision tree learning (MRDTL) uses a supervised algorithm that is similar to a decision tree.
Deep Feature Synthesis uses simpler methods.
Multi-relational Decision Tree Learning (MRDTL) extends traditional decision tree methods to relational databases, handling complex data relationships across tables. It innovatively uses selection graphs as decision nodes, refined systematically until a specific termination criterion is reached.
Most MRDTL studies base implementations on relational databases, which results in many redundant operations. These redundancies can be reduced by using techniques such as tuple id propagation.
There are a number of open-source libraries and tools that automate feature engineering on relational data and time series:
featuretools is a Python library for transforming time series and relational data into feature matrices for machine learning.
MCMD: An open-source feature engineering algorithm for joint clustering of multiple datasets .
OneBM or One-Button Machine combines feature transformations and feature selection on relational data with feature selection techniques. [OneBM] helps data scientists reduce data exploration time allowing them to try and error many ideas in short time. On the other hand, it enables non-experts, who are not familiar with data science, to quickly extract value from their data with a little effort, time, and cost.
getML community is an open source tool for automated feature engineering on time series and relational data. It is implemented in C/C++ with a Python interface. It has been shown to be at least 60 times faster than tsflex, tsfresh, tsfel, featuretools or kats.
tsfresh is a Python library for feature extraction on time series data. It evaluates the quality of the features using hypothesis testing.
tsflex is an open source Python library for extracting features from time series data. Despite being 100% written in Python, it has been shown to be faster and more memory efficient than tsfresh, seglearn or tsfel.
seglearn is an extension for multivariate, sequential time series data to the scikit-learn Python library.
tsfel is a Python package for feature extraction on time series data.
kats is a Python toolkit for analyzing time series data.
The deep feature synthesis (DFS) algorithm beat 615 of 906 human teams in a competition.
The feature store is where the features are stored and organized for the explicit purpose of being used to either train models (by data scientists) or make predictions (by applications that have a trained model). It is a central location where you can either create or update groups of features created from multiple different data sources, or create and update new datasets from those feature groups for training models or for use in applications that do not want to compute the features but just retrieve them when it needs them to make predictions.
A feature store includes the ability to store code used to generate features, apply the code to raw data, and serve those features to models upon request. Useful capabilities include feature versioning and policies governing the circumstances under which features can be used.
Feature stores can be standalone software tools or built into machine learning platforms.
Feature engineering can be a time-consuming and error-prone process, as it requires domain expertise and often involves trial and error. Deep learning algorithms may be used to process a large raw dataset without having to resort to feature engineering. However, deep learning algorithms still require careful preprocessing and cleaning of the input data. In addition, choosing the right architecture, hyperparameters, and optimization algorithm for a deep neural network can be a challenging and iterative process.
Instrumental variables estimation
List of datasets for machine learning research
Scale co-occurrence matrix In machine learning, feature selection is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for several reasons:
simplification of models to make them easier to interpret,
shorter training times,
to avoid the curse of dimensionality,
improve the compatibility of the data with a certain learning model class,
to encode inherent symmetries present in the input space.
The central premise when using feature selection is that data sometimes contains features that are redundant or irrelevant, and can thus be removed without incurring much loss of information. Redundancy and irrelevance are two distinct notions, since one relevant feature may be redundant in the presence of another relevant feature with which it is strongly correlated.
Feature extraction creates new features from functions of the original features, whereas feature selection finds a subset of the features. Feature selection techniques are often used in domains where there are many features and comparatively few samples (data points).
A feature selection algorithm can be seen as the combination of a search technique for proposing new feature subsets, along with an evaluation measure which scores the different feature subsets. The simplest algorithm is to test each possible subset of features finding the one which minimizes the error rate. This is an exhaustive search of the space, and is computationally intractable for all but the smallest of feature sets. The choice of evaluation metric heavily influences the algorithm, and it is these evaluation metrics which distinguish between the three main categories of feature selection algorithms: wrappers, filters and embedded methods.
Wrapper methods use a predictive model to score feature subsets. Each new subset is used to train a model, which is tested on a hold-out set. Counting the number of mistakes made on that hold-out set (the error rate of the model) gives the score for that subset. As wrapper methods train a new model for each subset, they are very computationally intensive, but usually provide the best performing feature set for that particular type of model or typical problem.
Filter methods use a proxy measure instead of the error rate to score a feature subset. This measure is chosen to be fast to compute, while still capturing the usefulness of the feature set. Common measures include the mutual information, the pointwise mutual information, Pearson product-moment correlation coefficient, Relief-based algorithms, and inter/intra class distance or the scores of significance tests for each class/feature combinations. Filters are usually less computationally intensive than wrappers, but they produce a feature set which is not tuned to a specific type of predictive model. This lack of tuning means a feature set from a filter is more general than the set from a wrapper, usually giving lower prediction performance than a wrapper. However the feature set doesn't contain the assumptions of a prediction model, and so is more useful for exposing the relationships between the features. Many filters provide a feature ranking rather than an explicit best feature subset, and the cut off point in the ranking is chosen via cross-validation. Filter methods have also been used as a preprocessing step for wrapper methods, allowing a wrapper to be used on larger problems. One other popular approach is the Recursive Feature Elimination algorithm, commonly used with Support Vector Machines to repeatedly construct a model and remove features with low weights.
Embedded methods are a catch-all group of techniques which perform feature selection as part of the model construction process. The exemplar of this approach is the LASSO method for constructing a linear model, which penalizes the regression coefficients with an L1 penalty, shrinking many of them to zero. Any features which have non-zero regression coefficients are 'selected' by the LASSO algorithm. Improvements to the LASSO include Bolasso which bootstraps samples; Elastic net regularization, which combines the L1 penalty of LASSO with the L2 penalty of ridge regression; and FeaLect which scores all the features based on combinatorial analysis of regression coefficients. AEFS further extends LASSO to nonlinear scenario with autoencoders. These approaches tend to be between filters and wrappers in terms of computational complexity.
In traditional regression analysis, the most popular form of feature selection is stepwise regression, which is a wrapper technique. It is a greedy algorithm that adds the best feature (or deletes the worst feature) at each round. The main control issue is deciding when to stop the algorithm. In machine learning, this is typically done by cross-validation. In statistics, some criteria are optimized. This leads to the inherent problem of nesting. More robust methods have been explored, such as branch and bound and piecewise linear network.
Subset selection evaluates a subset of features as a group for suitability. Subset selection algorithms can be broken up into wrappers, filters, and embedded methods. Wrappers use a search algorithm to search through the space of possible features and evaluate each subset by running a model on the subset. Wrappers can be computationally expensive and have a risk of over fitting to the model. Filters are similar to wrappers in the search approach, but instead of evaluating against a model, a simpler filter is evaluated. Embedded techniques are embedded in, and specific to, a model.
Many popular search approaches use greedy hill climbing, which iteratively evaluates a candidate subset of features, then modifies the subset and evaluates if the new subset is an improvement over the old. Evaluation of the subsets requires a scoring metric that grades a subset of features. Exhaustive search is generally impractical, so at some implementor (or operator) defined stopping point, the subset of features with the highest score discovered up to that point is selected as the satisfactory feature subset. The stopping criterion varies by algorithm; possible criteria include: a subset score exceeds a threshold, a program's maximum allowed run time has been surpassed, etc.
Alternative search-based techniques are based on targeted projection pursuit which finds low-dimensional projections of the data that score highly: the features that have the largest projections in the lower-dimensional space are then selected.
Search approaches include:
Greedy forward selection
Greedy backward elimination
Particle swarm optimization
Targeted projection pursuit
Variable neighborhood search
Two popular filter metrics for classification problems are correlation and mutual information, although neither are true metrics or 'distance measures' in the mathematical sense, since they fail to obey the triangle inequality and thus do not compute any actual 'distance' – they should rather be regarded as 'scores'. These scores are computed between a candidate feature (or set of features) and the desired output category. There are, however, true metrics that are a simple function of the mutual information; see here.
Other available filter metrics include:
Consistency-based feature selection
Correlation-based feature selection
The choice of optimality criteria is difficult as there are multiple objectives in a feature selection task. Many common criteria incorporate a measure of accuracy, penalised by the number of features selected. Examples include Akaike information criterion (AIC) and Mallows's Cp, which have a penalty of 2 for each added feature. AIC is based on information theory, and is effectively derived via the maximum entropy principle.
Other criteria are Bayesian information criterion (BIC), which uses a penalty of

for each added feature, minimum description length (MDL) which asymptotically uses

, Bonferroni / RIC which use

, maximum dependency feature selection, and a variety of new criteria that are motivated by false discovery rate (FDR), which use something close to

. A maximum entropy rate criterion may also be used to select the most relevant subset of features.
Filter feature selection is a specific case of a more general paradigm called structure learning. Feature selection finds the relevant feature set for a specific target variable whereas structure learning finds the relationships between all the variables, usually by expressing these relationships as a graph. The most common structure learning algorithms assume the data is generated by a Bayesian Network, and so the structure is a directed graphical model. The optimal solution to the filter feature selection problem is the Markov blanket of the target node, and in a Bayesian Network, there is a unique Markov Blanket for each node.
There are different Feature Selection mechanisms around that utilize mutual information for scoring the different features. They usually use all the same algorithm:
Calculate the mutual information as score for between all features (
\in F
) and the target class (c)
Select the feature with the largest score (e.g.
\in F (I(f_,c))
) and add it to the set of selected features (S)
Calculate the score which might be derived from the mutual information
Select the feature with the largest score and add it to the set of select features (e.g.
\in F (I_(f_,c))
Repeat 3. and 4. until a certain number of features is selected (e.g.
The simplest approach uses the mutual information as the "derived" score.
However, there are different approaches, that try to reduce the redundancy between features.
Peng et al. proposed a feature selection method that can use either mutual information, correlation, or distance/similarity scores to select features. The aim is to penalise a feature's relevancy by its redundancy in the presence of the other selected features. The relevance of a feature set S for the class c is defined by the average value of all mutual information values between the individual feature fi and the class c as follows:
\sum _\in SI(f_;c)
The redundancy of all features in the set S is the average value of all mutual information values between the feature fi and the feature fj:
\sum _,f_\in SI(f_;f_)
The mRMR criterion is a combination of two measures given above and is defined as follows:
 =\max _\left[\sum _\in SI(f_;c)-\sum _,f_\in SI(f_;f_)\right].
Suppose that there are n full-set features. Let xi be the set membership indicator function for feature fi, so that xi=1 indicates presence and xi=0 indicates absence of the feature fi in the globally optimal feature set. Let
. The above may then be written as an optimization problem:
 =\max _^\left[^c_x_^x_-^a_x_x_^x_)^\right].
The mRMR algorithm is an approximation of the theoretically optimal maximum-dependency feature selection algorithm that maximizes the mutual information between the joint distribution of the selected features and the classification variable. As mRMR approximates the combinatorial estimation problem with a series of much smaller problems, each of which only involves two variables, it thus uses pairwise joint probabilities which are more robust. In certain situations the algorithm may underestimate the usefulness of features as it has no way to measure interactions between features which can increase relevancy. This can lead to poor performance when the features are individually useless, but are useful when combined (a pathological case is found when the class is a parity function of the features). Overall the algorithm is more efficient (in terms of the amount of data required) than the theoretically optimal max-dependency selection, yet produces a feature set with little pairwise redundancy.
mRMR is an instance of a large class of filter methods which trade off between relevancy and redundancy in different ways.
mRMR is a typical example of an incremental greedy strategy for feature selection: once a feature has been selected, it cannot be deselected at a later stage. While mRMR could be optimized using floating search to reduce some features, it might also be reformulated as a global quadratic programming optimization problem as follows:
 :\min _ \left\ ^H\mathbf  -\mathbf  ^F\right\\quad \ \sum _^x_=1,x_\geq 0
=[I(f_;c),\ldots ,I(f_;c)]^
is the vector of feature relevancy assuming there are n features in total,
=[I(f_;f_)]_
is the matrix of feature pairwise redundancy, and
 _
represents relative feature weights. QPFS is solved via quadratic programming. It is recently shown that QFPS is biased towards features with smaller entropy, due to its placement of the feature self redundancy term
on the diagonal of H.
Another score derived for the mutual information is based on the conditional relevancy:
 :\max _ \left\ ^Q\mathbf  \right\\quad \ \|\mathbf  \|=1,x_\geq 0
=(I(f_;c|f_)+I(f_;c|f_))/2,i\neq j
An advantage of SPECCMI is that it can be solved simply via finding the dominant eigenvector of Q, thus is very scalable. SPECCMI also handles second-order feature interaction.
In a study of different scores Brown et al. recommended the joint mutual information as a good score for feature selection. The score tries to find the feature, that adds the most new information to the already selected features, in order to avoid redundancy. The score is formulated as follows:
JMI(f_)&=\sum _\in S(I(f_;c)+I(f_;c|f_))\\&=\sum _\in SI(f_;c)+I(f_;c)-I(f_;f_)-I(f_;f_|c)\end
The score uses the conditional mutual information and the mutual information to estimate the redundancy between the already selected features (
\in S
) and the feature under investigation (
For high-dimensional and small sample data (e.g., dimensionality > 105 and the number of samples < 103), the Hilbert-Schmidt Independence Criterion Lasso (HSIC Lasso) is useful. HSIC Lasso optimization problem is given as
 :\min _ \sum _^x_x_(f_,f_)-\sum _^x_(f_,c)+\lambda \|\mathbf  \|_,\quad \ x_,\ldots ,x_\geq 0,
(f_,c)=( ^ )
is a kernel-based independence measure called the (empirical) Hilbert-Schmidt independence criterion (HSIC),
(\cdot )
denotes the trace,

is the regularization parameter,
 ^=\mathbf  \mathbf  ^\mathbf  
 =\mathbf  \mathbf  \mathbf  
are input and output centered Gram matrices,
are Gram matrices,
are kernel functions,
 =\mathbf  _-\mathbf  _\mathbf  _^
is the centering matrix,
 _
is the m-dimensional identity matrix (m: the number of samples),
 _
is the m-dimensional vector with all ones, and


-norm. HSIC always takes a non-negative value, and is zero if and only if two random variables are statistically independent when a universal reproducing kernel such as the Gaussian kernel is used.
The HSIC Lasso can be written as
 :\min _ \left\| -\sum _^x_ ^\right\|_^+\lambda \|\mathbf  \|_,\quad \ x_,\ldots ,x_\geq 0,

is the Frobenius norm. The optimization problem is a Lasso problem, and thus it can be efficiently solved with a state-of-the-art Lasso solver such as the dual augmented Lagrangian method.
The correlation feature selection (CFS) measure evaluates subsets of features on the basis of the following hypothesis: "Good feature subsets contain features highly correlated with the classification, yet uncorrelated to each other". The following equation gives the merit of a feature subset S consisting of k features:
 _=.

is the average value of all feature-classification correlations, and

is the average value of all feature-feature correlations. The CFS criterion is defined as follows:
 =\max _\left[+r_+\cdots +r_f_+\cdots +r_f_+\cdots +r_f_)\right].
variables are referred to as correlations, but are not necessarily Pearson's correlation coefficient or Spearman's ρ. Hall's dissertation uses neither of these, but uses three different measures of relatedness, minimum description length (MDL), symmetrical uncertainty, and relief.
Let xi be the set membership indicator function for feature fi; then the above can be rewritten as an optimization problem:
 =\max _^\left[^a_x_)^^x_+\sum _2b_x_x_\right].
The combinatorial problems above are, in fact, mixed 0–1 linear programming problems that can be solved by using branch-and-bound algorithms.
The features from a decision tree or a tree ensemble are shown to be redundant. A recent method called regularized tree can be used for feature subset selection. Regularized trees penalize using a variable similar to the variables selected at previous tree nodes for splitting the current node. Regularized trees only need build one tree model (or one tree ensemble model) and thus are computationally efficient.
Regularized trees naturally handle numerical and categorical features, interactions and nonlinearities. They are invariant to attribute scales (units) and insensitive to outliers, and thus, require little data preprocessing such as normalization. Regularized random forest (RRF) is one type of regularized trees. The guided RRF is an enhanced RRF which is guided by the importance scores from an ordinary random forest.
A metaheuristic is a general description of an algorithm dedicated to solve difficult (typically NP-hard problem) optimization problems for which there is no classical solving methods. Generally, a metaheuristic is a stochastic algorithm tending to reach a global optimum. There are many metaheuristics, from a simple local search to a complex global search algorithm.
The feature selection methods are typically presented in three classes based on how they combine the selection algorithm and the model building.
Filter type methods select variables regardless of the model. They are based only on general features like the correlation with the variable to predict. Filter methods suppress the least interesting variables. The other variables will be part of a classification or a regression model used to classify or to predict data. These methods are particularly effective in computation time and robust to overfitting.
Filter methods tend to select redundant variables when they do not consider the relationships between variables. However, more elaborate features try to minimize this problem by removing variables highly correlated to each other, such as the Fast Correlation Based Filter (FCBF) algorithm.
Wrapper methods evaluate subsets of variables which allows, unlike filter approaches, to detect the possible interactions amongst variables. The two main disadvantages of these methods are:
The increasing overfitting risk when the number of observations is insufficient.
The significant computation time when the number of variables is large.
Embedded methods have been recently proposed that try to combine the advantages of both previous methods. A learning algorithm takes advantage of its own variable selection process and performs feature selection and classification simultaneously, such as the FRMT algorithm.
This is a survey of the application of feature selection metaheuristics lately used in the literature. This survey was realized by J. Hammon in her 2013 thesis.
Some learning algorithms perform feature selection as part of their overall operation. These include:
⁠-regularization techniques, such as sparse regression, LASSO, and ⁠
Regularized trees, e.g. regularized random forest implemented in the RRF package
Random multinomial logit (RMNL)
Auto-encoding networks with a bottleneck-layer
Submodular feature selection
Local learning based feature selection. Compared with traditional methods, it does not involve any heuristic search, can easily handle multi-class problems, and works for both linear and nonlinear problems. It is also supported by a strong theoretical foundation. Numeric experiments showed that the method can achieve a close-to-optimal solution even when data contains >1M irrelevant features.
Recommender system based on feature selection. The feature selection methods are introduced into recommender system research.
Relief (feature selection)
Guyon, Isabelle; Elisseeff, Andre (2003). "An Introduction to Variable and Feature Selection". Journal of Machine Learning Research. 3: 1157–1182.
Harrell, F. (2001). Regression Modeling Strategies. Springer. ISBN 0-387-95232-2.
Liu, Huan; Motoda, Hiroshi (1998). Feature Selection for Knowledge Discovery and Data Mining. Springer. ISBN 0-7923-8198-X.
Liu, Huan; Yu, Lei (2005). "Toward Integrating Feature Selection Algorithms for Classification and Clustering". IEEE Transactions on Knowledge and Data Engineering. 17 (4): 491–502. doi:10.1109/TKDE.2005.66. S2CID 1607600.
Feature Selection Package, Arizona State University (Matlab Code)
NIPS challenge 2003 (see also NIPS)
Naive Bayes implementation with feature selection in Visual Basic Archived 2009-02-14 at the Wayback Machine (includes executable and source code)
Minimum-redundancy-maximum-relevance (mRMR) feature selection program
FEAST (Open source Feature Selection algorithms in C and MATLAB) Minimum redundancy feature selection is an algorithm frequently used in a method to accurately identify characteristics of genes and phenotypes and narrow down their relevance and is usually described in its pairing with relevant feature selection as Minimum Redundancy Maximum Relevance (mRMR). This method was first proposed in 2003 by Hanchuan Peng and Chris Ding, followed by a theoretical formulation based on mutual information, along with the first definition of multivariate mutual information, published in IEEE Trans. Pattern Analysis and Machine Intelligence in 2005.
Feature selection, one of the basic problems in pattern recognition and machine learning, identifies subsets of data that are relevant to the parameters used and is normally called Maximum Relevance. These subsets often contain material which is relevant but redundant and mRMR attempts to address this problem by removing those redundant subsets. mRMR has a variety of applications in many areas such as cancer diagnosis and speech recognition.
Features can be selected in many different ways. One scheme is to select features that correlate strongest to the classification variable. This has been called maximum-relevance selection. Many heuristic algorithms can be used, such as the sequential forward, backward, or floating selections.
On the other hand, features can be selected to be mutually far away from each other while still having "high" correlation to the classification variable. This scheme, termed as Minimum Redundancy Maximum Relevance (mRMR) selection has been found to be more powerful than the maximum relevance selection.
As a special case, the "correlation" can be replaced by the statistical dependency between variables. Mutual information can be used to quantify the dependency. In this case, it is shown that mRMR is an approximation to maximizing the dependency between the joint distribution of the selected features and the classification variable.
Studies have tried different measures for redundancy and relevance measures. A recent study compared several measures within the context of biomedical images.
Peng, H.C., Long, F., and Ding, C., "Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy," IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 27, No. 8, pp. 1226–1238, 2005.
Chris Ding and Hanchuan Peng, "Minimum Redundancy Feature Selection from Microarray Gene Expression Data". 2nd IEEE Computer Society Bioinformatics Conference (CSB 2003), 11–14 August 2003, Stanford, CA, USA. Pages 523–529. Relief is an algorithm developed by Kira and Rendell in 1992 that takes a filter-method approach to feature selection that is notably sensitive to feature interactions. It was originally designed for application to binary classification problems with discrete or numerical features. Relief calculates a feature score for each feature which can then be applied to rank and select top scoring features for feature selection. Alternatively, these scores may be applied as feature weights to guide downstream modeling. Relief feature scoring is based on the identification of feature value differences between nearest neighbor instance pairs. If a feature value difference is observed in a neighboring instance pair with the same class (a 'hit'), the feature score decreases. Alternatively, if a feature value difference is observed in a neighboring instance pair with different class values (a 'miss'), the feature score increases. The original Relief algorithm has since inspired a family of Relief-based feature selection algorithms (RBAs), including the ReliefF algorithm. Beyond the original Relief algorithm, RBAs have been adapted to (1) perform more reliably in noisy problems, (2) generalize to multi-class problems (3) generalize to numerical outcome (i.e. regression) problems, and (4) to make them robust to incomplete (i.e. missing) data.
To date, the development of RBA variants and extensions has focused on four areas; (1) improving performance of the 'core' Relief algorithm, i.e. examining strategies for neighbor selection and instance weighting, (2) improving scalability of the 'core' Relief algorithm to larger feature spaces through iterative approaches, (3) methods for flexibly adapting Relief to different data types, and (4) improving Relief run efficiency.
Their strengths are that they are not dependent on heuristics, they run in low-order polynomial time, and they are noise-tolerant and robust to feature interactions, as well as being applicable for binary or continuous data; however, it does not discriminate between redundant features, and low numbers of training instances fool the algorithm.
Take a data set with n instances of p features, belonging to two known classes. Within the data set, each feature should be scaled to the interval [0 1] (binary data should remain as 0 and 1). The algorithm will be repeated m times. Start with a p-long weight vector (W) of zeros.
At each iteration, take the feature vector (X) belonging to one random instance, and the feature vectors of the instance closest to X (by Euclidean distance) from each class. The closest same-class instance is called 'near-hit', and the closest different-class instance is called 'near-miss'. Update the weight vector such that
=W_-(x_-\mathrm  _)^+(x_-\mathrm  _)^,
indexes the components and runs from 1 to p.
Thus the weight of any given feature decreases if it differs from that feature in nearby instances of the same class more than nearby instances of the other class, and increases in the reverse case.
After m iterations, divide each element of the weight vector by m. This becomes the relevance vector. Features are selected if their relevance is greater than a threshold τ.
Kira and Rendell's experiments showed a clear contrast between relevant and irrelevant features, allowing τ to be determined by inspection. However, it can also be determined by Chebyshev's inequality for a given confidence level (α) that a τ of 1/sqrt(α*m) is good enough to make the probability of a Type I error less than α, although it is stated that τ can be much smaller than that.
Relief was also described as generalizable to multinomial classification by decomposition into a number of binary problems.
Kononenko et al. propose a number of updates to Relief. Firstly, they find the near-hit and near-miss instances using the Manhattan (L1) norm rather than the Euclidean (L2) norm, although the rationale is not specified. Furthermore, they found taking the absolute differences between xi and near-hiti, and xi and near-missi to be sufficient when updating the weight vector (rather than the square of those differences).
Rather than repeating the algorithm m times, implement it exhaustively (i.e. n times, once for each instance) for relatively small n (up to one thousand). Furthermore, rather than finding the single nearest hit and single nearest miss, which may cause redundant and noisy attributes to affect the selection of the nearest neighbors, ReliefF searches for k nearest hits and misses and averages their contribution to the weights of each feature. k can be tuned for any individual problem.
In ReliefF, the contribution of missing values to the feature weight is determined using the conditional probability that two values should be the same or different, approximated with relative frequencies from the data set. This can be calculated if one or both features are missing.
Rather than use Kira and Rendell's proposed decomposition of a multinomial classification into a number of binomial problems, ReliefF searches for k near misses from each different class and averages their contributions for updating W, weighted with the prior probability of each class.
The following RBAs are arranged chronologically from oldest to most recent. They include methods for improving (1) the core Relief algorithm concept, (2) iterative approaches for scalability, (3) adaptations to different data types, (4) strategies for computational efficiency, or (5) some combination of these goals. For more on RBAs see these book chapters or this most recent review paper.
Robnik-Šikonja and Kononenko propose further updates to ReliefF, making it appropriate for regression.
Introduced deterministic neighbor selection approach and a new approach for incomplete data handling.
Implemented method to address bias against non-monotonic features. Introduced the first iterative Relief approach. For the first time, neighbors were uniquely determined by a radius threshold and instances were weighted by their distance from the target instance.
Introduced sigmoidal weighting based on distance from target instance. All instance pairs (not just a defined subset of neighbors) contributed to score updates. Proposed an on-line learning variant of Relief. Extended the iterative Relief concept. Introduced local-learning updates between iterations for improved convergence.
Specifically sought to address noise in large feature spaces through the recursive elimination of features and the iterative application of ReliefF.
Similarly seeking to address noise in large feature spaces. Utilized an iterative `evaporative' removal of lowest quality features using ReliefF scores in association with mutual information.
Addressing issues related to incomplete and multi-class data.
Dramatically improves the efficiency of detecting 2-way feature interactions in very large feature spaces by scoring random feature subsets rather than the entire feature space.
Introduced calculation of feature weights relative to average feature 'diff' between instance pairs.
SURF identifies nearest neighbors (both hits and misses) based on a distance threshold from the target instance defined by the average distance between all pairs of instances in the training data. Results suggest improved power to detect 2-way epistatic interactions over ReliefF.
SURF* extends the SURF algorithm to not only utilized 'near' neighbors in scoring updates, but 'far' instances as well, but employing inverted scoring updates for 'far instance pairs. Results suggest improved power to detect 2-way epistatic interactions over SURF, but an inability to detect simple main effects (i.e. univariate associations).
SWRF* extends the SURF* algorithm adopting sigmoid weighting to take distance from the threshold into account. Also introduced a modular framework for further developing RBAs called MoRF.
MultiSURF* extends the SURF* algorithm adapting the near/far neighborhood boundaries based on the average and standard deviation of distances from the target instance to all others. MultiSURF* uses the standard deviation to define a dead-band zone where 'middle-distance' instances do not contribute to scoring. Evidence suggests MultiSURF* performs best in detecting pure 2-way feature interactions.
Introduces a feature-wise adaptive k parameter for more flexibly detecting univariate effects and interaction effects.
MultiSURF simplifies the MultiSURF* algorithm by preserving the dead-band zone, and target-instance-centric neighborhood determination, but eliminating the 'far' scoring. Evidence suggests MultiSURF to be a well rounded option, able to detect 2-way and 3-way interactions, as well as simple univariate associations. Also introduced the RBA software package called ReBATE that includes implementations of (Relief, ReliefF, SURF, SURF*, MultiSURF*, MultiSURF, and TuRF).
STIR reformulates and slightly adjusts the original Relief formula by incorporating sample variance of the nearest neighbor distances into the attribute importance estimation. This variance permits the calculation of statistical significance of features and adjustment for multiple testing of Relief-based scores. Currently, STIR supports binary outcome variable but will soon be extended to multi-state and continuous outcome.
Different RBAs have been applied to feature selection in a variety of problem domains. Feature Selection Toolbox (FST) is software primarily for feature selection in the machine learning domain, written in C++, developed at the Institute of Information Theory and Automation (UTIA), of the Czech Academy of Sciences.
The first generation of Feature Selection Toolbox (FST1) was a Windows application with user interface allowing users to apply several sub-optimal, optimal and mixture-based feature selection methods on data stored in a trivial proprietary textual flat file format.
The third generation of Feature Selection Toolbox (FST3) was a library without user interface, written to be more efficient and versatile than the original FST1.
FST3 supports several standard data mining tasks, more specifically, data preprocessing and classification, but its main focus is on feature selection. In feature selection context, it implements several common as well as less usual techniques, with particular emphasis put on threaded implementation of various sequential search methods (a form of hill-climbing). Implemented methods include individual feature ranking, floating search, oscillating search (suitable for very high-dimension problems) in randomized or deterministic form, optimal methods of branch and bound type, probabilistic class distance criteria, various classifier accuracy estimators, feature subset size optimization, feature selection with pre-specified feature weights, criteria ensembles, hybrid methods, detection of all equivalent solutions, or two-criterion optimization. FST3 is more narrowly specialized than popular software like the Waikato Environment for Knowledge Analysis Weka, RapidMiner or PRTools.
By default, techniques implemented in the toolbox are predicated on the assumption that the data is available as a single flat file in a simple proprietary format or in Weka format ARFF, where each data point is described by a fixed number of numeric attributes. FST3 is provided without user interface, and is meant to be used by users familiar both with machine learning and C++ programming. The older FST1 software is more suitable for simple experimenting or educational purposes because it can be used with no need to code in C++.
In 1999, development of the first Feature Selection Toolbox version started at UTIA as part of a PhD thesis. It was originally developed in Optima++ (later renamed Power++) RAD C++ environment.
In 2002, the development of the first FST generation has been suspended, mainly due to end of Sybase's support of the then used development environment.
In 2002–2008, FST kernel was recoded and used for research experimentation within UTIA only.
In 2009, 3rd FST kernel recoding from scratch begun.
In 2010, FST3 was made publicly available in form of a C++ library without GUI. The accompanying webpage collects feature selection related links, references, documentation and the original FST1 available for download.
In 2011, an update of FST3 to version 3.1 included new methods (particularly a novel dependency-aware feature ranking suitable for very-high-dimension recognition problems) and core code improvements.
OpenNN, Open neural networks library for predictive analytics
Weka, comprehensive and popular Java open-source software from University of Waikato
RapidMiner, formerly Yet Another Learning Environment (YALE) a commercial machine learning framework
PRTools of the Delft University of Technology
Infosel++ specialized in information theory based feature selection
Tooldiag a C++ pattern recognition toolbox
List of numerical analysis software  Cross-validation, sometimes called rotation estimation or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set.
Cross-validation includes resampling and sample splitting methods that use different portions of the data to test and train a model on different iterations. It is often used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. It can also be used to assess the quality of a fitted model and the stability of its parameters.
In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set). The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).
One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model's predictive performance.
In summary, cross-validation combines (averages) measures of fitness in prediction to derive a more accurate estimate of model prediction performance.
Assume a model with one or more unknown parameters, and a data set to which the model can be fit (the training data set). The fitting process optimizes the model parameters to make the model fit the training data as well as possible. If an independent sample of validation data is taken from the same population as the training data, it will generally turn out that the model does not fit the validation data as well as it fits the training data. The size of this difference is likely to be large especially when the size of the training data set is small, or when the number of parameters in the model is large. Cross-validation is a way to estimate the size of this effect.
In linear regression, there exist real response values
,\ldots ,y_
, and n p-dimensional vector covariates x1, ..., xn. The components of the vector xi are denoted xi1, ..., xip. If least squares is used to fit a function in the form of a hyperplane ŷ = a + βTx to the data (xi, yi) 1 ≤ i ≤ n, then the fit can be assessed using the mean squared error (MSE). The MSE for given estimated parameter values a and β on the training set (xi, yi) 1 ≤ i ≤ n is defined as:
&=\sum _^(y_-_)^=\sum _^(y_-a-^\mathbf  _)^\\&=\sum _^(y_-a-\beta _x_-\dots -\beta _x_)^\end
If the model is correctly specified, it can be shown under mild assumptions that the expected value of the MSE for the training set is (n − p − 1)/(n + p + 1) < 1 times the expected value of the MSE for the validation set (the expected value is taken over the distribution of training sets). Thus, a fitted model and computed MSE on the training set will result in an optimistically biased assessment of how well the model will fit an independent data set. This biased estimate is called the in-sample estimate of the fit, whereas the cross-validation estimate is an out-of-sample estimate.
Since in linear regression it is possible to directly compute the factor (n − p − 1)/(n + p + 1) by which the training MSE underestimates the validation MSE under the assumption that the model specification is valid, cross-validation can be used for checking whether the model has been overfitted, in which case the MSE in the validation set will substantially exceed its anticipated value. (Cross-validation in the context of linear regression is also useful in that it can be used to select an optimally regularized cost function.)
In most other regression procedures (e.g. logistic regression), there is no simple formula to compute the expected out-of-sample fit. Cross-validation is, thus, a generally applicable way to predict the performance of a model on unavailable data using numerical computation in place of theoretical analysis.
Two types of cross-validation can be distinguished: exhaustive and non-exhaustive cross-validation.
Exhaustive cross-validation methods are cross-validation methods which learn and test on all possible ways to divide the original sample into a training and a validation set.
Leave-p-out cross-validation (LpO CV) involves using p observations as the validation set and the remaining observations as the training set. This is repeated on all ways to cut the original sample on a validation set of p observations and a training set.
LpO cross-validation require training and validating the model
times, where n is the number of observations in the original sample, and where
is the binomial coefficient. For p > 1 and for even moderately large n, LpO CV can become computationally infeasible. For example, with n = 100 and p = 30,
^\approx 3\times 10^.
A variant of LpO cross-validation with p=2 known as leave-pair-out cross-validation has been recommended as a nearly unbiased method for estimating the area under ROC curve of binary classifiers.
Leave-one-out cross-validation (LOOCV) is a particular case of leave-p-out cross-validation with p = 1. The process looks similar to jackknife; however, with cross-validation one computes a statistic on the left-out sample(s), while with jackknifing one computes a statistic from the kept samples only.
LOO cross-validation requires less computation time than LpO cross-validation because there are only
passes rather than
passes may still require quite a large computation time, in which case other approaches such as k-fold cross validation may be more appropriate.
x, 
y, 
interpolate( x_in, y_in, x_out ), 
err, 
err ← 0
for i ← 1, ..., N do
// define the cross-validation subsets
x_in ← (x[1], ..., x[i − 1], x[i + 1], ..., x[N])
y_in ← (y[1], ..., y[i − 1], y[i + 1], ..., y[N])
x_out ← x[i]
y_out ← interpolate(x_in, y_in, x_out)
err ← err + (y[i] − y_out)^2
err ← err/N
Non-exhaustive cross validation methods do not compute all ways of splitting the original sample. These methods are approximations of leave-p-out cross-validation.
In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples, often referred to as "folds". Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data. The k results can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling (see below) is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is commonly used, but in general k remains an unfixed parameter.
For example, setting k = 2 results in 2-fold cross-validation. In 2-fold cross-validation, we randomly shuffle the dataset into two sets d0 and d1, so that both sets are equal size (this is usually implemented by shuffling the data array and then splitting it in two). We then train on d0 and validate on d1, followed by training on d1 and validating on d0.
When k = n (the number of observations), k-fold cross-validation is equivalent to leave-one-out cross-validation.
In stratified k-fold cross-validation, the partitions are selected so that the mean response value is approximately equal in all the partitions. In the case of binary classification, this means that each partition contains roughly the same proportions of the two types of class labels.
In repeated cross-validation the data is randomly split into k partitions several times. The performance of the model can thereby be averaged over several runs, but this is rarely desirable in practice.
When many different statistical or machine learning models are being considered, greedy k-fold cross-validation can be used to quickly identify the most promising candidate models.
In the holdout method, we randomly assign data points to two sets d0 and d1, usually called the training set and the test set, respectively. The size of each of the sets is arbitrary although typically the test set is smaller than the training set. We then train (build a model) on d0 and test (evaluate its performance) on d1.
In typical cross-validation, results of multiple runs of model-testing are averaged together; in contrast, the holdout method, in isolation, involves a single run. It should be used with caution because without such averaging of multiple runs, one may achieve highly misleading results. One's indicator of predictive accuracy (F*) will tend to be unstable since it will not be smoothed out by multiple iterations (see below). Similarly, indicators of the specific role played by various predictor variables (e.g., values of regression coefficients) will tend to be unstable.
While the holdout method can be framed as "the simplest kind of cross-validation", many sources instead classify holdout as a type of simple validation, rather than a simple or degenerate form of cross-validation.
This method, also known as Monte Carlo cross-validation, creates multiple random splits of the dataset into training and validation data. For each such split, the model is fit to the training data, and predictive accuracy is assessed using the validation data. The results are then averaged over the splits. The advantage of this method (over k-fold cross validation) is that the proportion of the training/validation split is not dependent on the number of iterations (i.e., the number of partitions). The disadvantage of this method is that some observations may never be selected in the validation subsample, whereas others may be selected more than once. In other words, validation subsets may overlap. This method also exhibits Monte Carlo variation, meaning that the results will vary if the analysis is repeated with different random splits.
As the number of random splits approaches infinity, the result of repeated random sub-sampling validation tends towards that of leave-p-out cross-validation.
In a stratified variant of this approach, the random samples are generated in such a way that the mean response value (i.e. the dependent variable in the regression) is equal in the training and testing sets. This is particularly useful if the responses are dichotomous with an unbalanced representation of the two response values in the data.
A method that applies repeated random sub-sampling is RANSAC.
When cross-validation is used simultaneously for selection of the best set of hyperparameters and for error estimation (and assessment of generalization capacity), a nested cross-validation is required. Many variants exist. At least two variants can be distinguished:
This is a truly nested variant which contains an outer loop of k sets and an inner loop of l sets. The total data set is split into k sets. One by one, a set is selected as the (outer) test set and the k - 1 other sets are combined into the corresponding outer training set. This is repeated for each of the k sets. Each outer training set is further sub-divided into l sets. One by one, a set is selected as inner test (validation) set and the l - 1 other sets are combined into the corresponding inner training set. This is repeated for each of the l sets. The inner training sets are used to fit model parameters, while the outer test set is used as a validation set to provide an unbiased evaluation of the model fit. Typically, this is repeated for many different hyperparameters (or even different model types) and the validation set is used to determine the best hyperparameter set (and model type) for this inner training set. After this, a new model is fit on the entire outer training set, using the best set of hyperparameters from the inner cross-validation. The performance of this model is then evaluated using the outer test set.
This is a type of k*l-fold cross-validation when l = k - 1. A single k-fold cross-validation is used with both a validation and test set. The total data set is split into k sets. One by one, a set is selected as test set. Then, one by one, one of the remaining sets is used as a validation set and the other k - 2 sets are used as training sets until all possible combinations have been evaluated. Similar to the k*l-fold cross validation, the training set is used for model fitting and the validation set is used for model evaluation for each of the hyperparameter sets. Finally, for the selected parameter set, the test set is used to evaluate the model with the best parameter set. Here, two variants are possible: either evaluating the model that was trained on the training set or evaluating a new model that was fit on the combination of the training and the validation set.
The goal of cross-validation is to estimate the expected level of fit of a model to a data set that is independent of the data that were used to train the model. It can be used to estimate any quantitative measure of fit that is appropriate for the data and model. For example, for binary classification problems, each case in the validation set is either predicted correctly or incorrectly. In this situation the misclassification error rate can be used to summarize the fit, although other measures derived from information (e.g., counts, frequency) contained within a contingency table or confusion matrix could also be used. When the value being predicted is continuously distributed, the mean squared error, root mean squared error or median absolute deviation could be used to summarize the errors.
When users apply cross-validation to select a good configuration

, then they might want to balance the cross-validated choice with their own estimate of the configuration. In this way, they can attempt to counter the volatility of cross-validation when the sample size is small and include relevant information from previous research. In a forecasting combination exercise, for instance, cross-validation can be applied to estimate the weights that are assigned to each forecast. Since a simple equal-weighted forecast is difficult to beat, a penalty can be added for deviating from equal weights. Or, if cross-validation is applied to assign individual weights to observations, then one can penalize deviations from equal weights to avoid wasting potentially relevant information. Hoornweg (2018) shows how a tuning parameter

can be defined so that a user can intuitively balance between the accuracy of cross-validation and the simplicity of sticking to a reference parameter

that is defined by the user.

candidate configuration that might be selected, then the loss function that is to be minimized can be defined as
=(1-\gamma )_+\gamma _.
Relative accuracy can be quantified as
(\lambda _)/(\lambda _)
, so that the mean squared error of a candidate

is made relative to that of a user-specified

. The relative simplicity term measures the amount that


relative to the maximum amount of deviation from

. Accordingly, relative simplicity can be specified as
-\lambda _)^-\lambda _)^

corresponds to the

value with the highest permissible deviation from


, the user determines how high the influence of the reference parameter is relative to cross-validation.
One can add relative simplicity terms for multiple configurations
by specifying the loss function as
=_+\sum _^_.
Hoornweg (2018) shows that a loss function with such an accuracy-simplicity tradeoff can also be used to intuitively define shrinkage estimators like the (adaptive) lasso and Bayesian / ridge regression. Click on the lasso for an example.
Suppose we choose a measure of fit F, and use cross-validation to produce an estimate F* of the expected fit EF of a model to an independent data set drawn from the same population as the training data. If we imagine sampling multiple independent training sets following the same distribution, the resulting values for F* will vary. The statistical properties of F* result from this variation.
The variance of F* can be large. For this reason, if two statistical procedures are compared based on the results of cross-validation, the procedure with the better estimated performance may not actually be the better of the two procedures (i.e. it may not have the better value of EF). Some progress has been made on constructing confidence intervals around cross-validation estimates, but this is considered a difficult problem.
Most forms of cross-validation are straightforward to implement as long as an implementation of the prediction method being studied is available. In particular, the prediction method can be a "black box" – there is no need to have access to the internals of its implementation. If the prediction method is expensive to train, cross-validation can be very slow since the training must be carried out repeatedly. In some cases such as least squares and kernel regression, cross-validation can be sped up significantly by pre-computing certain values that are needed repeatedly in the training, or by using fast "updating rules" such as the Sherman–Morrison formula. However one must be careful to preserve the "total blinding" of the validation set from the training procedure, otherwise bias may result. An extreme example of accelerating cross-validation occurs in linear regression, where the results of cross-validation have a closed-form expression known as the prediction residual error sum of squares (PRESS).
Cross-validation only yields meaningful results if the validation set and training set are drawn from the same population and only if human biases are controlled.
In many applications of predictive modeling, the structure of the system being studied evolves over time (i.e. it is "non-stationary"). Both of these can introduce systematic differences between the training and validation sets. For example, if a model for prediction of trend changes in financial quotations is trained on data for a certain five-year period, it is unrealistic to treat the subsequent five-year period as a draw from the same population. As another example, suppose a model is developed to predict an individual's risk for being diagnosed with a particular disease within the next year. If the model is trained using data from a study involving only a specific population group (e.g. young people or males), but is then applied to the general population, the cross-validation results from the training set could differ greatly from the actual predictive performance.
In many applications, models also may be incorrectly specified and vary as a function of modeler biases and/or arbitrary choices. When this occurs, there may be an illusion that the system changes in external samples, whereas the reason is that the model has missed a critical predictor and/or included a confounded predictor. New evidence is that cross-validation by itself is not very predictive of external validity, whereas a form of experimental validation known as swap sampling that does control for human bias can be much more predictive of external validity. As defined by this large MAQC-II study across 30,000 models, swap sampling incorporates cross-validation in the sense that predictions are tested across independent training and validation samples. Yet, models are also developed across these independent samples and by modelers who are blinded to one another. When there is a mismatch in these models developed across these swapped training and validation samples as happens quite frequently, MAQC-II shows that this will be much more predictive of poor external predictive validity than traditional cross-validation.
The reason for the success of the swapped sampling is a built-in control for human biases in model building. In addition to placing too much faith in predictions that may vary across modelers and lead to poor external validity due to these confounding modeler effects, these are some other ways that cross-validation can be misused:
By performing an initial analysis to identify the most informative features using the entire data set – if feature selection or model tuning is required by the modeling procedure, this must be repeated on every training set. Otherwise, predictions will certainly be upwardly biased. If cross-validation is used to decide which features to use, an inner cross-validation to carry out the feature selection on every training set must be performed.
Performing mean-centering, rescaling, dimensionality reduction, outlier removal or any other data-dependent preprocessing using the entire data set. While very common in practice, this has been shown to introduce biases into the cross-validation estimates.
By allowing some of the training data to also be included in the test set – this can happen due to "twinning" in the data set, whereby some exactly identical or nearly identical samples are present in the data set, see pseudoreplication. To some extent twinning always takes place even in perfectly independent training and validation samples. This is because some of the training sample observations will have nearly identical values of predictors as validation sample observations. And some of these will correlate with a target at better than chance levels in the same direction in both training and validation when they are actually driven by confounded predictors with poor external validity. If such a cross-validated model is selected from a k-fold set, human confirmation bias will be at work and determine that such a model has been validated. This is why traditional cross-validation needs to be supplemented with controls for human bias and confounded model specification like swap sampling and prospective studies.
Due to correlations, cross-validation with random splits might be problematic for time-series models (if we are more interested in evaluating extrapolation, rather than interpolation). A more appropriate approach might be to use rolling cross-validation.
However, if performance is described by a single summary statistic, it is possible that the approach described by Politis and Romano as a stationary bootstrap will work. The statistic of the bootstrap needs to accept an interval of the time series and return the summary statistic on it. The call to the stationary bootstrap needs to specify an appropriate mean interval length.
Cross-validation can be used to compare the performances of different predictive modeling procedures. For example, suppose we are interested in optical character recognition, and we are considering using either a Support Vector Machine (SVM) or k-nearest neighbors (KNN) to predict the true character from an image of a handwritten character. Using cross-validation, we can obtain empirical estimates comparing these two methods in terms of their respective fractions of misclassified characters. In contrast, the in-sample estimate will not represent the quantity of interest (i.e. the generalization error).
Cross-validation can also be used in variable selection. Suppose we are using the expression levels of 20 proteins to predict whether a cancer patient will respond to a drug. A practical goal would be to determine which subset of the 20 features should be used to produce the best predictive model. For most modeling procedures, if we compare feature subsets using the in-sample error rates, the best performance will occur when all 20 features are used. However under cross-validation, the model with the best fit will generally include only a subset of the features that are deemed truly informative.
A recent development in medical statistics is its use in meta-analysis. It forms the basis of the validation statistic, Vn which is used to test the statistical validity of meta-analysis summary estimates. It has also been used in a more conventional sense in meta-analysis to estimate the likely prediction error of meta-analysis results.
Bengio, Yoshua; Grandvalet, Yves (2004). "No Unbiased Estimator of the Variance of K-Fold Cross-Validation" (PDF). Journal of Machine Learning Research. 5: 1089–1105.
Kim, Ji-Hyun (September 2009). "Estimating classification error rate: Repeated cross-validation, repeated hold-out and bootstrap". Computational Statistics & Data Analysis. 53 (11): 3735–3745. doi:10.1016/j.csda.2009.04.009.
Beleites, Claudia; Baumgartner, Richard; Bowman, Christopher; Somorjai, Ray; Steiner, Gerald; Salzer, Reiner; Sowa, Michael G. (October 2005). "Variance reduction in estimating classification error using sparse datasets". Chemometrics and Intelligent Laboratory Systems. 79 (1–2): 91–100. doi:10.1016/j.chemolab.2005.04.008.
Trippa, Lorenzo; Waldron, Levi; Huttenhower, Curtis; Parmigiani, Giovanni (March 2015). "Bayesian nonparametric cross-study validation of prediction methods". The Annals of Applied Statistics. 9 (1). arXiv:1506.00474. doi:10.1214/14-AOAS798. Purged cross-validation is a variant of k-fold cross-validation designed to prevent look-ahead bias in time series and other structured data, developed in 2017 by Marcos López de Prado at Guggenheim Partners and Cornell University. It is primarily used in financial machine learning to ensure the independence of training and testing samples when labels depend on future events. It provides an alternative to conventional cross-validation and walk-forward backtesting methods, which often yield overly optimistic performance estimates due to information leakage and overfitting.
Standard cross-validation assumes that observations are independently and identically distributed (IID), which often does not hold in time series or financial datasets. If the label of a test sample overlaps in time with the features or labels in the training set, the result may be data leakage and overfitting. Purged cross-validation addresses this issue by removing overlapping observations and, optionally, adding a temporal buffer ("embargo") around the test set to further reduce the risk of leakage.
The figure below illustrates standard 5 Fold Cross-Validation
Purging removes from the training set any observation whose timestamp falls within the time range of formation of a label in the test set. This can be the case for train set observations before and after the test set. Their removal ensures that the algorithm cannot learn during train time information that will be used to assess the performance of the algorithm. See the figure below for an illustration of purging.
Embargoing addresses a more subtle form of leakage: even if an observation does not directly overlap the test set, it may still be affected by test events due to market reaction lag or downstream dependencies. To guard against this, a percentage-based embargo is imposed after each test fold. For example, with a 5% embargo and 1000 observations, the 50 observations following each test fold are excluded from training.
Unlike purging, embargoing can only occur after the test set. The figure below illustrates the application of embargo:
Purged and embargoed cross-validation has been useful in:
Backtesting of trading strategies
Validation of classifiers on labeled event-driven returns
Any machine learning task with overlapping label horizons
To illustrate the effect of purging and embargoing, consider the figures below. Both diagrams show the structure of 5-fold cross-validation over a 20-day period. In each row, blue squares indicate training samples and red squares denote test samples. Each label is defined based on the value of the next two observations, hence creating an overlap. If this overlap is left untreated, test set information leaks into the train set.
The second figure applies the Purged CV procedure. Notice how purging removes overlapping observations from the training set and the embargo widens the gap between test and training data. This approach ensures that the evaluation more closely resembles a true out-of-sample test and reduces the risk of backtest overfitting.
Walk-forward backtesting analysis, another common cross-validation technique in finance, preserves temporal order but evaluates the model on a single sequence of test sets. This leads to high variance in performance estimation, as results are contingent on a specific historical path.
Combinatorial Purged Cross-Validation (CPCV) addresses this limitation by systematically constructing multiple train-test splits, purging overlapping samples, and enforcing an embargo period to prevent information leakage. The result is a distribution of out-of-sample performance estimates, enabling robust statistical inference and more realistic assessment of a model's predictive power.
CPCV divides a time-series dataset into N sequential, non-overlapping groups. These groups preserve the temporal order of observations. Then, all combinations of k groups (where k < N) are selected as test sets, with the remaining N − k groups used for training. For each combination, the model is trained and evaluated under strict controls to prevent leakage.
To eliminate potential contamination between training and test sets, CPCV introduces two additional mechanisms:
Purging: Any training observations whose label horizon overlaps with the test period are excluded. This ensures that future information does not influence model training.
Embargoing: After the end of each test period, a fixed number of observations (typically a small percentage) are removed from the training set. This prevents leakage due to delayed market reactions or auto-correlated features.
Each data point appears in multiple test sets across different combinations. Because test groups are drawn combinatorially, this process produces multiple backtest "paths," each of which simulates a plausible market scenario. From these paths, practitioners can compute a distribution of performance statistics such as the Sharpe ratio, drawdown, or classification accuracy.
Let N be the number of sequential groups into which the dataset is divided, and let k be the number of groups selected as the test set for each split. Then:
The number of unique train-test combinations is given by the binomial coefficient:

Each observation is used in
test sets and contributes to

unique backtest paths:

This yields a distribution of performance metrics rather than a single point estimate, making it possible to apply Monte Carlo-based or probabilistic techniques to assess model robustness.
Consider the case where N = 6 and k = 2. The number of possible test set combinations is
=15
. Each of the six groups appears in five test splits. Consequently, five distinct backtest paths can be constructed, each incorporating one appearance from every group.
This table shows the 15 test combinations. An "x" indicates that the corresponding group is included in the test set for that split.
Each group contributes to five different backtest paths. The number in each cell indicates the path to which the group's result is assigned for that split.
Combinatorial Purged Cross-Validation offers several key benefits over conventional methods:
It produces a distribution of performance metrics, enabling more rigorous statistical inference.
The method systematically eliminates lookahead bias through purging and embargoing.
By simulating multiple historical scenarios, it reduces the dependence on any single market regime or realization.
It supports high-confidence comparisons between competing models or strategies.
CPCV is commonly used in quantitative strategy research, especially for evaluating predictive models such as classifiers, regressors, and portfolio optimizers. It has been applied to estimate realistic Sharpe ratios, assess the risk of overfitting, and support the use of statistical tools such as the Deflated Sharpe Ratio (DSR).
The main limitation of CPCV stems from its high computational cost. However, this cost can be managed by sampling a finite number of splits from the space of all possible combinations.
Machine learning in finance   The Dropout is an American biographical drama television miniseries about the rise and fall of the disgraced biotechnology company Theranos and its founder, Elizabeth Holmes, played by Amanda Seyfried. Created by Elizabeth Meriwether, it is based on the ABC News podcast of the same name hosted by Rebecca Jarvis. The series features an ensemble supporting cast, including Naveen Andrews, Elizabeth Marvel, William H. Macy, Stephen Fry, Mary Lynn Rajskub, Bill Irwin, Utkarsh Ambudkar, LisaGay Hamilton, Michael Ironside, Laurie Metcalf, Anne Archer, and Sam Waterston. It is the first television project produced by Searchlight Television.
The Dropout premiered on the streaming service Hulu on March 3, 2022. It received positive reviews from critics, who praised the writing and the performances of the cast, particularly Seyfried. At the 74th Primetime Emmy Awards, the series earned six nominations, including Outstanding Limited or Anthology Series, with Seyfried winning for Outstanding Lead Actress. It was also nominated for Best Limited or Anthology Series or Television Film and Seyfried won Best Actress at the 80th Golden Globe Awards.
The Dropout, based on the ABC Audio podcast of the same name — is a series about the rise and fall of Elizabeth Holmes and her company, Theranos. The show touches on experiences that likely motivated Holmes's deceptions, starting from her preteens and all the way to her exposure as a fraud.
Amanda Seyfried as Elizabeth Holmes, a Stanford University dropout who goes on to found healthcare startup Theranos
Naveen Andrews as Sunny Balwani, Holmes' lover and Theranos' COO
Elizabeth Marvel as Noel Holmes, Elizabeth's mother
Michel Gill as Chris Holmes, Elizabeth's father, a former Enron employee
William H. Macy as Richard Fuisz, the Holmes' family acquaintance with a history of healthcare patents
Mary Lynn Rajskub as Lorraine Fuisz, Richard's wife who behaves deferentially towards Noel
Bill Irwin as Channing Robertson, Holmes' chemical engineering professor at Stanford who becomes Theranos' first board member
Utkarsh Ambudkar as Rakesh Madhava, a Theranos engineer and Holmes' former TA at Robertson's research group. Madhava is a fictional character.
Laurie Metcalf as Phyllis Gardner, a physician and pharmacologist who was one of Theranos' early skeptics
Stephen Fry as Ian Gibbons, Theranos' chief scientist
Kate Burton as Rochelle Gibbons, Ian's supportive wife
James Hiroyuki Liao as Edmond Ku, a Theranos engineer who becomes increasingly disgruntled with the company's practices
Michael Ironside as Don Lucas, a venture capitalist who invested in Oracle and joins Theranos' board
Hart Bochner as Larry Ellison, Oracle CEO
Nicky Endres as Ana Arriola, a former Apple engineer who briefly joins Theranos' design team
Amir Arison as Avie Tevanian, a Theranos board member who casts doubts about the company's progress
Bashir Salahuddin as Brendan Morris, an engineer hired by Holmes to develop an altogether different prototype of the Theranos machine. Morris is a fictional character, closest to real-life Theranos employee Tony Nugent.
Shaun J. Brown as Daniel Young, a Theranos vice president who enforces the company's culture of secrecy
Alan Ruck as Jay "Dr. Jay" Rosan, an eccentric Walgreens executive enamored with Silicon Valley
Josh Pais as Wade Miquelon, Walgreens CFO
Rich Sommer as Kevin Hunter, a Walgreens-employed lab consultant who remains skeptical of Theranos
Andrew Leeds as Roland, a Walgreens executive
Sam Waterston as George Shultz, former United States Secretary of State and Theranos board member
Anne Archer as Charlotte Shultz, wife of George Shultz
Dylan Minnette as Tyler Shultz, George Shultz's grandson who interns at Theranos before becoming a whistleblower
Kurtwood Smith as David Boies, a prominent litigator whom Holmes hires as her attorney
Michaela Watkins as Linda Tanner, Theranos' chief in-house counsel
Sam Straley as Christian Holmes, Elizabeth's brother who is hired as Theranos' Chief of Strategic Operations
Kevin Sussman as Mark Roessler, Theranos' lab director who helps break the story on its fraudulent practices
Camryn Mi-Young Kim as Erika Cheung, a young Theranos employee who becomes a company whistleblower alongside Tyler Shultz
Ebon Moss-Bachrach as John Carreyrou, a reporter for The Wall Street Journal who exposes Theranos' fraud (and who wrote the book Bad Blood about Holmes in real life).
LisaGay Hamilton as Judith Baker, Carreyrou's editor. The character is loosely based on Carreyrou's real-life editor at the Journal, Gerard Baker.
Garrett Coffey as Billy Evans, a hotel heir and Holmes' new boyfriend
On April 10, 2019, Deadline Hollywood reported that Hulu had given the production a series order for 6 to 10 episodes. The series would be executive produced by Kate McKinnon, the host of The Dropout, Rebecca Jarvis, and its producers Taylor Dunn and Victoria Thompson. The series would be Searchlight Television’s inaugural production. Upon the casting of Amanda Seyfried, she also joined the miniseries as a producer while Elizabeth Meriwether, Liz Heldens, Liz Hannah, and Katherine Pope joined Dunn and Thompson as executive producers. On March 31, 2021, Michael Showalter and Jordana Mollick joined the limited series as executive producers. Showalter directed the first four episodes of the series.
Kate McKinnon was originally cast to star as Elizabeth Holmes, former CEO of Theranos. On February 18, 2021, McKinnon dropped out of the project without an explanation. On March 29, 2021, Amanda Seyfried was cast to replace McKinnon. A day later, Naveen Andrews joined the main cast. On June 10, 2021, William H. Macy, Laurie Metcalf, Elizabeth Marvel, Utkarsh Ambudkar, Kate Burton, Stephen Fry, Michel Gill, Michael Ironside, Bill Irwin, and Josh Pais were cast in recurring roles. On August 3, 2021, Dylan Minnette, Alan Ruck, Bashir Salahuddin, Mary Lynn Rajskub, Hart Bochner, James Hiroyuki Liao, Nicky Endres, Camryn Mi-Young Kim, and Andrew Leeds were cast in recurring roles. On August 5, 2021, Sam Waterston, Kurtwood Smith and Anne Archer were cast in recurring roles. On September 14, 2021, LisaGay Hamilton, Michaela Watkins, Ebon Moss-Bachrach, Kevin Sussman, Sam Straley, and Shaun Brown joined the cast in recurring capacities.
The series premiered on March 3, 2022, with the first three episodes available immediately and the rest debuting on a weekly basis on Hulu. In international markets, it was released simultaneously via Star content hub on Disney+, on Star+ in Latín America, and on Disney+ Hotstar in India and Southeast Asia.
According to Parrot Analytics, which looks at consumer engagement in consumer research, streaming, downloads, and on social media, The Dropout was the ninth most in-demand new show, from March 12–18. Analytics company Samba TV, which gathers viewership data from certain smart TVs and content providers, revealed that 499,000 US households watched the series in its first 4 days of streaming. Nielsen Media Research, which records streaming viewership on U.S. television screens, calculated that The Dropout amassed 255 million minutes of watch time from February 28 to March 6. The series later garnered 190 million minutes of watch time from March 28 to April 3, according to Nielsen Media Research.
The streaming aggregator Reelgood, which tracks real-time data from 5 million U.S. users for original and acquired content across SVOD and AVOD services, announced that The Dropout was the sixth most-watched program during the week of March 23. It was later the most-watched television show during the week of March 31. JustWatch, a guide to streaming content with access to data from more than 20 million users around the world, stated that The Dropout was the ninth most-streamed television series in the U.S. during the week ending April 3.
On review aggregator website Rotten Tomatoes, the limited series holds a 90% approval rating, based on 98 critic reviews, with an average rating of 7.4/10. The website's critics consensus reads: "The Dropout succeeds more as a docudrama than a dark comedy, but Amanda Seyfried's disquieting portrayal of Elizabeth Holmes brings fresh blood to this retelling of recent history." On Metacritic, it has a score of 75 out of 100, based on 34 critics, indicating "generally favorable reviews".
Caroline Framke of Variety found the miniseries very impressive for its genuine portrait of Elizabeth Holmes, acclaimed the performances of the actors, especially Amanda Seyfried and Naveen Andrews, and wrote that it skillfully manages to cover Holmes' life throughout the years flawlessly with its edits. Daniel Fienberg of The Hollywood Reporter acclaimed Seyfried for her performance, while praising the supporting cast, complimented how the miniseries manages to depict Holmes and the different aspects of her personality, and found the characterization of Holmes' entourage its best feature.
Reviewing the miniseries for Rolling Stone, Alan Sepinwall gave a rating of 5 out of 5 stars and described it as "a maddening, gripping, and at times startlingly funny recreation of a story that would feel too absurd to be true if we didn't already know otherwise". Beth Webb of Empire rated the miniseries 4 out of 5 stars, praised the performances of Seyfried and Andrews, and found it cohesive and entrancing. Joyce Slaton of Common Sense Media rated the miniseries 4 out of 5 stars, praised the performances of the actors, and stated that it illustrates Holmes went too far to make a positive impact on the world. Lucy Mangan of The Guardian rated the miniseries 3 out of 5 stars, praised the performances of the actors and the story, writing: "It's clunky at points, but Amanda Seyfried excels as one-time billionaire grifter Elizabeth Holmes – and the story is simply too jaw-dropping to pass up."
List of podcast adaptations
The Dropout on Hulu
The Dropout at IMDb
The Dropout on ABC Audio Dropout is an American comedy subscription streaming service run by the production company of the same name (formerly CollegeHumor), founded in September 2018. Its ad-free original shows are mainly composed of live play, such as Dimension 20 hosted by Brennan Lee Mulligan, and improv comedy and panel shows like Game Changer, Make Some Noise (both hosted by Dropout owner and CEO Sam Reich), and Very Important People. Dropout's series often feature a rotating cast of regular comedians and performers.
Originally founded in 1999 by Josh Abramson and Ricky Van Veen, then-independent website CollegeHumor was acquired by holding media and entertainment company IAC in August 2006. CollegeHumor's work originally only included editorial articles, but eventually expanded to include online video and development and production of TV shows. In 2013, the video production CollegeHumor team moved to Los Angeles to continue to create online and traditional video consisting of shows like Adam Ruins Everything and Hot Date as well as sketch and short-form comedy on its YouTube channel. However, ad revenue became increasingly scarce, with YouTube's unfavorable ad rates and an ongoing risk of CollegeHumor's content being demonetized on the platform, as well as Facebook's inflated viewership numbers not bringing in anticipated ad sales.
In the mid-2010s, CollegeHumor syndicated its content to various companies, including Condé Nast Entertainment, DailyMotion, Samsung, Vessel, and Watchable. CollegeHumor subsidiary Big Breakfast signed a deal with Verizon's go90 streaming service in 2015, which included the show Fatal Decision and a commitment of ten video clips per month for the platform. During this time, Shane Rahmani served as CollegeHumor's general manager. Fatal Decision was ultimately released on Dropout's own platform in 2025.
After starting production in 2017, CollegeHumor launched its Dropout TV video platform on September 26, 2018. CollegeHumor's then-CEO, Rich Cusick, announced the service as a "TV-MA version of CollegeHumor" that would "allow us to double down our investment into premium original content, resulting in a bigger, better, badder CollegeHumor." Dropout was also pitched as a way to allow fans to dive deeper into pre-established popular characters and shows from CollegeHumor's YouTube channel. CollegeHumor's Chief Creative Officer, Sam Reich, also claimed that the founding of Dropout was in response to difficulty in receiving advertising dollars on traditional media platforms for mature content. At launch, Dropout announced a mix of scripted and unscripted content, as well as digital comics and chat-story content and a subscriber-only Discord.
Dropout utilizes CollegeHumor spinoff Vimeo as its hosting service. Dropout officially launched native iOS and Android apps for its service in December 2018, allowing users to watch shows and also cast to smart TVs. Comics and chat stories were also integrated into the app.
After launch, Dropout saw that the unscripted and less expensive content both acquired and retained viewers, while more expensive and scripted shows neither acquired nor retained viewers, with the limited exception of WTF 101. Dimension 20 became the most popular show on the platform, followed by Um, Actually and Cartoon Hell. Game Changer, hosted by Reich, would also become a success upon launch in 2019. A year after Dropout's launch, the service had between 75,000 and 100,000 subscribers. Dropout planned to release a new original title per month in 2019, according to Sam Reich. However, the programming slate did not allow Dropout to pivot quickly enough away from scripted content and it was still not profitable by the end of 2019; Reich later noted that "we had a writer's room full of scripted comedy writers. All of us were sort of having to pivot to think about something that wasn't our primary skill set." Because Dropout was in the middle of a 10 million by the end of 2019 before it would be profitable.
In January of 2020, IAC announced it had ceased financing CollegeHumor, leading to the layoff of 105 employees. IAC unsuccessfully attempted to sell CollegeHumor to numerous studios and entertainment companies at this time, reportedly seeking up to 3 million. IAC ultimately sold CollegeHumor to then-Chief Creative Officer Reich in 2020, who transitioned to CEO. IAC kept a minority stake in Dropout, having been convinced by Reich it had the potential to become worth more than Viacom's offer. The deal was finalized in March 2020. Dropout ended production on scripted shows and focused on unscripted shows such as Um, Actually, Dimension 20, and Game Changer, briefly producing these series through online conference during the beginning of the COVID-19 pandemic.
During the July-November 2023 SAG-AFTRA strike, Dropout series such as Dimension 20, Game Changer and Um, Actually were initially shut down. Reich stated:
Because we aren't associated with the AMPTP, it's possible we may be able to reach an interim agreement with SAG that allows us to continue to produce content during the strike. But we'll only do that, obviously, if we get the blessing of the union and the buy-in of our performers. If not, we have enough content in the can to last us a little past the end of the year. ... As for me, I intend to honor my union's position that I not promote SAG productions as a performer – even if they are produced by me. That means that I won't personally be promoting any of our shows for the time being.
In August 2023, Reich announced that all Dropout shows had resumed production as it was determined that their "New Media Agreement for Non-Dramatic Programming" was actually a non-struck SAG-AFTRA contract.
Also in August 2023, NPR stated that "Dropout has not shared their official subscriber count, but Reich says it's in the mid-hundreds of thousands. He's very aware that doesn't come close to the hundreds of millions of subscribers that large media companies have, but, to him, that's not necessarily a problem. ... Though they're not required to by unions, he said Dropout is working to become one of the first streamers to pay residuals to their writers, actors, and crew members". Dropout also pays performers to audition. Dropout's overall subscriber count almost doubled during 2023; by mid-2025, Dropout had nearly a million subscribers.
Dropout officially retired all CollegeHumor branding in September 2023. Later that year, Dropout shared its profit with its employees and other workers, giving them between a tenth and a fourth of their total earnings. This practice continued in 2024 for a second year. Additionally, Dropout offers higher-than-average pay to performers: due to its heavy use of prosthetics, actors on Very Important People are paid between 10,000 per episode, and main cast members of Dimension 20 are paid approximately 1 million.
In August 2025, Variety reported that Brennan Lee Mulligan had "struck a new three-year development deal at his longtime home media company Dropout".
Dropout is available worldwide; as of 2024 around 60% of subscribers were in the United States.
The company has justified the pricing model as allowing them to create content without being dependent on, or beholden to, requests from advertisers. In December 2018, visitors to the service spent on average 31 minutes per visit, and visited on average 3.5 times per week. Throughout 2019, Dropout began to experiment with more live streaming versions of their shows, through using Twitch and podcast-like formats. At the end of 2023, the average user subscribed for 18 months; Reich said in June 2024 that the statistic is continuing to increase. As of June 2024, Dropout earns around 80–85% of its revenue via subscriptions.
Dropout launched with a beta price of 5.99/month, semi-annual memberships for 3.99/month. As of January 2022, new subscribers paid 59.99 yearly, while those who already subscribed prior to that date were charged 47.99 yearly. In April 2025, Dropout announced a subscription cost increase (69.99 annually) which took effect on May 7, 2025. However, this increase did not impact existing subscribers who were grandfathered in to the legacy subscription cost.
In August 2019, CollegeHumor partnered with Facebook to offer Dropout content via paid video subscriptions on Facebook's platform. Users are also able to access Dropout content through YouTube via the join function.
In addition to original series, Dropout offered videos produced by CollegeHumor 72 hours before they were released to the public. Previously, a subscriber-only Discord server was included as a service, later expanded to non-subscribers as well, until its closure on May 26, 2024. Dropout also has a store which sells merchandise (such as shirts, stickers, and mugs) related to shows on the platform.
Dropout productions often feature common rotating talent from a roster of performers, including:
Shows on Dropout are usually released fortnightly, a schedule used more often for podcasts than for television. Dropout planned to release a new original title per month in 2019, according to Sam Reich. By 2020, the service had retired all scripted shows in favor of cheaper and more successful unscripted content.
Dirty Laundry, Play It By Ear, and Make Some Noise, each spun off from Game Changer, premiered throughout 2022. In 2023, Dropout started airing the improvisational interview show Very Important People, and in 2024 six new shows premiered: Smartypants, Thousandaires, Dropout Presents, Monét's Slumber Party, Gastronauts, and Nobody Asked.
A new show called Parlor Room, hosted by Becca Scott, was announced in March 2025 and premiered on April 18, 2025. A fourth Game Changer spinoff, Crowd Control, is scheduled for release later in the year. The second season of Gastronauts will premiere on August 15, 2025.
The following shows have concluded and are no longer producing new episodes.
Dropout Presents is a series of recorded live shows by different comedians and improv groups.
Mulligan and Roland are scheduled to perform Bigger! With Brennan and Izzy live in Seattle on July 18, 2025 and at the MGM Grand Las Vegas on October 31.
Dropout Improv is an improvisational comedy tour featuring regular Dropout performers.
A panel event called "Cue the Chaos: In Convo with Dropout Improv" took place at the Just for Laughs comedy festival in Montreal on July 26, 2025. Sam Reich also appeared at a "fireside chat" on July 24 as one of the festival's more exclusive "ComedyPRO" events.
Dates for a Dropout Improv tour in Australia and New Zealand are scheduled in 2026 for Perth, Sydney, Melbourne, and Auckland.
Dropout's show Very Important People was the People's Voice Winner in the video comedy category at the 2024 Webby Awards. The College Dropout is the debut studio album by the American rapper Kanye West. It was released on February 10, 2004, by Roc-A-Fella Records and Def Jam Recordings. In the years leading up to its release, West had received praise for his production work for rappers such as Jay-Z and Talib Kweli, but he faced difficulty being accepted as an artist in his own right by figures in the music industry. Intent on pursuing a solo career, he signed a record deal with Roc-A-Fella and recorded the album over a period of four years, beginning in 1999.
The production of The College Dropout was primarily handled by West and showcased his "chipmunk soul" musical style, which made use of sped-up, pitch shifted vocal samples from soul and R&B records, in addition to West's own drum programming, string accompaniments, and gospel choirs; the album also features contributions from Jay-Z, Mos Def, Jamie Foxx, Syleena Johnson, and Ludacris, among others. Diverging from the then-dominant gangster persona in hip-hop, West's lyrics concern themes of family, self-consciousness, materialism, religion, racism, and higher education.
The College Dropout debuted at number two on the US Billboard 200, selling 441,000 copies in its first week of sales. It was a large-scale commercial success, with domestic sales of over 3.4 million copies by 2014 and was certified 4× platinum by the Recording Industry Association of America (RIAA) in 2020. The album was promoted with singles such as "Through the Wire", "Jesus Walks", "All Falls Down", and "Slow Jamz", the latter two of which peaked within the top ten of the Billboard Hot 100, with "Slow Jamz" becoming West's first number-one single as a lead artist.
A widespread critical success, The College Dropout was praised for West's production, humorous and emotional raps, and the music's balance of self-examination and mainstream sensibilities. The album earned the rapper several accolades, including nominations for Album of the Year and Best Rap Album at the 2005 Grammy Awards, winning for the latter. It has since been named by numerous publications as one of the greatest albums of all time, including Rolling Stone and NME, who ranked it at 74 and 273 respectively on their "500 Greatest Albums of All Time" lists, and is credited for popularizing the chipmunk soul and conscious rap subgenres in the 2000s.
Kanye West began his early production career in the mid-1990s, making beats primarily for burgeoning local artists, eventually developing a style that involved speeding up vocal samples from classic soul records. For a time, he acted as a ghost producer for Deric "D-Dot" Angelettie. Due to his association with D-Dot, West was unable to release a solo album, so he formed and became a member and producer of the Go-Getters, a late-1990s Chicago rap group composed of him, GLC, Timmy G, Really Doe, and Arrowstar. The group released their first and only studio album World Record Holders in 1999. West came to achieve recognition with his contributions to Jay-Z's influential 2001 album The Blueprint. The Blueprint has been named by Rolling Stone as the 50th greatest album of all time, and the critical and financial success of the album generated substantial interest in West as a producer. Serving as an in-house producer for Roc-A-Fella Records, West produced records for other artists from the label, including Beanie Sigel, Freeway, and Cam'ron. He also crafted hit songs for Ludacris, Alicia Keys, and Janet Jackson.
Although he had attained success as a producer, Kanye West aspired to be a rapper, but had struggled to attain a record deal. Record companies ignored him because he did not portray the gangsta image prominent in mainstream hip-hop at the time. After a series of meetings with Capitol Records, West was ultimately denied an artist deal. According to Capitol Records's A&R, Joe Weinberger, he was approached by West and almost signed a deal with him, but another person in the company convinced Capitol's president not to. Desperate to keep West from defecting to another label, then-label head Damon Dash reluctantly signed West to Roc-A-Fella Records. Jay-Z, West's colleague and one of his mentors, later admitted that Roc-A-Fella was initially reluctant to support West as a rapper, claiming that many saw him as a producer first and foremost, and that his background contrasted with that of his labelmates.
West's breakthrough came a year later on October 23, 2002, when, while driving home from a California recording studio after working late, he fell asleep at the wheel and was involved in a near-fatal car crash. The crash left him with a shattered jaw, which had to be wired shut in reconstructive surgery. The accident inspired West; two weeks after being admitted to a hospital, he recorded a song at the Record Plant with his jaw still wired shut. The composition, "Through the Wire", expressed West's experience after the accident, and helped lay the foundation for his debut album, as according to West "all the better artists have expressed what they were going through". West added that "the album was my medicine", as working on the record distracted him from the pain. "Through the Wire" was first available on West's Get Well Soon... mixtape, released December 2002. At the same time, West announced that he was working on an album called The College Dropout, whose overall theme was to "make your own decisions. Don't let society tell you, 'This is what you have to do.'"
West began recording The College Dropout in 1999, taking four years to complete. Recording sessions took place at Record Plant in Los Angeles, California, but the production featured on the record took place elsewhere over the course of several years. According to John Monopoly, West's friend, manager and business partner, the album "...[didn't have] a particular start date. He's been gathering beats for years. He was always producing with the intention of being a rapper. There's beats on the album he's been literally saving for himself for years". At one point, West hovered between making a portion of the production in the studio and the majority within his own apartment in Hoboken, New Jersey. Because it was a two-bedroom apartment, West was able to set up a home studio in one of the rooms and his bedroom in the other.
West brought a Louis Vuitton backpack filled with old disks and demos to the studio, producing tracks in less than fifteen minutes at a time. He recorded the remainder of the album in Los Angeles while recovering from the car accident. Once he had completed the album, it was leaked months before its release date. However, West decided to use the opportunity to review the album, and The College Dropout was significantly remixed, remastered, and revised before being released. As a result, certain tracks originally destined for the album were subsequently retracted, among them "Keep the Receipt" with Ol' Dirty Bastard and "The Good, the Bad, and the Ugly" with Consequence. West meticulously refined the production, adding string arrangements, gospel choirs, improved drum programming and new verses. On his personal blog in 2009, West stated he was most inspired by The Miseducation of Lauryn Hill and listened to the album every day while working on The College Dropout.
The song "School Spirit" was censored for the album because Aretha Franklin would not allow the rapper to sample her music without censorship being promised. It was revealed by Plain Pat that there were around three other versions of the song, but West disliked them. Pat said in reference to the Franklin sample: "That song would have been so weak if we didn't get that sample cleared". In 2011, an uncensored version of the track was distributed online.
West finished recording around December 2003, according to his older cousin and singer Tony Williams, who was recruited by the rapper two weeks before the album's deadline to contribute vocals. Williams had impressed West by singing improvisations to "Spaceship" during one of their drives together. The singer later recounted recording with West for The College Dropout at the Record Plant: "I get in, go in the booth, start vibing out on 'Spaceship' and finished it up. At that point he was like, 'Ok, Well let me see what you do on this song.' I think that's when we did 'Last Call.' One song lead to another, and by the end of the weekend, I was on like five songs. Then we did the 'I'll Fly Away' joint". In a January 2020 interview with GQ, West revealed that around 30 to 40 percent of the album was recorded on a Roland VS-1680.
The College Dropout diverged from the then-dominant gangster persona in hip-hop in favor of more diverse, topical subjects for the lyrics. Throughout the album, West touches on a number of different issues drawn from his own experiences and observations, including organized religion, family, sexuality, excessive materialism, self-consciousness, minimum wage labor, institutional prejudice, and personal struggles. Music journalist Kelefa Sanneh wrote, "Throughout the album, Mr. West taunts everyone who didn't believe in him: teachers, record executives, police officers, even his former boss at the Gap". West explained, "My persona is that I'm the regular person. Just think about whatever you've been through in the past week, and I have a song about that on my album". The album was musically notable for West's unique development of his "chipmunk soul" production style, in which R&B and soul music samples were sped up and pitch shifted.
The album begins with a skit featuring a college professor asking West to deliver a graduation speech. The skit is followed by "We Don't Care" featuring West comically celebrating drug life with lines like "We wasn't supposed to make it past 25, joke's on you, we still alive" and then criticizing its influence amongst children. The next track, "Graduation Day", features Miri Ben-Ari on violin and vocals by John Legend.
On "All Falls Down", West wages an attack on consumerism. The song features singer Syleena Johnson and contains an interpolation of Lauryn Hill's "Mystery of Iniquity". West called upon Johnson to re-sing a vocal portion of "Mystery of Iniquity", which ended up in the final mix. Gospel hymn with doo-wop elements "I'll Fly Away" precedes "Spaceship", a track with a relaxed beat containing a soulful Marvin Gaye sample. The lyrics are mostly critical of the working world, where West muses about flying away in a spaceship to leave his boring job, and guest rappers GLC and Consequence compare the modern-day retail environment to slavery.
On "Jesus Walks", West professes his belief in Jesus, while also discussing how religion is used by various people and how the media seems to avoid songs that address matters of faith while embracing compositions on violence, sex, and drugs. "Jesus Walks" is built around a sample of "Walk With Me" as performed by the ARC Choir. Garry Mulholland of The Observer described it as a "towering inferno of martial beats, fathoms-deep chain gang backing chants, a defiant children's choir, gospel wails, and sizzling orchestral breaks". The first verse of the song is told through the eyes of a drug dealer seeking help from God, and it reportedly took over six months for West to draw inspiration for the second verse.
"Never Let Me Down" is influenced by West's near-death car crash. The song features Jay-Z, who rhymes about maintaining status and power given his chart success, while West comments on racism and poverty. The song features verses by spoken word performer J. Ivy who offers comments of upliftment. "Never Let Me Down" reuses a Jay-Z verse first heard in the remix of his song "Hovi Baby". "Get Em High" is a collaboration by West with two socially conscious rappers, Talib Kweli and Common. "The New Workout Plan" is a call to fitness to improve one's love life. "Slow Jamz" features Twista and Jamie Foxx and serves as a tribute to classic smooth soul artists and slow jam songs. The song also appeared on Twista's album Kamikaze. On the song "School Spirit", West relates the experience of dropping out of school and contains references to well-known fraternities, sororities, singer Norah Jones, and record label Roc-A-Fella Records. "Two Words" features commentary on social issues and features Mos Def, Freeway, and the Harlem Boys Choir.
"Through the Wire" features a high-pitched vocal sample of Chaka Khan and relates West's real life experience with being in a car accident. The song provides a mostly comedic account of his difficult recovery, and features West rapping with his jaw still wired shut from the accident. The chorus and instrumentals sample a pitched up version of Chaka Khan's 1985 single "Through the Fire". "Family Business" is a soulful tribute to the godbrother of Tarrey Torae, one of the many collaborators in the album. The song "Last Call" is about West's transition from being a producer to a rapper, and the album ends with a nearly nine-minute autobiographical monologue that follows the song "Last Call". However, this is not a separate track.
The album's title is in part a reference to West's decision to drop out of college to pursue his dream of becoming a musician. This action greatly displeased his mother, who was a professor at the university from which he withdrew. She later said, "It was drummed into my head that college is the ticket to a good life... but some career goals don't require college. For Kanye to make an album called [The] College Dropout it was more about having the guts to embrace who you are, rather than following the path society has carved out for you".
The artwork for the album was developed by Eric Duvauchelle, who was then part of Roc-A-Fella's in-house brand design team. West had already taken pictures dressed as the Dropout Bear – which would reappear in his later work – and Duvauchelle picked the image of him sitting on a set of bleachers, as he was attracted to the loneliness of what was supposed to be "the most popular representation of a school". The image is framed inside gold ornaments, which Duvauchelle found in a book of illustrations from the 16th-century and West wanted to use to "bring a sense of elegance and style to what was typically a gangster-led image of rap artists". The inside cover follows a college yearbook, with photos of the featured artists from their youth.
While the original release of the album features a brown background on the cover artwork, later remasters of the album feature a white background.
The College Dropout was originally scheduled for release in August 2003, but West's perfectionist habits producing the album led to it being postponed three times. It was first delayed to October 2003, then to January 2004, before finally being released to stores on February 10, 2004.
In its first week of release, the album sold 441,000 copies and debuted at number two on the US Billboard 200 chart, being held off the top spot by Norah Jones' second studio album Feels Like Home. The College Dropout remained at the second spot behind Feels Like Home for two consecutive weeks, with 196,000 units sold in the second week and 132,000 in the third week, respectively. In 2004, The College Dropout ranked as the twelfth most popular of the year on the Billboard 200. The album had sold 2.3 million units in the United States by November 2004. By June 2014, the album had become West's best-selling album in the US at the time, with domestic sales of 3,358,000 copies. On November 23, 2020, The College Dropout was certified four-times platinum by the Recording Industry Association of America (RIAA). On the UK Albums Chart, the album peaked at number 12, and the British Phonographic Industry (BPI) had certified it double platinum by November 25, 2004; this indicated shipments of 600,000 copies. As of 2018, The College Dropout is the fourteenth highest selling rap album in the UK in the 21st-century. The album has sold over 4 million copies worldwide.
Four of the singles released in promotion of the album became top-20 chart hits: "Through the Wire", "Slow Jamz", "All Falls Down" and "Jesus Walks". "The New Workout Plan" was the fifth and last single. "Spaceship" was planned to be the sixth single, but Def Jam decided to move on from The College Dropout's promotional campaign to begin marketing West's next album, Late Registration (2005). At one point, "Two Words" was also intended to be released as a single, and a video for the song was filmed, and later uploaded by West online in 2009.
The College Dropout was met with widespread critical acclaim. At Metacritic, which assigns a normalized rating out of 100 to reviews from professional publications, the album received an average score of 87, based on 25 reviews, indicating "universal acclaim".
The record was hailed by Kelefa Sanneh from The New York Times as "2004's first great hip-hop album". Reviewing it for The A.V. Club, Nathan Rabin observed in the music "substance, social commentary, righteous anger, ornery humanism, dark humor, and even Christianity", calling it "one of those wonderful crossover albums that appeal to a huge audience without sacrificing a shred of integrity". The staff of Mojo said its exceptional hip-hop production was miraculous during a time when hip-hop's practice of sampling was becoming "increasingly litigious", and those of Urb deemed it "both visceral and emotive, sprinkling the dancefloors with tears and sweat". Dave Heaton from PopMatters found it "musically engaging" and "a genuine extension of Kanye's personality and experiences", while Hua Hsu of The Village Voice felt that his sped-up samples "carry a humble, human air", allowing listeners to "hear tiny traces of actual people inside". Fellow Village Voice critic Robert Christgau wrote that "not only does [West] create a unique role model, that role model is dangerous—his arguments against education are as market-targeted as other rappers' arguments for thug life". In the opinion of Stylus Magazine's Josh Love, West "subverts cliches from both sides of the hip-hop divide" while "trying to reflect the entire spectrum of hip-hop and black experience, looking for solace and salvation in the traditional safehouses of church and family". Entertainment Weekly's Michael Endelman elaborated on West's avoidance of the then-dominant "gangsta" persona of hip-hop:
West delivers the goods with a disarming mix of confessional honesty and sarcastic humor, earnest idealism and big-pimping materialism. In a scene still dominated by authenticity battles and gangsta posturing, he's a middle-class, politically conscious, post-thug, bourgeois rapper – and that's nothing to be ashamed of.
Some reviewers were more qualified in their praise. Rolling Stone's Jon Caramanica felt that "West isn't quite MC enough to hold down the entire disc", though claimed that West's "ace in the hole is his signature cozy sound", while Slant Magazine's Sal Cinquemani observed "too many guest artists, too many interludes, and just too many songs period" on what he considered a "chest-beatingly self-congratulatory" yet humorous, deeply sincere, and affecting record. It was regarded by Pitchfork critic Rob Mitchum as a "flawed, overlong, hypocritical, egotistical, and altogether terrific album". The staff of Rolling Stone were more receptive in a retrospective review than Caramanica was previously for the publication, calling the album "a demonstration that hip-hop—real, banging, commercial hip-hop—could be a vehicle for nuanced self-examination and musical subtlety".
The College Dropout was voted as the best album of the year by The Village Voice's Pazz & Jop, an annual poll of American critics. The album elsewhere topped year-end lists by Rolling Stone, Spin, Vibe, and PopMatters. Dutch magazine OOR named it the seventh best album of 2004. Billboard named The College Dropout the second best album of 2004. Rhapsody named it the seventh best album of the decade and the fourth best hip-hop album of the decade.
In 2005, Pitchfork named it No. 50 in their best albums of 2000–2004. In 2006, the album was named by Time as one of the 100 best albums of all time. In its retrospective 2007 issue, XXL named it one of the magazine's "XXL"-rated releases; this perfect rating had previously been given by the magazine to only sixteen other albums. In 2012, Complex named the album one of the classic albums of the last decade, and the 20th best hip-hop debut album ever. Dagsavisen listed the album eleventh in its list of the top forty albums of the 2000s decade. The album was also included in the book 1001 Albums You Must Hear Before You Die. NME placed the album at 273 on its 2013 list of "The 500 Greatest Albums of All Time", while Rolling Stone ranked it at 74 on their list of "The 500 Greatest Albums of All Time" in 2020.
The College Dropout sparked a resurgence of socially conscious rap in the mid-2000s, arriving at a time when pop rap was saturated with songs featuring product placement and intensely violent lyrics, epitomized by rappers like 50 Cent, Nelly, Ja Rule, Ludacris, and P. Diddy. West instead created a space in the mainstream for rappers to express themselves and black identity without resorting to hip-hop's prevalent theme of gang culture. Raul Verma of The Independent said "West is charged with proving mainstream hip hop has a conscience with his nourishing messages of substance flying in the face of the amoral majority perpetuating clichés of guns, girls and bling", while Vibe senior editor Noah Callahan-Bever argued that West's infusion of "pop sensibility" into his otherwise progressive hip-hop had "bridged the gap" and encouraged rappers to gravitate more towards the center between mainstream and alternative forms. Today commented that The College Dropout "stood out in the rap landscape because of its atypical prose. It avoided the usual plotlines about sex, money and violence and touched on everything from his faith to his fears of failure and other crises from his life."
According to DJBooth journalist Brad Callas, the album also "helped solidify chipmunk soul as not only the defining sound of the Roc-A-Fella era but also the most popular sub-genre in hip-hop". "It feels like that album birthed an entire sub-genre", Max Weinstein wrote in retrospect for Vibe, going on to say, "The palette of emotions was so broad, the depth of topics so searingly relevant, that it was bound to make an impression on any artist that heard it. RZA might have birthed chipmunk soul, and Black Star perfected smart lyricism for the JanSport bunch, but 'Ye brought all that to the masses in one single, digestible product, breaking down the divisions between mainstream rap and Rawkus-grade consciousness." Weinstein also credited The College Dropout with directly influencing 10 albums: Lupe Fiasco's Food & Liquor (2006) by Lupe Fiasco, School Was My Hustle (2006) by Kidz in the Hall, Don't Quit Your Day Job! (2007) by Consequence, A Kid Named Cudi (2008) by Kid Cudi, Asleep in the Bread Aisle (2009) by Asher Roth, Kendrick Lamar's self-titled first EP (2009), Camp (2011) by Childish Gambino, Cole World: The Sideline Story (2011) by J. Cole, When Fish Ride Bicycles (2011) by The Cool Kids, and Acid Rap (2013) by Chance the Rapper.
With the album, West began to develop a following of listeners who could not relate to lyrics glorifying gangster lifestyle but still enjoyed rap music and connected more with his musings on family and love. In 2005, comedian Chris Rock attested to listening to The College Dropout while writing his stand-up material. Music journalists such as Meaghan Garvey, Andrew Barber, and Erika Ramirez also connected to the album during their formative years, with Barber saying in a roundtable discussion for Noisey, "I could identify with this project the most because I was in college at the time, and I felt like an underdog in my own life. I was uncertain of my future. [West's] words on 'Last Call' inspired me to follow my dreams, and motivated me to graduate despite the album title." In the same discussion, music journalist Eric Sundermann cited The College Dropout as the first in West's pop rap album trilogy that would be followed by Late Registration in 2005 and Graduation in 2007, while Craig Jenkins called it "a watershed moment in 2000s rap history where the nerds stormed the school to seize control from the jocks, a shift memorialized two albums later when Graduation trounced 50 Cent's Curtis album in their 2007 sales showdown."
All tracks are produced by Kanye West, except "Last Call" (co-produced with Evidence; additional production by Porse) and "Breathe In Breathe Out" (co-produced with Brian Miller).
"We Don't Care" contains samples of "I Just Wanna Stop", written by Ross Vannelli and performed by The Jimmy Castor Bunch.
"All Falls Down" contains interpolations of "Mystery of Iniquity", written and performed by Lauryn Hill.
"Spaceship" contains samples of "Distant Lover", written by Marvin Gaye, Gwen Gordy Fuqua and Sandra Greene, and performed by Marvin Gaye.
"Jesus Walks" contains samples of "Walk with Me", performed by The ARC Choir and "(Don't Worry) If There's a Hell Below, We're All Going to Go", written and performed by Curtis Mayfield.
"Never Let Me Down" contains samples of "Maybe It's the Power of Love", written by Michael Bolton and Bruce Kulick, and performed by Blackjack.
"Slow Jamz" contains samples of "A House Is Not a Home", written by Burt Bacharach and Hal David, and performed by Luther Vandross.
"School Spirit" contains samples of "Spirit in the Dark", written and performed by Aretha Franklin.
"Two Words" contains samples of "Peace & Love (Amani Na Mapenzi) – Movement IV (Encounter)", written by Lou Wilson, Ric Wilson and Carlos Wilson, and performed by Mandrill.
"Through the Wire" contains samples of "Through the Fire", written by David Foster, Tom Keane and Cynthia Weil, and performed by Chaka Khan.
"Family Business" contains samples of "Fonky Thang", written by Terry Callier and Charles Stepney, and performed by The Dells.
"Last Call" contains samples of "Mr. Rockefeller", written by Jerry Blatt and Bette Midler, and performed by Bette Midler.
Credits are adapted from the album's liner notes.
2004 in hip-hop
Kanye West albums discography
Kanye West production discography
List of Billboard number-one R&B albums of 2004
College Dropout (restaurant)
Brown, Jake (2006). Kanye West in the Studio: Beats Down! Money Up! (2000–2006). Colossus Books. ISBN 0-9767735-6-2.
Hess, Mickey (2007). Icons of Hip Hop: an Encyclopedia of the Movement, Music, and Culture. Greenwood Publishing Group. ISBN 978-0-313-33904-2.
West, Donda; Hunter, Karen (2007). Raising Kanye: Life Lessons from the Mother of a Hip-Hop Superstar. Simon & Schuster. ISBN 978-1-4165-4470-8.
The College Dropout at Discogs A dropout is a momentary loss of signal in a communications system, usually caused by noise, propagation anomalies, or system malfunctions. For analog signals, a dropout is frequently gradual and partial, depending on the cause. For digital signals, dropouts are more pronounced, usually being sudden and complete, due to the cliff effect. In mobile telephony, a dropout of more than a few seconds will result in a dropped call.
This article incorporates public domain material from Federal Standard 1037C. General Services Administration. Archived from the original on 2022-01-22. (in support of MIL-STD-188). Dropout Kings (formerly Phoenix Down) is an American rap metal band from Phoenix, Arizona, United States, that formed in 2016. They are currently signed to Suburban Noize Records. Dropout Kings have released two albums: Audiodope (released August 10, 2018) and Riot Music (released 5 May 2023), and one EP: GlitchGang (released April 6, 2021).
Dropout Kings was founded by Adam Ramey after his previous band The Bad Chapter broke up in 2015. As a way to stay active musically, he approached rapper Eddie Wellz to do a cover of Linkin Park's "Lying from You" after seeing a video of Wellz performing on YouTube. Due to positive reception, Ramey approached previous members of The Bad Chapter; Trevor Norgren and later Staig Flynn and Rob Sebastian and previous bandmate Chucky Guzman from We The Collectors to form a new band called Phoenix Down, which by 2017 had changed its name to Dropout Kings.
On April 6, 2018, it was announced that Dropout Kings signed with Napalm Records, & the band released the music video for '"NVM", their first single from their debut album AudioDope the same day. On June 8, 2018, the band released the music video for "Scratch & Claw", the second single from AudioDope. AudioDope was released on August 10, 2018. AudioDope reached 2 US Charts, #52-Top New Artist Albums & #112-Record Label Independent Current Albums.
In July–August 2018, the band toured the United States and Canada with OTEP. During the Otep tour, the band announced that they were managed by Dez Fafara, the lead vocalist for DevilDriver & Coal Chamber.
In February–March 2019, the band toured with Outline in Color, Deadships and Dead Crown.
On March 19, 2019, Dayshell featured the band on their single “KOMBAT."
In May–June 2019, the band joined Crazy Town as direct support on their 20th Anniversary Tour.
On May 26, 2019, it was announced that the band would be playing the Main Stage at the 20th Annual Soopa Gathering of the Juggalos on August 3, 2019.
On April 6, 2021, it was announced that Dropout Kings had signed to Suburban Noize Records.
On June 15, 2021, Dropout Kings embarked on their first headlining tour.
On September 8, 2021, “Virus” peaked at #33 on the Billboard Mainstream Rock Radio Charts while spending 12 weeks in the Top 40.
On October 26, 2021, it was announced that “Virus” was being considered for a Grammy nomination in the Best New Artist category.
On January 24, 2022, it was announced that Dropout Kings would be playing at UFest 2022 on April 24, 2022.
On April 30, 2023, Staig parted ways with the band after his mother got into an accident.
On June 6, 2023, Chucky parted ways with the band, and Rob switched to guitar, converting the band into a permanent quartet.
Vocalist and founding member Adam Ramey died by suicide on May 19, 2025, at the age of 32.
William "Black Cat Bill" Lauderdale - lead vocals (2025–present), rapping vocals (2016–present)
Rob Sebastian - lead vocals (2025–present), guitar (2023–2025), bass (2017–2025)
Jeremy Garcia - guitars (2025–present), drums (2025)
Joe Lana Jr. - drums (2021–2024, 2025–present)
Chucky Guzman - guitars (2016–2023, 2025–present)
Staig Flynn - guitars (2017–2023)
Trevor Norgren - drums (2016–2021)
Tre Scott - guitar, bass (2016–2017)
Charlie Mumbles - turntables, programming (2016–2017)
Adam Ramey - lead vocals (2016–2025; his death)
Riot Music (2023)
GlitchGang (2020 Stay Sick Recordings, reissued 2021 on Suburban Noize Records)
"Street Sharks" (2016; as Phoenix Down)
"Scratch & Claw" (2018)
"Going Rogue" feat. Landon Tewers (2018)
"I Ain't Depressed" feat. Hacktivist (2020)
"Virus" feat. Shayley Bourget (2021) – No. 33 Mainstream Rock Songs
"Street Sharks" (2016; as Phoenix Down)
"Scratch & Claw" (2018)
"Going Rogue" (2018)
"Bad Day" (2019)
"Something Awful" (2019)
"I Ain't Depressed" (2020)
"Hey Uh" (2022)
Dropout Kings at AllMusic The Dropout Bear (also referred to as the Kanye Bear, Late Registration Bear, or the Graduation Bear) is an anthropomorphic symbol, character, and mascot for American rapper Kanye West. The bear was originally designed by graphic designer Sam Hansen and was used in the album cover art, promotion, and music videos for West's first three studio albums, The College Dropout (2004), Late Registration (2005), and Graduation (2007).
The Dropout Bear was originally designed by graphic designer Sam Hansen. Its first commercial appearance came on the cover art for West's debut single "Through the Wire" in September 2003. The character would later appear on the cover art of West's debut studio album, The College Dropout, in February 2004. The album's cover was handled by art director Eric Duvauchelle of Roc-A-Fella Records, which depicts the Dropout Bear in a school gymnasium, wearing a suit coat, red T-shirt, and jeans. The Dropout Bear suit also made an appearance in the music video for West's "The New Workout Plan" in 2004.
The first redesign of the Dropout Bear came with the development of West's sophomore album, Late Registration.
Similar to the cover art of The College Dropout, the artwork on Late Registration features the mascot, showing it at a child's size and standing in the center of two large wooden doors at Princeton University. The mascot has googly eyes, perky ears, and a collegian outfit, wearing a blazer with a school insignia. In the album booklet, Dropout Bear appears in the university, sitting alone in classrooms and reading books before exiting.
This design for the Dropout Bear on Late Registration was carried over onto the cover art for West's live album Late Orchestration in 2006.
The second redesign of the Dropout Bear came with the development of West's third studio album, Graduation. West collaborated with Japanese contemporary artist Takashi Murakami to oversee the art direction of Graduation as well as design the cover art for the album's accompanying singles. Often called "the Warhol of Japan", Murakami's surrealistic visual art is characterized by cartoonish creatures that appear friendly and cheerful at first glance but possess dark, twisted undertones.
The album's artwork of the Dropout Bear expresses colorful, pastel imagery influenced by Murakami's affiliation with Superflat, a postmodern art movement influenced by manga and anime. Murakami later reproduced the artwork designs through the use of cel-shaded animation within a three-minute animated music video for the opening track, "Good Morning".
After collaborating with West on the artwork and video, Murakami later worked on the cover art for West and Kid Cudi's eponymous debut studio album, Kids See Ghosts (2018).
The Dropout Bear's most recent appearances came with the promotion and artwork for West's collaborative album Kids See Ghosts with Kid Cudi in 2018. Takashi Murakami had stated that West had brought forward the idea of portraying an anthropomorphized bear and fox to reflect him and Cudi, respectively. A trailer for a supposed Kids See Ghosts animated show was released on YouTube in June 2020, depicting the Dropout Bear and Kid Cudi's anthropomorphic fox. Harverd Dropout is the second studio album by American rapper Lil Pump. It was released through Tha Lights Global and Warner Records on February 22, 2019. The album consists of 16 tracks and features guest appearances from Kanye West, Smokepurpp, Migos rappers Offset and Quavo, Lil Uzi Vert, Lil Wayne, YG, and 2 Chainz. It follows his self-titled debut studio album (2017).
It was supported by seven singles – "Esskeetit", "Drug Addicts", "I Love It" (with Kanye West), "Multi Millionaire" (featuring Lil Uzi Vert), "Butterfly Doors", "Racks on Racks" and "Be Like Me" (featuring Lil Wayne). Harverd Dropout received mixed reviews and debuted at number seven on the US Billboard 200 with 48,000 album-equivalent units, of which 25,000 were pure album sales. It is Lil Pump's second US top 10 album. The album was certified gold by the Recording Industry Association of America (RIAA) on September 27, 2019 for sales over 500,000 units.
Lil Pump released his eponymous self-titled album in October 2017. In January 2018, he announced his second project Harverd Dropout, which was finished in April 2018. The project was initially planned to be released on August 17, 2018, Garcia's 18th birthday, but was postponed due to Lil Pump "losing the album".
Following Garcia's arrest on August 29, 2018, for unlicensed driving, Garcia's management team announced the release date of Harverd Dropout to be September 14, 2018. A week later, on September 6, 2018, Lil Pump teamed up with American rapper Kanye West for the song "I Love It" with comedian Adele Givens. The song was released by Lil Pump's management team.
The September 14, 2018, release date was also canceled. In October 2018, he released the song "Multi Millionaire" with Lil Uzi Vert.
"Butterfly Doors" was released on January 4, 2019. Lil Pump then posted the cover art on his social media accounts on January 23, 2019, along with announcing the album's release date as February 22.
On January 31, 2019, "Racks on Racks" was released along with its music video. A day before the album's release, "Be Like Me" featuring Lil Wayne was released as the seventh single.
The album was released on digital streaming platforms on February 22, 2019.
Harverd Dropout received mixed to negative reviews from music critics. At Metacritic, which assigns a normalized rating out of 100 to reviews from mainstream publications, the album received a score of 46, based on eight reviews, indicating "mixed or average reviews".
Luke Morgan Britton of NME wrote that much of the album felt like "little more than regurgitated punchlines or uninspired variations on themes already set up and adequately executed on the rapper's early tracks." Alphonse Pierre of Pitchfork felt that by the time of the album's release, Lil Pump's act had become contrived, writing, "Having abandoned his lo-fi roots in the South Florida rap scene, Lil Pump has become a caricature of himself. His second album is sometimes fun but mostly unnecessary." In a negative review, Tommy Monroe of Consequence of Sound felt that the album was weighed down by "songs without structure, lines without meaning, and hooks without melody," calling the release "utterly tasteless." Nick Soulsby of PopMatters heavily criticized the album for its lack of depth and its repetitive nature, writing, "Lil Pump's ambition apparently goes no further than having one song slip onto a party playlist and make just enough impact people occasionally ask for 'that song where the guy says...' having caught the couple of words he's hammered over and over."
Conversely, in a positive review, A.D. Amorosi of Variety wrote that although Lil Pump's style or flow hadn't changed since his initial singles or his self-titled album debut, his songs "[helped] turn trap into a pop-hop vibe," describing the album as "simplistic, contagiously catchy, and occasionally enhanced by rousing choruses." Writing for HipHopDX, Scott Glaysher praised the album's production, while taking note of its polished vocals and improved mixing. He wrote that "the beats are big and brazen" and that "the choruses are wildly catchy and some of the featured guests even get off some decently solid verses."
Harverd Dropout debuted at number seven on the US Billboard 200 with 48,000 album-equivalent units, of which 25,000 were pure album sales. It is Lil Pump's second US top 10 album. The album was certified gold by the Recording Industry Association of America (RIAA) on September 27, 2019 for sales over 500,000 units.
Credits adapted from ASCAP, BMI and Tidal.
Credits adapted from Tidal.
CBMix – mixing (track 6, 8, 10, 12, 14, 15), mastering (track 6, 8, 10, 12, 14, 15), recording (track 6, 10, 12, 14, 15)
Harverd Dropout at Discogs (list of releases) Batch normalization (also known as batch norm) is a normalization technique used to make training of artificial neural networks faster and more stable by adjusting the inputs to each layer—re-centering them around zero and re-scaling them to a standard size. It was introduced by Sergey Ioffe and Christian Szegedy in 2015.
Experts still debate why batch normalization works so well. It was initially thought to tackle internal covariate shift, a problem where parameter initialization and changes in the distribution of the inputs of each layer affect the learning rate of the network. However, newer research suggests it doesn’t fix this shift but instead smooths the objective function—a mathematical guide the network follows to improve—enhancing performance. In very deep networks, batch normalization can initially cause a severe gradient explosion—where updates to the network grow uncontrollably large—but this is managed with shortcuts called skip connections in residual networks. Another theory is that batch normalization adjusts data by handling its size and path separately, speeding up training.
Each layer in a neural network has inputs that follow a specific distribution, which shifts during training due to two main factors: the random starting values of the network’s settings (parameter initialization) and the natural variation in the input data. This shifting pattern affecting the inputs to the network’s inner layers is called internal covariate shift. While a strict definition isn’t fully agreed upon, experiments show that it involves changes in the means and variances of these inputs during training.
Batch normalization was first developed to address internal covariate shift. During training, as the parameters of preceding layers adjust, the distribution of inputs to the current layer changes accordingly, such that the current layer needs to constantly readjust to new distributions. This issue is particularly severe in deep networks, because small changes in shallower hidden layers will be amplified as they propagate within the network, resulting in significant shift in deeper hidden layers. Batch normalization was proposed to reduced these unwanted shifts to speed up training and produce more reliable models.
Beyond possibly tackling internal covariate shift, batch normalization offers several additional advantages. It allows the network to use a higher learning rate—a setting that controls how quickly the network learns—without causing problems like vanishing or exploding gradients, where updates become too small or too large. It also appears to have a regularizing effect, improving the network’s ability to generalize to new data, reducing the need for dropout, a technique used to prevent overfitting (when a model learns the training data too well and fails on new data). Additionally, networks using batch normalization are less sensitive to the choice of starting settings or learning rates, making them more robust and adaptable.
In a neural network, batch normalization is achieved through a normalization step that fixes the means and variances of each layer's inputs. Ideally, the normalization would be conducted over the entire training set, but to use this step jointly with stochastic optimization methods, it is impractical to use the global information. Thus, normalization is restrained to each mini-batch in the training process.
Let us use B to denote a mini-batch of size m of the entire training set. The empirical mean and variance of B could thus be denoted as
=\sum _^x_
^=\sum _^(x_-\mu _)^
For a layer of the network with d-dimensional input,
, each dimension of its input is then normalized (i.e. re-centered and re-scaled) separately,
_^=^-\mu _^^\right)^+\epsilon 


^
^
are the per-dimension mean and standard deviation, respectively.

is added in the denominator for numerical stability and is an arbitrarily small constant. The resulting normalized activation
^
have zero mean and unit variance, if

is not taken into account. To restore the representation power of the network, a transformation step then follows as
^=\gamma ^_^+\beta ^
where the parameters


are subsequently learned in the optimization process.
Formally, the operation that implements batch normalization is a transform
,\beta ^:x_^\rightarrow y_^
called the Batch Normalizing transform. The output of the BN transform
=BN_,\beta ^(x^)
is then passed to other network layers, while the normalized output
_^
remains internal to the current layer.
The described BN transform is a differentiable operation, and the gradient of the loss l with respect to the different parameters can be computed directly with the chain rule.
^
depends on the choice of activation function, and the gradient against other parameters could be expressed as a function of
^
_^=^\gamma ^
=\sum _^^_^
=\sum _^^
^=\sum _^^(x_^-\mu _^)\left(-(\sigma _^+\epsilon )^\right)
^=\sum _^^^+\epsilon +^\sum _^(-2)\cdot (x_^-\mu _^)
^=_^^+\epsilon +^^-\mu _^)+^
During the training stage, the normalization steps depend on the mini-batches to ensure efficient and reliable training. However, in the inference stage, this dependence is not useful any more. Instead, the normalization step in this stage is computed with the population statistics such that the output could depend on the input in a deterministic manner. The population mean,
, and variance,
 [x^]
, are computed as:
]=E_[\mu _^]
 [x^]=E_[\left(\sigma _^\right)^]
The population statistics thus is a complete representation of the mini-batches.
The BN transform in the inference step thus becomes
=BN_,\beta ^^(x^)=\gamma ^-E[x^] [x^]+\epsilon +\beta ^
is passed on to future layers instead of
. Since the parameters are fixed in this transformation, the batch normalization procedure is essentially applying a linear transform to the activation.
Although batch normalization has become popular due to its strong empirical performance, the working mechanism of the method is not yet well-understood. The explanation made in the original paper was that batch norm works by reducing internal covariate shift, but this has been challenged by more recent work. One experiment trained a VGG-16 network under 3 different training regimes: standard (no batch norm), batch norm, and batch norm with noise added to each layer during training. In the third model, the noise has non-zero mean and non-unit variance, i.e. it explicitly introduces covariate shift. Despite this, it showed similar accuracy to the second model, and both performed better than the first, suggesting that covariate shift is not the reason that batch norm improves performance.
Using batch normalization causes the items in a batch to no longer be iid, which can lead to difficulties in training due to lower quality gradient estimation.
One alternative explanation, is that the improvement with batch normalization is instead due to it producing a smoother parameter space and smoother gradients, as formalized by a smaller Lipschitz constant.
Consider two identical networks, one contains batch normalization layers and the other does not, the behaviors of these two networks are then compared. Denote the loss functions as

, respectively. Let the input to both networks be
, and the output be
, for which
is the layer weights. For the second network,
additionally goes through a batch normalization layer. Denote the normalized activation as

, which has zero mean and unit variance. Let the transformed activation be
+\beta 
, and suppose


are constants. Finally, denote the standard deviation over a mini-batch
\in \mathbb  ^

First, it can be shown that the gradient magnitude of a batch normalized network,
||
, is bounded, with the bound expressed as
||^\leq ^||\triangledown _L||^-\langle 1,\triangledown _L\rangle ^-\langle \triangledown _L,_\rangle ^
Since the gradient magnitude represents the Lipschitzness of the loss, this relationship indicates that a batch normalized network could achieve greater Lipschitzness comparatively. Notice that the bound gets tighter when the gradient

correlates with the activation

, which is a common phenomena. The scaling of
^
is also significant, since the variance is often large.
Secondly, the quadratic form of the loss Hessian with respect to activation in the gradient direction can be bounded as
)^\partial y_(\triangledown _)\leq ^\partial y_-\langle \triangledown _L,\rangle ^
The scaling of
^
indicates that the loss Hessian is resilient to the mini-batch variance, whereas the second term on the right hand side suggests that it becomes smoother when the Hessian and the inner product are non-negative. If the loss is locally convex, then the Hessian is positive semi-definite, while the inner product is positive if

is in the direction towards the minimum of the loss. It could thus be concluded from this inequality that the gradient generally becomes more predictive with the batch normalization layer.
It then follows to translate the bounds related to the loss with respect to the normalized activation to a bound on the loss with respect to the network weights:
\leq ^(g_^-m\mu _^-\lambda ^\langle \triangledown _L,_\rangle ^)
=max_||\triangledown _L||^
_=max_||\triangledown _||^
In addition to the smoother landscape, it is further shown that batch normalization could result in a better initialization with the following inequality:
-^||^\leq ||W_-W^||^-||^(||W^||^-\langle W^,W_\rangle )^
^
are the local optimal weights for the two networks, respectively.
Some scholars argue that the above analysis cannot fully capture the performance of batch normalization, because the proof only concerns the largest eigenvalue, or equivalently, one direction in the landscape at all points. It is suggested that the complete eigenspectrum needs to be taken into account to make a conclusive analysis.
Since it is hypothesized that batch normalization layers could reduce internal covariate shift, an experiment is set up to measure quantitatively how much covariate shift is reduced. First, the notion of internal covariate shift needs to be defined mathematically. Specifically, to quantify the adjustment that a layer's parameters make in response to updates in previous layers, the correlation between the gradients of the loss before and after all previous layers are updated is measured, since gradients could capture the shifts from the first-order training method. If the shift introduced by the changes in previous layers is small, then the correlation between the gradients would be close to 1.
The correlation between the gradients are computed for four models: a standard VGG network, a VGG network with batch normalization layers, a 25-layer deep linear network (DLN) trained with full-batch gradient descent, and a DLN network with batch normalization layers. Interestingly, it is shown that the standard VGG and DLN models both have higher correlations of gradients compared with their counterparts, indicating that the additional batch normalization layers are not reducing internal covariate shift.
Even though batchnorm was originally introduced to alleviate gradient vanishing or explosion problems, a deep batchnorm network in fact suffers from gradient explosion at initialization time, no matter what it uses for nonlinearity. Thus the optimization landscape is very far from smooth for a randomly initialized, deep batchnorm network.
More precisely, if the network has
layers, then the gradient of the first layer weights has norm


depending only on the nonlinearity.
For any fixed nonlinearity,

decreases as the batch size increases. For example, for ReLU,


as the batch size tends to infinity.
Practically, this means deep batchnorm networks are untrainable.
This is only relieved by skip connections in the fashion of residual networks.
This gradient explosion on the surface contradicts the smoothness property explained in the previous section, but in fact they are consistent. The previous section studies the effect of inserting a single batchnorm in a network, while the gradient explosion depends on stacking batchnorms typical of modern deep neural networks.
Another possible reason for the success of batch normalization is that it decouples the length and direction of the weight vectors and thus facilitates better training.
By interpreting batch norm as a reparametrization of weight space, it can be shown that the length and the direction of the weights are separated and can thus be trained separately. For a particular neural network unit with input
and weight vector
, denote its output as
[\phi (x^w)]

is the activation function, and denote
. Assume that
, and that the spectrum of the matrix
is bounded as
(S)
(S)<\infty 
, such that
is symmetric positive definite. Adding batch normalization to this unit thus results in
(w,\gamma ,\beta )=E_[\phi (BN(x^w))]=E_\phi \gamma (w-E_[x^w][x^w]^)+\beta 
, by definition.
The variance term can be simplified such that
. Assume that
has zero mean and

can be omitted, then it follows that
(w,\gamma )=E_\phi \gamma wSw)^
Sw)^
is the induced norm of
Hence, it could be concluded that
(w,\gamma )=E_[\phi (x^)]
=\gamma 

accounts for its length and direction separately. This property could then be used to prove the faster convergence of problems with batch normalization.
With the reparametrization interpretation, it could then be proved that applying batch normalization to the ordinary least squares problem achieves a linear convergence rate in gradient descent, which is faster than the regular gradient descent with only sub-linear convergence.
Denote the objective of minimizing an ordinary least squares problem as
\in R^f_()=min_\in R^(E_[(y-x^)^])=min_\in R^(2u^+^S)
=\gamma 
, the objective thus becomes
\backslash \,\gamma \in Rf_(w,\gamma )=min_\backslash \,\gamma \in R2\gamma w+\gamma ^
, where 0 is excluded to avoid 0 in the denominator.
Since the objective is convex with respect to

, its optimal value could be calculated by setting the partial derivative of the objective against

to 0. The objective could be further simplified to be
\backslash \\rho (w)=min_\backslash \-uu^wSw
Note that this objective is a form of the generalized Rayleigh quotient
(w)=BwAw

is a symmetric matrix and

is a symmetric positive definite matrix.
It is proven that the gradient descent convergence rate of the generalized Rayleigh quotient is
-\rho (w_)-\lambda _)\leq 1--\lambda _-\lambda _^-\rho (w_))-\lambda _

is the largest eigenvalue of

is the second largest eigenvalue of

is the smallest eigenvalue of
In our case,
is a rank one matrix, and the convergence result can be simplified accordingly. Specifically, consider gradient descent steps of the form
=w_-\eta _\triangledown \rho (w_)
with step size
=^Sw_)|
, and starting from
)\neq 0
)-\rho (w^)\leq 1-^(\rho (w_)-\rho (w^))
The problem of learning halfspaces refers to the training of the Perceptron, which is the simplest form of neural network. The optimization problem in this case is
\in R^f_()=E_[\phi (z^)]

is an arbitrary loss function.

is infinitely differentiable and has a bounded derivative. Assume that the objective function

-smooth, and that a solution
=argmin_||\triangledown f(\alpha w)||^
exists and is bounded such that
<\infty 
. Also assume
is a multivariate normal random variable. With the Gaussian assumption, it can be shown that all critical points lie on the same line, for any choice of loss function

. Specifically, the gradient of
could be represented as
f_()=c_()u+c_()S
()=E_[\phi ^(z^)]-E_[\phi ^(z^)](u^)
()=E_[\phi ^(z^)]

-th derivative of

By setting the gradient to 0, it thus follows that the bounded critical points
_
can be expressed as
_=g_S^u
_

. Combining this global property with length-direction decoupling, it could thus be proved that this optimization problem converges linearly.
First, a variation of gradient descent with batch normalization, Gradient Descent in Normalized Parameterization (GDNP), is designed for the objective function
\backslash \,\gamma \in Rf_(w,\gamma )
, such that the direction and length of the weights are updated separately. Denote the stopping criterion of GDNP as
,\gamma _)=E_[\phi '(z^_)](u^w_)-E_[\phi ''(z^_)](u^w_)^
Let the step size be
=s(w_,\gamma _)=-||_^h(w_,\gamma _)
For each step, if
,\gamma _)\neq 0
, then update the direction as
=w_-s_\triangledown _f(w_,\gamma _)
Then update the length according to
=(T_,f,w_)
is the classical bisection algorithm, and
is the total iterations ran in the bisection step.
Denote the total number of iterations as
, then the final output of GDNP is
_=\gamma _||_
The GDNP algorithm thus slightly modifies the batch normalization step for the ease of mathematical analysis.
It can be shown that in GDNP, the partial derivative of
against the length component converges to zero at a linear rate, such that
f_(w_,a_^))^\leq \zeta |b_^-a_^|
are the two starting points of the bisection algorithm on the left and on the right, correspondingly.
Further, for each iteration, the norm of the gradient of
with respect to
converges linearly, such that
||_^||\triangledown f_(w_,g_)||_^\leq 1-^\Phi ^\gamma _^(\rho (w_)-\rho ^)
Combining these two inequalities, a bound could thus be obtained for the gradient with respect to
_
f(_)||^\leq 1-^\Phi ^(\rho (w_)-\rho ^)+\zeta |b_^-a_^|
, such that the algorithm is guaranteed to converge linearly.
Although the proof stands on the assumption of Gaussian input, it is also shown in experiments that GDNP could accelerate optimization without this constraint.
Consider a multilayer perceptron (MLP) with one hidden layer and
hidden units with mapping from input

to a scalar output described as
(,\Theta )=\sum _^\theta _\phi (x^^)
^

are the input and output weights of unit

is the activation function and is assumed to be a tanh function.
The input and output weights could then be optimized with
,\Theta (f_(,\Theta )=E_[l(-yF_(,\Theta ))])
is a loss function,
=\^,...,^\
,...,\theta ^\

and optimizing only

, it can be shown that the critical points of
()
of a particular hidden unit
^
, all align along one line depending on incoming information into the hidden layer, such that
^=^S^u
^\in R
is a scalar,
This result could be proved by setting the gradient of
to zero and solving the system of equations.
Apply the GDNP algorithm to this optimization problem by alternating optimization over the different hidden units. Specifically, for each hidden unit, run GDNP to find the optimal

. With the same choice of stopping criterion and stepsize, it follows that
^f(_^)||_^\leq 1-^C(\rho (w_)-\rho ^)+^\zeta |b_^-a_^|
Since the parameters of each hidden unit converge linearly, the whole optimization problem has a linear rate of convergence.
Ioffe, Sergey; Szegedy, Christian (2015). "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", ICML'15: Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, July 2015 Pages 448–456
Simonyan, Karen; Zisserman, Andrew (2014). "Very Deep Convolutional Networks for Large-Scale Image Recognition". arXiv:1409.1556 [cs.CV]. In computer science and operations research, the ant colony optimization algorithm (ACO) is a probabilistic technique for solving computational problems that can be reduced to finding good paths through graphs. Artificial ants represent multi-agent methods inspired by the behavior of real ants.
The pheromone-based communication of biological ants is often the predominant paradigm used. Combinations of artificial ants and local search algorithms have become a preferred method for numerous optimization tasks involving some sort of graph, e.g., vehicle routing and internet routing.
As an example, ant colony optimization is a class of optimization algorithms modeled on the actions of an ant colony. Artificial 'ants' (e.g. simulation agents) locate optimal solutions by moving through a parameter space representing all possible solutions. Real ants lay down pheromones to direct each other to resources while exploring their environment. The simulated 'ants' similarly record their positions and the quality of their solutions, so that in later simulation iterations more ants locate better solutions. One variation on this approach is the bees algorithm, which is more analogous to the foraging patterns of the honey bee, another social insect.
This algorithm is a member of the ant colony algorithms family, in swarm intelligence methods, and it constitutes some metaheuristic optimizations. Initially proposed by Marco Dorigo in 1992 in his PhD thesis, the first algorithm was aiming to search for an optimal path in a graph, based on the behavior of ants seeking a path between their colony and a source of food. The original idea has since diversified to solve a wider class of numerical problems, and as a result, several problems have emerged, drawing on various aspects of the behavior of ants. From a broader perspective, ACO performs a model-based search and shares some similarities with estimation of distribution algorithms.
In the natural world, ants of some species (initially) wander randomly, and upon finding food return to their colony while laying down pheromone trails. If other ants find such a path, they are likely to stop travelling at random and instead follow the trail, returning and reinforcing it if they eventually find food (see Ant communication).
Over time, however, the pheromone trail starts to evaporate, thus reducing its attractive strength. The more time it takes for an ant to travel down the path and back again, the more time the pheromones have to evaporate. A short path, by comparison, is marched over more frequently, and thus the pheromone density becomes higher on shorter paths than longer ones. Pheromone evaporation also has the advantage of avoiding the convergence to a locally optimal solution. If there were no evaporation at all, the paths chosen by the first ants would tend to be excessively attractive to the following ones. In that case, the exploration of the solution space would be constrained. The influence of pheromone evaporation in real ant systems is unclear, but it is very important in artificial systems.
The overall result is that when one ant finds a good (i.e., short) path from the colony to a food source, other ants are more likely to follow that path, and positive feedback eventually leads to many ants following a single path. The idea of the ant colony algorithm is to mimic this behavior with "simulated ants" walking around the graph representing the problem to be solved.
New concepts are required since “intelligence” is no longer centralized but can be found throughout all minuscule objects. Anthropocentric concepts have been known to lead to the production of IT systems in which data processing, control units and calculating power are centralized. These centralized units have continually increased their performance and can be compared to the human brain. The model of the brain has become the ultimate vision of computers. Ambient networks of intelligent objects and, sooner or later, a new generation of information systems that are even more diffused and based on nanotechnology, will profoundly change this concept. Small devices that can be compared to insects do not possess a high intelligence on their own. Indeed, their intelligence can be classed as fairly limited. It is, for example, impossible to integrate a high performance calculator with the power to solve any kind of mathematical problem into a biochip that is implanted into the human body or integrated in an intelligent tag designed to trace commercial articles. However, once those objects are interconnected they develop a form of intelligence that can be compared to a colony of ants or bees. In the case of certain problems, this type of intelligence can be superior to the reasoning of a centralized system similar to the brain.
Nature offers several examples of how minuscule organisms, if they all follow the same basic rule, can create a form of collective intelligence on the macroscopic level. Colonies of social insects perfectly illustrate this model which greatly differs from human societies. This model is based on the cooperation of independent units with simple and unpredictable behavior. They move through their surrounding area to carry out certain tasks and only possess a very limited amount of information to do so. A colony of ants, for example, represents numerous qualities that can also be applied to a network of ambient objects. Colonies of ants have a very high capacity to adapt themselves to changes in the environment, as well as great strength in dealing with situations where one individual fails to carry out a given task. This kind of flexibility would also be very useful for mobile networks of objects which are perpetually developing. Parcels of information that move from a computer to a digital object behave in the same way as ants would do. They move through the network and pass from one node to the next with the objective of arriving at their final destination as quickly as possible.
Pheromone-based communication is one of the most effective ways of communication which is widely observed in nature. Pheromone is used by social insects such as
bees, ants and termites; both for inter-agent and agent-swarm communications. Due to its feasibility, artificial pheromones have been adopted in multi-robot and swarm robotic systems. Pheromone-based communication was implemented by different means such as chemical or physical (RFID tags, light, sound) ways. However, those implementations were not able to replicate all the aspects of pheromones as seen in nature.
Using projected light was presented in a 2007 IEEE paper by Garnier, Simon, et al. as an experimental setup to study pheromone-based communication with micro autonomous robots. Another study presented a system in which pheromones were implemented via a horizontal LCD screen on which the robots moved, with the robots having downward facing light sensors to register the patterns beneath them.
In the ant colony optimization algorithms, an artificial ant is a simple computational agent that searches for good solutions to a given optimization problem. To apply an ant colony algorithm, the optimization problem needs to be converted into the problem of finding the shortest path on a weighted graph. In the first step of each iteration, each ant stochastically constructs a solution, i.e. the order in which the edges in the graph should be followed. In the second step, the paths found by the different ants are compared. The last step consists of updating the pheromone levels on each edge.
procedure ACO_MetaHeuristic is
while not terminated do
Each ant needs to construct a solution to move through the graph. To select the next edge in its tour, an ant will consider the length of each edge available from its current position, as well as the corresponding pheromone level. At each step of the algorithm, each ant moves from a state
, corresponding to a more complete intermediate solution. Thus, each ant
computes a set
of feasible expansions to its current state in each iteration, and moves to one of these in probability. For ant
, the probability
of moving from state
depends on the combination of two values, the attractiveness

of the move, as computed by some heuristic indicating the a priori desirability of that move and the trail level

of the move, indicating how proficient it has been in the past to make that particular move. The trail level represents a posteriori indication of the desirability of that move.
In general, the
th ant moves from state
^=^)(\eta _^) _(\tau _^)(\eta _^)

is the amount of pheromone deposited for transition from state

≥ 0 is a parameter to control the influence of


is the desirability of state transition
(a priori knowledge, typically
is the distance) and

≥ 1 is a parameter to control the influence of



represent the trail level and attractiveness for the other possible state transitions.
Trails are usually updated when all ants have completed their solution, increasing or decreasing the level of trails corresponding to moves that were part of "good" or "bad" solutions, respectively. An example of a global pheromone updating rule is now
\leftarrow (1-\rho )\tau _+\sum _^\Delta \tau _^

is the amount of pheromone deposited for a state transition

is the pheromone evaporation coefficient,
is the number of ants and
^
is the amount of pheromone deposited by
th ant, typically given for a TSP problem (with moves corresponding to arcs of the graph) by
in its tour
^=Q/L_&kxy\\0&\end
is the cost of the
th ant's tour (typically length) and
is a constant.
Here are some of the most popular variations of ACO algorithms.
The ant system is the first ACO algorithm. This algorithm corresponds to the one presented above. It was developed by Dorigo.
In the ant colony system algorithm, the original ant system was modified in three aspects:
The edge selection is biased towards exploitation (i.e. favoring the probability of selecting the shortest edges with a large amount of pheromone);
While building a solution, ants change the pheromone level of the edges they are selecting by applying a local pheromone updating rule;
At the end of each iteration, only the best ant is allowed to update the trails by applying a modified global pheromone updating rule.
In this algorithm, the global best solution deposits pheromone on its trail after every iteration (even if this trail has not been revisited), along with all the other ants. The elitist strategy has as its objective directing the search of all ants to construct a solution to contain links of the current best route.
This algorithm controls the maximum and minimum pheromone amounts on each trail. Only the global best tour or the iteration best tour are allowed to add pheromone to its trail. To avoid stagnation of the search algorithm, the range of possible pheromone amounts on each trail is limited to an interval [τmax,τmin]. All edges are initialized to τmax to force a higher exploration of solutions. The trails are reinitialized to τmax when nearing stagnation.
All solutions are ranked according to their length. Only a fixed number of the best ants in this iteration are allowed to update their trials. The amount of pheromone deposited is weighted for each solution, such that solutions with shorter paths deposit more pheromone than the solutions with longer paths.
An ant colony system (ACS) with communication strategies is developed. The artificial ants are partitioned into several groups. Seven communication
methods for updating the pheromone level between groups in ACS are proposed and work on the traveling salesman problem.
The pheromone deposit mechanism of COAC is to enable ants to search for solutions collaboratively and effectively. By using an orthogonal design method, ants in the feasible domain can explore their chosen regions rapidly and efficiently, with enhanced global search capability and accuracy. The orthogonal design method and the adaptive radius adjustment method can also be extended to other optimization algorithms for delivering wider advantages in solving practical problems.
It is a recursive form of ant system which divides the whole search domain into several sub-domains and solves the objective on these subdomains. The results from all the subdomains are compared and the best few of them are promoted for the next level. The subdomains corresponding to the selected results are further subdivided and the process is repeated until an output of desired precision is obtained. This method has been tested on ill-posed geophysical inversion problems and works well.
For some versions of the algorithm, it is possible to prove that it is convergent (i.e., it is able to find the global optimum in finite time). The first evidence of convergence for an ant colony algorithm was made in 2000, the graph-based ant system algorithm, and later on for the ACS and MMAS algorithms. Like most metaheuristics, it is very difficult to estimate the theoretical speed of convergence. A performance analysis of a continuous ant colony algorithm with respect to its various parameters (edge selection strategy, distance measure metric, and pheromone evaporation rate) showed that its performance and rate of convergence are sensitive to the chosen parameter values, and especially to the value of the pheromone evaporation rate. In 2004, Zlochin and his colleagues showed that ACO-type algorithms are closely related to stochastic gradient descent, Cross-entropy method and estimation of distribution algorithm. They proposed an umbrella term "Model-based search" to describe this class of metaheuristics.
Ant colony optimization algorithms have been applied to many combinatorial optimization problems, ranging from quadratic assignment to protein folding or routing vehicles and a lot of derived methods have been adapted to dynamic problems in real variables, stochastic problems, multi-targets and parallel implementations.
It has also been used to produce near-optimal solutions to the travelling salesman problem. They have an advantage over simulated annealing and genetic algorithm approaches of similar problems when the graph may change dynamically; the ant colony algorithm can be run continuously and adapt to changes in real time. This is of interest in network routing and urban transportation systems.
The first ACO algorithm was called the ant system and it was aimed to solve the travelling salesman problem, in which the goal is to find the shortest round-trip to link a series of cities. The general algorithm is relatively simple and based on a set of ants, each making one of the possible round-trips along the cities. At each stage, the ant chooses to move from one city to another according to some rules:
It must visit each city exactly once;
A distant city has less chance of being chosen (the visibility);
The more intense the pheromone trail laid out on an edge between two cities, the greater the probability that that edge will be chosen;
Having completed its journey, the ant deposits more pheromones on all edges it traversed, if the journey is short;
After each iteration, trails of pheromones evaporate.
Sequential ordering problem (SOP)
Job-shop scheduling problem (JSP)
Open-shop scheduling problem (OSP)
Permutation flow shop problem (PFSP)
Single machine total tardiness problem (SMTTP)
Single machine total weighted tardiness problem (SMTWTP)
Resource-constrained project scheduling problem (RCPSP)
Group-shop scheduling problem (GSP)
Single-machine total tardiness problem with sequence dependent setup times (SMTTPDST)
Multistage flowshop scheduling problem (MFSP) with sequence dependent setup/changeover times
Assembly sequence planning (ASP) problems
Capacitated vehicle routing problem (CVRP)
Multi-depot vehicle routing problem (MDVRP)
Period vehicle routing problem (PVRP)
Split delivery vehicle routing problem (SDVRP)
Stochastic vehicle routing problem (SVRP)
Vehicle routing problem with pick-up and delivery (VRPPD)
Vehicle routing problem with time windows (VRPTW)
Time dependent vehicle routing problem with time windows (TDVRPTW)
Vehicle routing problem with time windows and multiple service workers (VRPTWMS)
Quadratic assignment problem (QAP)
Generalized assignment problem (GAP)
Frequency assignment problem (FAP)
Redundancy allocation problem (RAP)
Set cover problem (SCP)
Partition problem (SPP)
Weight constrained graph tree partition problem (WCGTPP)
Arc-weighted l-cardinality tree problem (AWlCTP)
Multiple knapsack problem (MKP)
Maximum independent set problem (MIS)
Ant colony optimization (ACO) based optimization of 45 nm CMOS-based sense amplifier circuit could converge to optimal solutions in very minimal time.
Ant colony optimization (ACO) based reversible circuit synthesis could improve efficiency significantly.
To optimize the form of antennas, ant colony algorithms can be used. As example can be considered antennas RFID-tags based on ant colony algorithms (ACO), loopback and unloopback vibrators 10×10
The ACO algorithm is used in image processing for image edge detection and edge linking.
The graph here is the 2-D image and the ants traverse from one pixel depositing pheromone. The movement of ants from one pixel to another is directed by the local variation of the image's intensity values. This movement causes the highest density of the pheromone to be deposited at the edges.
The following are the steps involved in edge detection using ACO:
Step 1: Initialization. Randomly place
ants on the image
*M_)^
. Pheromone matrix

are initialized with a random value. The major challenge in the initialization process is determining the heuristic matrix.
There are various methods to determine the heuristic matrix. For the below example the heuristic matrix was calculated based on the local statistics:
the local statistics at the pixel position
=*Vc*I_,
is the image of size
\sum _Vc(I_)
is a normalization factor, and
Vc(I_)=&f\left(\left\vert I_-I_\right\vert +\left\vert I_-I_\right\vert \right.\\&+\left\vert I_-I_\right\vert +\left\vert I_-I_\right\vert \\&+\left\vert I_-I_\right\vert +\left\vert I_-I_\right\vert \\&+\left.\left\vert I_-I_\right\vert +\left\vert I_-I_\right\vert \right)\end

can be calculated using the following functions:
for x ≥ 0; (1)

for x ≥ 0; (2)
,\quad 
for 0 ≤ x ≤
\sin(),&\lambda \\0,&\end
for 0 ≤ x ≤
\pi x\sin(),&\lambda \\0,&\end

in each of above functions adjusts the functions’ respective shapes.
Step 2: Construction process. The ant's movement is based on 4-connected pixels or 8-connected pixels. The probability with which the ant moves is given by the probability equation
Step 3 and step 5: Update process. The pheromone matrix is updated twice. in step 3 the trail of the ant (given by

) is updated where as in step 5 the evaporation rate of the trail is updated which is given by:
\leftarrow (1-\psi )\tau _+\psi \tau _

is the pheromone decay coefficient

Step 7: Decision process. Once the K ants have moved a fixed distance L for N iteration, the decision whether it is an edge or not is based on the threshold T on the pheromone matrix τ. Threshold for the below example is calculated based on Otsu's method.
Image edge detected using ACO: The images below are generated using different functions given by the equation (1) to (4).
Edge linking: ACO has also proven effective in edge linking algorithms.
Connection-oriented network routing
Connectionless network routing
Discounted cash flows in project scheduling
Distributed information retrieval
Energy and electricity network design
Grid workflow scheduling problem
Inhibitory peptide design for protein protein interactions
Intelligent testing system
Power electronic circuit design
With an ACO algorithm, the shortest path in a graph, between two points A and B, is built from a combination of several paths. It is not easy to give a precise definition of what algorithm is or is not an ant colony, because the definition may vary according to the authors and uses. Broadly speaking, ant colony algorithms are regarded as populated metaheuristics with each solution represented by an ant moving in the search space. Ants mark the best solutions and take account of previous markings to optimize their search. They can be seen as probabilistic multi-agent algorithms using a probability distribution to make the transition between each iteration. In their versions for combinatorial problems, they use an iterative construction of solutions. According to some authors, the thing which distinguishes ACO algorithms from other relatives (such as algorithms to estimate the distribution or particle swarm optimization) is precisely their constructive aspect. In combinatorial problems, it is possible that the best solution eventually be found, even though no ant would prove effective. Thus, in the example of the travelling salesman problem, it is not necessary that an ant actually travels the shortest route: the shortest route can be built from the strongest segments of the best solutions. However, this definition can be problematic in the case of problems in real variables, where no structure of 'neighbours' exists. The collective behaviour of social insects remains a source of inspiration for researchers. The wide variety of algorithms (for optimization or not) seeking self-organization in biological systems has led to the concept of "swarm intelligence", which is a very general framework in which ant colony algorithms fit.
There is in practice a large number of algorithms claiming to be "ant colonies", without always sharing the general framework of optimization by canonical ant colonies. In practice, the use of an exchange of information between ants via the environment (a principle called "stigmergy") is deemed enough for an algorithm to belong to the class of ant colony algorithms. This principle has led some authors to create the term "value" to organize methods and behavior based on search of food, sorting larvae, division of labour and cooperative transportation.
Genetic algorithms (GA)
These maintain a pool of solutions rather than just one. The process of finding superior solutions mimics that of evolution, with solutions being combined or mutated to alter the pool of solutions, with solutions of inferior quality being discarded.
Estimation of distribution algorithm (EDA)
An evolutionary algorithm that substitutes traditional reproduction operators by model-guided operators. Such models are learned from the population by employing machine learning techniques and represented as probabilistic graphical models, from which new solutions can be sampled or generated from guided-crossover.
Simulated annealing (SA)
A related global optimization technique which traverses the search space by generating neighboring solutions of the current solution. A superior neighbor is always accepted. An inferior neighbor is accepted probabilistically based on the difference in quality and a temperature parameter. The temperature parameter is modified as the algorithm progresses to alter the nature of the search.
Reactive search optimization
Focuses on combining machine learning with optimization, by adding an internal feedback loop to self-tune the free parameters of an algorithm to the characteristics of the problem, of the instance, and of the local situation around the current solution.
Tabu search (TS)
Similar to simulated annealing in that both traverse the solution space by testing mutations of an individual solution. While simulated annealing generates only one mutated solution, tabu search generates many mutated solutions and moves to the solution with the lowest fitness of those generated. To prevent cycling and encourage greater movement through the solution space, a tabu list is maintained of partial or complete solutions. It is forbidden to move to a solution that contains elements of the tabu list, which is updated as the solution traverses the solution space.
Artificial immune system (AIS)
Modeled on vertebrate immune systems.
Particle swarm optimization (PSO)
A swarm intelligence method.
Intelligent water drops (IWD)
A swarm-based optimization algorithm based on natural water drops flowing in rivers
Gravitational search algorithm (GSA)
A swarm intelligence method.
Ant colony clustering method (ACCM)
A method that make use of clustering approach, extending the ACO.
Stochastic diffusion search (SDS)
An agent-based probabilistic global search and optimization technique best suited to problems where the objective function can be decomposed into multiple independent partial-functions.
Chronology of ant colony optimization algorithms.
1959, Pierre-Paul Grassé invented the theory of stigmergy to explain the behavior of nest building in termites;
1983, Deneubourg and his colleagues studied the collective behavior of ants;
1988, and Moyson Manderick have an article on self-organization among ants;
1989, the work of Goss, Aron, Deneubourg and Pasteels on the collective behavior of Argentine ants, which will give the idea of ant colony optimization algorithms;
1989, implementation of a model of behavior for food by Ebling and his colleagues;
1991, M. Dorigo proposed the ant system in his doctoral thesis (which was published in 1992). A technical report extracted from the thesis and co-authored by V. Maniezzo and A. Colorni was published five years later;
1994, Appleby and Steward of British Telecommunications Plc published the first application to telecommunications networks
1995, Gambardella and Dorigo proposed ant-q, the preliminary version of ant colony system as first extension of ant system;.
1996, Gambardella and Dorigo proposed ant colony system
1996, publication of the article on ant system;
1997, Dorigo and Gambardella proposed ant colony system hybridized with local search;
1997, Schoonderwoerd and his colleagues published an improved application to telecommunication networks;
1998, Dorigo launches first conference dedicated to the ACO algorithms;
1998, Stützle proposes initial parallel implementations;
1999, Gambardella, Taillard and Agazzi proposed macs-vrptw, first multi ant colony system applied to vehicle routing problems with time windows,
1999, Bonabeau, Dorigo and Theraulaz publish a book dealing mainly with artificial ants
2000, special issue of the Future Generation Computer Systems journal on ant algorithms
2000, Hoos and Stützle invent the max-min ant system;
2000, first applications to the scheduling, scheduling sequence and the satisfaction of constraints;
2000, Gutjahr provides the first evidence of convergence for an algorithm of ant colonies
2001, the first use of COA algorithms by companies (Eurobios and AntOptima);
2001, Iredi and his colleagues published the first multi-objective algorithm
2002, first applications in the design of schedule, Bayesian networks;
2002, Bianchi and her colleagues suggested the first algorithm for stochastic problem;
2004, Dorigo and Stützle publish the Ant Colony Optimization book with MIT Press
2004, Zlochin and Dorigo show that some algorithms are equivalent to the stochastic gradient descent, the cross-entropy method and algorithms to estimate distribution
2005, first applications to protein folding problems.
2012, Prabhakar and colleagues publish research relating to the operation of individual ants communicating in tandem without pheromones, mirroring the principles of computer network organization. The communication model has been compared to the Transmission Control Protocol.
2016, first application to peptide sequence design.
2017, successful integration of the multi-criteria decision-making method PROMETHEE into the ACO algorithm (HUMANT algorithm).
M. Dorigo, 1992. Optimization, Learning and Natural Algorithms, PhD thesis, Politecnico di Milano, Italy.
M. Dorigo, V. Maniezzo & A. Colorni, 1996. "Ant System: Optimization by a Colony of Cooperating Agents", IEEE Transactions on Systems, Man, and Cybernetics–Part B, 26 (1): 29–41.
M. Dorigo & L. M. Gambardella, 1997. "Ant Colony System: A Cooperative Learning Approach to the Traveling Salesman Problem". IEEE Transactions on Evolutionary Computation, 1 (1): 53–66.
M. Dorigo, G. Di Caro & L. M. Gambardella, 1999. "Ant Algorithms for Discrete Optimization Archived 2018-10-06 at the Wayback Machine". Artificial Life, 5 (2): 137–172.
E. Bonabeau, M. Dorigo et G. Theraulaz, 1999. Swarm Intelligence: From Natural to Artificial Systems, Oxford University Press. ISBN 0-19-513159-2
M. Dorigo & T. Stützle, 2004. Ant Colony Optimization, MIT Press. ISBN 0-262-04219-3
M. Dorigo, 2007. "Ant Colony Optimization". Scholarpedia.
C. Blum, 2005 "Ant colony optimization: Introduction and recent trends". Physics of Life Reviews, 2: 353-373
M. Dorigo, M. Birattari & T. Stützle, 2006 Ant Colony Optimization: Artificial Ants as a Computational Intelligence Technique. TR/IRIDIA/2006-023
Mohd Murtadha Mohamad,"Articulated Robots Motion Planning Using Foraging Ant Strategy", Journal of Information Technology - Special Issues in Artificial Intelligence, Vol. 20, No. 4 pp. 163–181, December 2008, ISSN 0128-3790.
N. Monmarché, F. Guinand & P. Siarry (eds), "Artificial Ants", August 2010 Hardback 576 pp. ISBN 978-1-84821-194-0.
A. Kazharov, V. Kureichik, 2010. "Ant colony optimization algorithms for solving transportation problems", Journal of Computer and Systems Sciences International, Vol. 49. No. 1. pp. 30–43.
C-M. Pintea, 2014, Advances in Bio-inspired Computing for Combinatorial Optimization Problem, Springer ISBN 978-3-642-40178-7
K. Saleem, N. Fisal, M. A. Baharudin, A. A. Ahmed, S. Hafizah and S. Kamilah, "Ant colony inspired self-optimized routing protocol based on cross layer architecture for wireless sensor networks", WSEAS Trans. Commun., vol. 9, no. 10, pp. 669–678, 2010. ISBN 978-960-474-200-4
K. Saleem and N. Fisal, "Enhanced Ant Colony algorithm for self-optimized data assured routing in wireless sensor networks", Networks (ICON) 2012 18th IEEE International Conference on, pp. 422–427. ISBN 978-1-4673-4523-1
Abolmaali S, Roodposhti FR. Portfolio Optimization Using Ant Colony Method a Case Study on Tehran Stock Exchange. Journal of Accounting. 2018 Mar;8(1).
Scholarpedia Ant Colony Optimization page
"Ant Colony Optimization" - Russian scientific and research community
AntSim - Simulation of Ant Colony Algorithms
MIDACO-Solver General purpose optimization software based on ant colony optimization (Matlab, Excel, VBA, C/C++, R, C#, Java, Fortran and Python)
University of Kaiserslautern, Germany, AG Wehn: Ant Colony Optimization Applet Visualization of Traveling Salesman solved by ant system with numerous options and parameters (Java Applet)
Ant algorithm simulation (Java Applet)
Ant Colony Optimization Algorithm Implementation (Python Notebook) Quantum optimization algorithms are quantum algorithms that are used to solve optimization problems. Mathematical optimization deals with finding the best solution to a problem (according to some criteria) from a set of possible solutions. Mostly, the optimization problem is formulated as a minimization problem, where one tries to minimize an error which depends on the solution: the optimal solution has the minimal error. Different optimization techniques are applied in various fields such as mechanics, economics and engineering, and as the complexity and amount of data involved rise, more efficient ways of solving optimization problems are needed. Quantum computing may allow problems which are not practically feasible on classical computers to be solved, or suggest a considerable speed up with respect to the best known classical algorithm.
Data fitting is a process of constructing a mathematical function that best fits a set of data points. The fit's quality is measured by some criteria, usually the distance between the function and the data points.
One of the most common types of data fitting is solving the least squares problem, minimizing the sum of the squares of differences between the data points and the fitted function.
The algorithm is given
input data points
. The algorithm finds and gives as output a continuous function

that is a linear combination of
(x)=\sum _^f_(x)\lambda _
In other words, the algorithm finds the complex coefficients

, and thus the vector
=(\lambda _,\lambda _,...,\lambda _)
The algorithm is aimed at minimizing the error, which is given by:
^\left\vert f_(x_)-y_\right\vert ^=\sum _^\left\vert \sum _^f_(x_)\lambda _-y_\right\vert ^=\left\vert F-\right\vert ^
is defined to be the following matrix:
=f_(x_)&\cdots &f_(x_)\\f_(x_)&\cdots &f_(x_)\\\vdots &\ddots &\vdots \\f_(x_)&\cdots &f_(x_)\\\end
The quantum least-squares fitting algorithm makes use of a version of Harrow, Hassidim, and Lloyd's quantum algorithm for linear systems of equations (HHL), and outputs the coefficients

and the fit quality estimation
. It consists of three subroutines: an algorithm for performing a pseudo-inverse operation, one routine for the fit quality estimation, and an algorithm for learning the fit parameters.
Because the quantum algorithm is mainly based on the HHL algorithm, it suggests an exponential improvement in the case where
is sparse and the condition number (namely, the ratio between the largest and the smallest eigenvalues) of both

F
Semidefinite programming (SDP) is an optimization subfield dealing with the optimization of a linear objective function (a user-specified function to be minimized or maximized), over the intersection of the cone of positive semidefinite matrices with an affine space. The objective function is an inner product of a matrix
(given as an input) with the variable
. Denote by
 ^
the space of all

symmetric matrices. The variable
must lie in the (closed convex) cone of positive semidefinite symmetric matrices
 _^
. The inner product of two matrices is defined as:
 ^=(A^B)=\sum _^A_B_.
The problem may have additional constraints (given as inputs), also usually formulated as inner products. Each constraint forces the inner product of the matrices
(given as an input) with the optimization variable
to be smaller than a specified value
(given as an input). Finally, the SDP problem can be written as:
 ^&\langle C,X\rangle _ ^\\&\langle A_,X\rangle _ ^\leq b_,\quad k=1,\ldots ,m\\&X\succeq 0\end
The best classical algorithm is not known to unconditionally run in polynomial time. The corresponding feasibility problem is known to either lie outside of the union of the complexity classes NP and co-NP, or in the intersection of NP and co-NP.
The algorithm inputs are
and parameters regarding the solution's trace, precision and optimal value (the objective function's value at the optimal point).
The quantum algorithm consists of several iterations. In each iteration, it solves a feasibility problem, namely, finds any solution satisfying the following conditions (giving a threshold
\langle C,X\rangle _ ^\leq t\\\langle A_,X\rangle _ ^\leq b_,\quad k=1,\ldots ,m\\X\succeq 0\end
In each iteration, a different threshold
is chosen, and the algorithm outputs either a solution
 ^\leq t
(and the other constraints are satisfied, too) or an indication that no such solution exists. The algorithm performs a binary search to find the minimal threshold
for which a solution
still exists: this gives the minimal solution to the SDP problem.
The quantum algorithm provides a quadratic improvement over the best classical algorithm in the general case, and an exponential improvement when the input matrices are of low rank.
The combinatorial optimization problem is aimed at finding an optimal object from a finite set of objects. The problem can be phrased as a maximization of an objective function which is a sum of Boolean functions. Each Boolean function
\colon \lbrace ^\rightarrow \lbrace \rbrace 
gets as input the
z_\ldots z_
and gives as output one bit (0 or 1). The combinatorial optimization problem of
clauses is finding an
that maximizes the function
^C_(z)
Approximate optimization is a way of finding an approximate solution to an optimization problem, which is often NP-hard. The approximated solution of the combinatorial optimization problem is a string
that is close to maximizing
For combinatorial optimization, the quantum approximate optimization algorithm (QAOA) briefly had a better approximation ratio than any known polynomial time classical algorithm (for a certain problem), until a more effective classical algorithm was proposed. The relative speed-up of the quantum algorithm is an open research question.
QAOA consists of the following steps:
Defining a cost Hamiltonian
such that its ground state encodes the solution to the optimization problem.
Defining a mixer Hamiltonian
Defining the oracles
(\gamma )=\exp(-\imath \gamma H_)
(\alpha )=\exp(-\imath \alpha H_)
, with parameters

Repeated application of the oracles
, in the order:
,)=\coprod _^(U_(\gamma _)U_(\alpha _))
Preparing an initial state, that is a superposition of all possible states and apply
,)
to the state.
Using classical methods to optimize the parameters
,
and measure the output state of the optimized circuit to obtain the approximate optimal solution to the cost Hamiltonian. An optimal solution will be one that maximizes the expectation value of the cost Hamiltonian
The layout of the algorithm, viz, the use of cost and mixer Hamiltonians are inspired from the Quantum Adiabatic theorem, which states that starting in a ground state of a time-dependent Hamiltonian, if the Hamiltonian evolves slowly enough, the final state will be a ground state of the final Hamiltonian. Moreover, the adiabatic theorem can be generalized to any other eigenstate as long as there is no overlap (degeneracy) between different eigenstates across the evolution. Identifying the initial Hamiltonian with
and the final Hamiltonian with
, whose ground states encode the solution to the optimization problem of interest, one can approximate the optimization problem as the adiabatic evolution of the Hamiltonian from an initial to the final one, whose ground (eigen)state gives the optimal solution. In general, QAOA relies on the use of unitary operators dependent on
angles (parameters), where
is an input integer, which can be identified the number of layers of the oracle
,)
. These operators are iteratively applied on a state that is an equal-weighted quantum superposition of all the possible states in the computational basis. In each iteration, the state is measured in the computational basis and the Boolean function
is estimated. The angles are then updated classically to increase
. After this procedure is repeated a sufficient number of times, the value of
is almost optimal, and the state being measured is close to being optimal as well. A sample circuit that implements QAOA on a quantum computer is given in figure. This procedure is highlighted using the following example of finding the minimum vertex cover of a graph.
The goal here is to find a minimum vertex cover of a graph: a collection of vertices such that each edge in the graph contains at least one of the vertices in the cover. Hence, these vertices “cover” all the edges. We wish to find a vertex cover that has the smallest possible number of vertices. Vertex covers can be represented by a bit string where each bit denotes whether the corresponding vertex is present in the cover. For example, the bit string 0101 represents a cover consisting of the second and fourth vertex in a graph with four vertices.
Consider the graph given in the figure. It has four vertices and there are two minimum vertex cover for this graph: vertices 0 and 2, and the vertices 1 and 2. These can be respectively represented by the bit strings 1010 and 0110. The goal of the algorithm is to sample these bit strings with high probability. In this case, the cost Hamiltonian has two ground states, |1010⟩ and |0110⟩, coinciding with the solutions of the problem. The mixer Hamiltonian is the simple, non-commuting sum of Pauli-X operations on each node of the graph and they are given by:
Implementing QAOA algorithm for this four qubit circuit with two layers of the ansatz in qiskit (see figure) and optimizing leads to a probability distribution for the states given in the figure. This shows that the states |0110⟩ and |1010⟩ have the highest probabilities of being measured, just as expected.
In principle the optimal value of
can be reached up to arbitrary precision, this is guaranteed by the adiabatic theorem or alternatively by the universality of the QAOA unitaries. However, it is an open question whether this can be done in a feasible way.
For example, it was shown that QAOA exhibits a strong dependence on the ratio of a problem's constraint to variables (problem density) placing a limiting restriction on the algorithm's capacity to minimize a corresponding objective function.
It was soon recognized that a generalization of the QAOA process is essentially an alternating application of a continuous-time quantum walk on an underlying graph followed by a quality-dependent phase shift applied to each solution state. This generalized QAOA was termed as QWOA (Quantum Walk-based Optimisation Algorithm).
In the paper How many qubits are needed for quantum computational supremacy submitted to arXiv, the authors conclude that a QAOA circuit with 420 qubits and 500 constraints would require at least one century to be simulated using a classical simulation algorithm running on state-of-the-art supercomputers so that would be sufficient for quantum computational supremacy.
A rigorous comparison of QAOA with classical algorithms can give estimates on depth
and number of qubits required for quantum advantage. A study of QAOA and MaxCut algorithm shows that
is required for scalable advantage.
Several variations to the basic structure of QAOA have been proposed, which include variations to the ansatz of the basic algorithm. The choice of ansatz typically depends on the problem type, such as combinatorial problems represented as graphs, or problems strongly influenced by hardware design. However, ansatz design must balance specificity and generality to avoid overfitting and maintain applicability to a wide range of problems. For this reason, designing optimal ansatze for QAOA is an extensively researched and widely investigated topic. Some of the proposed variants are:
Expressive QAOA (XQAOA)
Digitised counteradiabatic QAOA
Quantum alternating operator ansatz,which allows for constrains on the optimization problem etc.
Another variation of QAOA focuses on techniques for parameter optimization, which aims at selecting the optimal set of initial parameters for a given problem and avoiding barren plateaus, which represent parameters leading to eigenstates which correspond to plateaus in the energy landscape of the cost Hamiltonian.
Finally, there has been significant research interest in leveraging specific hardware to enhance the performance of QAOA across various platforms, such as trapped ion, neutral atoms, superconducting qubits, and photonic quantum computers. The goals of these approaches include overcoming hardware connectivity limitations and mitigating noise-related issues to broaden the applicability of QAOA to a wide range of combinatorial optimization problems.
The quantum circuit shown here is from a simple example of how the QAOA algorithm can be implemented in Python using Qiskit, an open-source quantum computing software development framework by IBM.
Adiabatic quantum computation
Implementation of the QAOA algorithm for the knapsack problem with Classiq Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in exchange for a lower convergence rate.
The basic idea behind stochastic approximation can be traced back to the Robbins–Monro algorithm of the 1950s. Today, stochastic gradient descent has become an important optimization method in machine learning.
Both statistical estimation and machine learning consider the problem of minimizing an objective function that has the form of a sum:
\sum _^Q_(w),
where the parameter
is to be estimated. Each summand function
is typically associated with the
-th observation in the data set (used for training).
In classical statistics, sum-minimization problems arise in least squares and in maximum-likelihood estimation (for independent observations). The general class of estimators that arise as minimizers of sums are called M-estimators. However, in statistics, it has been long recognized that requiring even local minimization is too restrictive for some problems of maximum-likelihood estimation. Therefore, contemporary statistical theorists often consider stationary points of the likelihood function (or zeros of its derivative, the score function, and other estimating equations).
The sum-minimization problem also arises for empirical risk minimization. There,
is the value of the loss function at
-th example, and
is the empirical risk.
When used to minimize the above function, a standard (or "batch") gradient descent method would perform the following iterations:
\sum _^\nabla Q_(w).
The step size is denoted by

(sometimes called the learning rate in machine learning) and here "
" denotes the update of a variable in the algorithm.
In many cases, the summand functions have a simple form that enables inexpensive evaluations of the sum-function and the sum gradient. For example, in statistics, one-parameter exponential families allow economical function-evaluations and gradient-evaluations.
However, in other cases, evaluating the sum-gradient may require expensive evaluations of the gradients from all summand functions. When the training set is enormous and no simple formulas exist, evaluating the sums of gradients becomes very expensive, because evaluating the gradient requires evaluating all the summand functions' gradients. To economize on the computational cost at every iteration, stochastic gradient descent samples a subset of summand functions at every step. This is very effective in the case of large-scale machine learning problems.
In stochastic (or "on-line") gradient descent, the true gradient of
is approximated by a gradient at a single sample:
(w).
As the algorithm sweeps through the training set, it performs the above update for each training sample. Several passes can be made over the training set until the algorithm converges. If this is done, the data can be shuffled for each pass to prevent cycles. Typical implementations may use an adaptive learning rate so that the algorithm converges.
In pseudocode, stochastic gradient descent can be presented as :
A compromise between computing the true gradient and the gradient at a single sample is to compute the gradient against more than one training sample (called a "mini-batch") at each step. This can perform significantly better than "true" stochastic gradient descent described, because the code can make use of vectorization libraries rather than computing each step separately as was first shown in where it was called "the bunch-mode back-propagation algorithm". It may also result in smoother convergence, as the gradient computed at each step is averaged over more training samples.
The convergence of stochastic gradient descent has been analyzed using the theories of convex minimization and of stochastic approximation. Briefly, when the learning rates

decrease with an appropriate rate,
and subject to relatively mild assumptions, stochastic gradient descent converges almost surely to a global minimum
when the objective function is convex or pseudoconvex,
and otherwise converges almost surely to a local minimum.
This is in fact a consequence of the Robbins–Siegmund theorem.
Suppose we want to fit a straight line
=w_+w_x
to a training set with observations
,y_),(x_,y_)\ldots ,(x_,y_))
and corresponding estimated responses
_,_,\ldots ,_)
using least squares. The objective function to be minimized is
^Q_(w)=\sum _^\left(_-y_\right)^=\sum _^\left(w_+w_x_-y_\right)^.
The last line in the above pseudocode for this specific problem will become:
w_\\w_\end\leftarrow w_\\w_\end-\eta (w_+w_x_-y_)^\\(w_+w_x_-y_)^\end=w_\\w_\end-\eta 2(w_+w_x_-y_)\\2x_(w_+w_x_-y_)\end.
Note that in each iteration or update step, the gradient is only evaluated at a single
. This is the key difference between stochastic gradient descent and batched gradient descent.
In general, given a linear regression
=\sum _w_x_
problem, stochastic gradient descent behaves differently when

(overparameterized). In the overparameterized case, stochastic gradient descent converges to
x_=y_\forall k\in 1:n\|w-w_\|
. That is, SGD converges to the interpolation solution with minimum distance from the starting
. This is true even when the learning rate remains constant. In the underparameterized case, SGD does not converge if learning rate remains constant.
In 1951, Herbert Robbins and Sutton Monro introduced the earliest stochastic approximation methods, preceding stochastic gradient descent. Building on this work one year later, Jack Kiefer and Jacob Wolfowitz published an optimization algorithm very close to stochastic gradient descent, using central differences as an approximation of the gradient. Later in the 1950s, Frank Rosenblatt used SGD to optimize his perceptron model, demonstrating the first applicability of stochastic gradient descent to neural networks.
Backpropagation was first described in 1986, with stochastic gradient descent being used to efficiently optimize parameters across neural networks with multiple hidden layers. Soon after, another improvement was developed: mini-batch gradient descent, where small batches of data are substituted for single samples. In 1997, the practical performance benefits from vectorization achievable with such small batches were first explored, paving the way for efficient optimization in machine learning. As of 2023, this mini-batch approach remains the norm for training neural networks, balancing the benefits of stochastic gradient descent with gradient descent.
By the 1980s, momentum had already been introduced, and was added to SGD optimization techniques in 1986. However, these optimization techniques assumed constant hyperparameters, i.e. a fixed learning rate and momentum parameter. In the 2010s, adaptive approaches to applying SGD with a per-parameter learning rate were introduced with AdaGrad (for "Adaptive Gradient") in 2011 and RMSprop (for "Root Mean Square Propagation") in 2012. In 2014, Adam (for "Adaptive Moment Estimation") was published, applying the adaptive approaches of RMSprop to momentum; many improvements and branches of Adam were then developed such as Adadelta, Adagrad, AdamW, and Adamax.
Within machine learning, approaches to optimization in 2023 are dominated by Adam-derived optimizers. TensorFlow and PyTorch, by far the most popular machine learning libraries, as of 2023 largely only include Adam-derived optimizers, as well as predecessors to Adam such as RMSprop and classic SGD. PyTorch also partially supports Limited-memory BFGS, a line-search method, but only for single-device setups without parameter groups.
Stochastic gradient descent is a popular algorithm for training a wide range of models in machine learning, including (linear) support vector machines, logistic regression (see, e.g., Vowpal Wabbit) and graphical models. When combined with the back propagation algorithm, it is the de facto standard algorithm for training artificial neural networks. Its use has been also reported in the Geophysics community, specifically to applications of Full Waveform Inversion (FWI).
Stochastic gradient descent competes with the L-BFGS algorithm, which is also widely used. Stochastic gradient descent has been used since at least 1960 for training linear regression models, originally under the name ADALINE.
Another stochastic gradient descent algorithm is the least mean squares (LMS) adaptive filter.
Many improvements on the basic stochastic gradient descent algorithm have been proposed and used. In particular, in machine learning, the need to set a learning rate (step size) has been recognized as problematic. Setting this parameter too high can cause the algorithm to diverge; setting it too low makes it slow to converge. A conceptually simple extension of stochastic gradient descent makes the learning rate a decreasing function ηt of the iteration number t, giving a learning rate schedule, so that the first iterations cause large changes in the parameters, while the later ones do only fine-tuning. Such schedules have been known since the work of MacQueen on k-means clustering. Practical guidance on choosing the step size in several variants of SGD is given by Spall.
As mentioned earlier, classical stochastic gradient descent is generally sensitive to learning rate η. Fast convergence requires large learning rates but this may induce numerical instability. The problem can be largely solved by considering implicit updates whereby the stochastic gradient is evaluated at the next iterate rather than the current one:
:=w^-\eta \,\nabla Q_(w^).
This equation is implicit since
appears on both sides of the equation. It is a stochastic form of the proximal gradient method since the update
can also be written as:
:=\arg \min _\left\(w)+\left\|w-w^\right\|^\right\.
As an example,
consider least squares with features
,\ldots ,x_\in \mathbb  ^
,\ldots ,y_\in \mathbb  
. We wish to solve:
\sum _^\left(y_-x_'w\right)^,
indicates the inner product.
could have "1" as the first element to include an intercept. Classical stochastic gradient descent proceeds as follows:
=w^+\eta \left(y_-x_'w^\right)x_
is uniformly sampled between 1 and
. Although theoretical convergence of this procedure happens under relatively mild assumptions, in practice the procedure can be quite unstable. In particular, when

is misspecified so that
x_'
has large absolute eigenvalues with high probability, the procedure may diverge numerically within a few iterations. In contrast, implicit stochastic gradient descent (shortened as ISGD) can be solved in closed-form as:
=w^+\right\|^\left(y_-x_'w^\right)x_.
This procedure will remain numerically stable virtually for all

as the learning rate is now normalized. Such comparison between classical and implicit stochastic gradient descent in the least squares problem is very similar to the comparison between least mean squares (LMS) and
normalized least mean squares filter (NLMS).
Even though a closed-form solution for ISGD is only possible in least squares, the procedure can be efficiently implemented in a wide range of models. Specifically, suppose that
only through a linear combination with features
, so that we can write
Q_(w)=-q(x_'w)x_
 
may depend on
as well but not on
. Least squares obeys this rule, and so does logistic regression, and most generalized linear models. For instance, in least squares,
, and in logistic regression
is the logistic function. In Poisson regression,
, and so on.
In such settings, ISGD is simply implemented as follows. Let
'w^+\xi \|x_\|^)

Then, ISGD is equivalent to:
=w^+\xi ^x_,~~\xi ^=f(\xi ^).
The scaling factor
\in \mathbb  
can be found through the bisection method since in most regular models, such as the aforementioned generalized linear models, function
is decreasing, and thus the search bounds for

Further proposals include the momentum method or the heavy ball method, which in ML context appeared in Rumelhart, Hinton and Williams' paper on backpropagation learning and borrowed the idea from Soviet mathematician Boris Polyak's 1964 article on solving functional equations. Stochastic gradient descent with momentum remembers the update Δw at each iteration, and determines the next update as a linear combination of the gradient and the previous update:
(w)

that leads to:
(w)+\alpha \Delta w
where the parameter
is to be estimated,

is a step size (sometimes called the learning rate in machine learning) and

is an exponential decay factor between 0 and 1 that determines the relative contribution of the current gradient and earlier gradients to the weight change.
The name momentum stems from an analogy to momentum in physics: the weight vector
, thought of as a particle traveling through parameter space, incurs acceleration from the gradient of the loss ("force"). Unlike in classical stochastic gradient descent, it tends to keep traveling in the same direction, preventing oscillations. Momentum has been used successfully by computer scientists in the training of artificial neural networks for several decades.
The momentum method is closely related to underdamped Langevin dynamics, and may be combined with simulated annealing.
In mid-1980s the method was modified by Yurii Nesterov to use the gradient predicted at the next point, and the resulting so-called Nesterov Accelerated Gradient was sometimes used in ML in the 2010s.
Averaged stochastic gradient descent, invented independently by Ruppert and Polyak in the late 1980s, is ordinary stochastic gradient descent that records an average of its parameter vector over time. That is, the update is the same as for ordinary stochastic gradient descent, but the algorithm also keeps track of
=\sum _^w_.
When optimization is done, this averaged parameter vector takes the place of w.
AdaGrad (for adaptive gradient algorithm) is a modified stochastic gradient descent algorithm with per-parameter learning rate, first published in 2011. Informally, this increases the learning rate for sparser parameters and decreases the learning rate for ones that are less sparse. This strategy often improves convergence performance over standard stochastic gradient descent in settings where data is sparse and sparse parameters are more informative. Examples of such applications include natural language processing and image recognition.
It still has a base learning rate η, but this is multiplied with the elements of a vector  which is the diagonal of the outer product matrix
^g_g_^
=\nabla Q_(w)
, the gradient, at iteration τ. The diagonal is given by
=\sum _^g_^.
This vector essentially stores a historical sum of gradient squares by dimension and is updated after every iteration. The formula for an update is now
 (G)^\odot g
or, written as per-parameter updates,
:=w_-g_.
Each  gives rise to a scaling factor for the learning rate that applies to a single parameter wi. Since the denominator in this factor,
=^g_^
is the ℓ2 norm of previous derivatives, extreme parameter updates get dampened, while parameters that get few or small updates receive higher learning rates.
While designed for convex problems, AdaGrad has been successfully applied to non-convex optimization.
RMSProp (for Root Mean Square Propagation) is a method invented in 2012 by James Martens and Ilya Sutskever, at the time both PhD students in Geoffrey Hinton's group, in which the learning rate is, like in Adagrad, adapted for each of the parameters. The idea is to divide the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight. Unusually, it was not published in an article but merely described in a Coursera lecture.
So, first the running average is calculated in terms of means square,
(w)\right)^

is the forgetting factor. The concept of storing the historical gradient as sum of squares is borrowed from Adagrad, but "forgetting" is introduced to solve Adagrad's diminishing learning rates in non-convex problems by gradually decreasing the influence of old data.
And the parameters are updated as,
\nabla Q_(w)
RMSProp has shown good adaptation of learning rate in different applications. RMSProp can be seen as a generalization of Rprop and is capable to work with mini-batches as well opposed to only full-batches.
Adam (short for Adaptive Moment Estimation) is a 2014 update to the RMSProp optimizer combining it with the main feature of the Momentum method. In this optimization algorithm, running averages with exponential forgetting of both the gradients and the second moments of the gradients are used. Given parameters
and a loss function
indexes the current training iteration (indexed at
), Adam's parameter update is given by:
^:=\beta _m_^+\left(1-\beta _\right)\nabla _L^
^:=\beta _v_^+\left(1-\beta _\right)\left(\nabla _L^\right)^
_^=^^
_^=^^
:=w^-\eta _^_^+\varepsilon 

is a small scalar (e.g.
) used to prevent division by 0, and

(e.g. 0.9) and

(e.g. 0.999) are the forgetting factors for gradients and second moments of gradients, respectively. Squaring and square-rooting is done element-wise.
As the exponential moving averages of the gradient
and the squared gradient
are initialized with a vector of 0's, there would be a bias towards zero in the first training iterations. A factor
^
is introduced to compensate this bias and get better estimates
_^
_^
The initial proof establishing the convergence of Adam was incomplete, and subsequent analysis has revealed that Adam does not converge for all convex objectives. Despite this, Adam continues to be used due to its strong performance in practice.
The popularity of Adam inspired many variants and enhancements. Some examples include:
Nesterov-enhanced gradients: NAdam, FASFA
varying interpretations of second-order information: Powerpropagation and AdaSqrt.
Using infinity norm: AdaMax
AMSGrad, which improves convergence over Adam by using maximum of past squared gradients instead of the exponential average. AdamX further improves convergence over AMSGrad.
AdamW, which improves the weight decay.
Even though sign-based optimization goes back to the aforementioned Rprop, in 2018 researchers tried to simplify Adam by removing the magnitude of the stochastic gradient from being taken into account and only considering its sign.
Backtracking line search is another variant of gradient descent. All of the below are sourced from the mentioned link. It is based on a condition known as the Armijo–Goldstein condition. Both methods allow learning rates to change at each iteration; however, the manner of the change is different. Backtracking line search uses function evaluations to check Armijo's condition, and in principle the loop in the algorithm for determining the learning rates can be long and unknown in advance. Adaptive SGD does not need a loop in determining learning rates. On the other hand, adaptive SGD does not guarantee the "descent property" – which Backtracking line search enjoys – which is that
)\leq f(x_)
for all n. If the gradient of the cost function is globally Lipschitz continuous, with Lipschitz constant L, and learning rate is chosen of the order 1/L, then the standard version of SGD is a special case of backtracking line search.
A stochastic analogue of the standard (deterministic) Newton–Raphson algorithm (a "second-order" method) provides an asymptotically optimal or near-optimal form of iterative optimization in the setting of stochastic approximation. A method that uses direct measurements of the Hessian matrices of the summands in the empirical risk function was developed by Byrd, Hansen, Nocedal, and Singer. However, directly determining the required Hessian matrices for optimization may not be possible in practice. Practical and theoretically sound methods for second-order versions of SGD that do not require direct Hessian information are given by Spall and others. (A less efficient method based on finite differences, instead of simultaneous perturbations, is given by Ruppert.) Another approach to the approximation Hessian matrix is replacing it with the Fisher information matrix, which transforms usual gradient to natural. These methods not requiring direct Hessian information are based on either values of the summands in the above empirical risk function or values of the gradients of the summands (i.e., the SGD inputs). In particular, second-order optimality is asymptotically achievable without direct calculation of the Hessian matrices of the summands in the empirical risk function. When the objective is a nonlinear least-squares loss
\sum _^Q_(w)=\sum _^(m(w;x_)-y_)^,
is the predictive model (e.g., a deep neural network)
the objective's structure can be exploited to estimate 2nd order information using gradients only. The resulting
methods are simple and often effective
For small learning rate

stochastic gradient descent
)_ _
can be viewed as a discretization of the gradient flow ODE
W_=-\nabla Q(W_)
subject to additional stochastic noise. This approximation is only valid on a finite time-horizon in the following sense: assume that all the coefficients
are sufficiently smooth. Let
 ^\to \mathbb  
be a sufficiently smooth test function. Then, there exists a constant
such that for all

\left|\mathbb  [g(w_)]-g(W_)\right|\leq C\eta ,
 
denotes taking the expectation with respect to the random choice of indices in the stochastic gradient descent scheme.
Since this approximation does not capture the random fluctuations around the mean behavior of stochastic gradient descent solutions to stochastic differential equations (SDEs) have been proposed as limiting objects. More precisely, the solution to the SDE
=-\nabla \left(Q(W_)+\eta |\nabla Q(W_)|^\right)dt+\Sigma (W_)^dB_,
\left(\sum _^Q_(w)-Q(w)\right)\left(\sum _^Q_(w)-Q(w)\right)^
denotes the Ito-integral with respect to a Brownian motion is a more precise approximation in the sense that there exists a constant
\left|\mathbb  [g(w_)]-\mathbb  [g(W_)]\right|\leq C\eta ^.
However this SDE only approximates the one-point motion of stochastic gradient descent. For an approximation of the stochastic flow one has to consider SDEs with infinite-dimensional noise.
Backtracking line search
Coordinate descent – changes one coordinate at a time, rather than one example
Online machine learning
Stochastic hill climbing
Stochastic variance reduction
Bottou, Léon (2004), "Stochastic Learning", Advanced Lectures on Machine Learning, LNAI, vol. 3176, Springer, pp. 146–168, ISBN 978-3-540-23122-6
Buduma, Nikhil; Locascio, Nicholas (2017), "Beyond Gradient Descent", Fundamentals of Deep Learning : Designing Next-Generation Machine Intelligence Algorithms, O'Reilly, ISBN 9781491925584
LeCun, Yann A.; Bottou, Léon; Orr, Genevieve B.; Müller, Klaus-Robert (2012), "Efficient BackProp", Neural Networks: Tricks of the Trade, Springer, pp. 9–48, ISBN 978-3-642-35288-1
Spall, James C. (2003), Introduction to Stochastic Search and Optimization, Wiley, ISBN 978-0-471-33052-3
"Gradient Descent, How Neural Networks Learn". 3Blue1Brown. October 16, 2017. Archived from the original on 2021-12-22 – via YouTube.
Goh (April 4, 2017). "Why Momentum Really Works". Distill. 2 (4). doi:10.23915/distill.00006. Interactive paper explaining momentum.    Transfer learning (TL) is a technique in machine learning (ML) in which knowledge learned from a task is re-used in order to boost performance on a related task. For example, for image classification, knowledge gained while learning to recognize cars could be applied when trying to recognize trucks. This topic is related to the psychological literature on transfer of learning, although practical ties between the two fields are limited. Reusing/transferring information from previously learned tasks to new tasks has the potential to significantly improve learning efficiency.
Since transfer learning makes use of training with multiple objective functions it is related to cost-sensitive machine learning and multi-objective optimization.
In 1976, Bozinovski and Fulgosi published a paper addressing transfer learning in neural network training. The paper gives a mathematical and geometrical model of the topic. In 1981, a report considered the application of transfer learning to a dataset of images representing letters of computer terminals, experimentally demonstrating positive and negative transfer learning.
In 1992, Lorien Pratt formulated the discriminability-based transfer (DBT) algorithm.
By 1998, the field had advanced to include multi-task learning, along with more formal theoretical foundations. Influential publications on transfer learning include the book Learning to Learn in 1998, a 2009 survey and a 2019 survey.
Ng said in his NIPS 2016 tutorial that TL would become the next driver of machine learning commercial success after supervised learning.
In the 2020 paper, "Rethinking Pre-Training and self-training", Zoph et al. reported that pre-training can hurt accuracy, and advocate self-training instead.
The definition of transfer learning is given in terms of domains and tasks. A domain

consists of: a feature space

and a marginal probability distribution
,...,x_\\in 
. Given a specific domain,
=\,P(X)\
, a task consists of two components: a label space

and an objective predictive function
\rightarrow 
. The function
is used to predict the corresponding label
of a new instance
. This task, denoted by
=\,f(x)\
, is learned from the training data consisting of pairs
\in 
\in 
Given a source domain
_
and learning task
_
, a target domain
_
and learning task
_
_\neq _
_\neq _
, transfer learning aims to help improve the learning of the target predictive function
(\cdot )
_
using the knowledge in
_
_
Algorithms for transfer learning are available in Markov logic networks and Bayesian networks. Transfer learning has been applied to cancer subtype discovery, building utilization, general game playing, text classification, digit recognition, medical imaging and spam filtering.
In 2020, it was discovered that, due to their similar physical natures, transfer learning is possible between electromyographic (EMG) signals from the muscles and classifying the behaviors of electroencephalographic (EEG) brainwaves, from the gesture recognition domain to the mental state recognition domain. It was noted that this relationship worked in both directions, showing that electroencephalographic can likewise be used to classify EMG. The experiments noted that the accuracy of neural networks and convolutional neural networks were improved through transfer learning both prior to any learning (compared to standard random weight distribution) and at the end of the learning process (asymptote). That is, results are improved by exposure to another domain. Moreover, the end-user of a pre-trained model can change the structure of fully-connected layers to improve performance.
Crossover (genetic algorithm)
General game playing
Transfer of learning in educational psychology
Thrun, Sebastian; Pratt, Lorien (6 December 2012). Learning to Learn. Springer Science & Business Media. ISBN 978-1-4615-5529-2.  Cold start is a potential problem in computer-based information systems which involves a degree of automated data modelling. Specifically, it concerns the issue that the system cannot draw any inferences for users or items about which it has not yet gathered sufficient information.
The cold start problem is a well known and well researched problem for recommender systems. Recommender systems form a specific type of information filtering (IF) technique that attempts to present information items (e-commerce, films, music, books, news, images, web pages) that are likely of interest to the user. Typically, a recommender system compares the user's profile to some reference characteristics. These characteristics may be related to item characteristics (content-based filtering) or the user's social environment and past behavior (collaborative filtering).
Depending on the system, the user can be associated to various kinds of interactions: ratings, bookmarks, purchases, likes, number of page visits etc.
There are three cases of cold start:
New community: refers to the start-up of the recommender, when, although a catalogue of items might exist, almost no users are present and the lack of user interaction makes it very hard to provide reliable recommendations
New item: a new item is added to the system, it might have some content information but no interactions are present
New user: a new user registers and has not provided any interaction yet, therefore it is not possible to provide personalized recommendations
The new community problem, or systemic bootstrapping, refers to the startup of the system, when virtually no information the recommender can rely upon is present.
This case presents the disadvantages of both the New user and the New item case, as all items and users are new.
Due to this some of the techniques developed to deal with those two cases are not applicable to the system bootstrapping.
The item cold-start problem refers to when items added to the catalogue have either none or very little interactions. This constitutes a problem mainly for collaborative filtering algorithms due to the fact that they rely on the item's interactions to make recommendations. If no interactions are available then a pure collaborative algorithm cannot recommend the item. In case only a few interactions are available, although a collaborative algorithm will be able to recommend it, the quality of those recommendations will be poor.
This raises another issue, which is not anymore related to new items, but rather to unpopular items.
In some cases (e.g. movie recommendations) it might happen that a handful of items receive an extremely high number of interactions, while most of the items only receive a fraction of them. This is referred to as popularity bias.
In the context of cold-start items the popularity bias is important because it might happen that many items, even if they have been in the catalogue for months, received only a few interactions. This creates a negative loop in which unpopular items will be poorly recommended, therefore will receive much less visibility than popular ones, and will struggle to receive interactions. While it is expected that some items will be less popular than others, this issue specifically refers to the fact that the recommender has not enough collaborative information to recommend them in a meaningful and reliable way.
Content-based filtering algorithms, on the other hand, are in theory much less prone to the new item problem. Since content based recommenders choose which items to recommend based on the feature the items possess, even if no interaction for a new item exist, still its features will allow for a recommendation to be made.
This of course assumes that a new item will be already described by its attributes, which is not always the case. Consider the case of so-called editorial features (e.g. director, cast, title, year), those are always known when the item, in this case movie, is added to the catalogue. However, other kinds of attributes might not be e.g. features extracted from user reviews and tags. Content-based algorithms relying on user provided features suffer from the cold-start item problem as well, since for new items if no (or very few) interactions exist, also no (or very few) user reviews and tags will be available.
The new user case refers to when a new user enrolls in the system and for a certain period of time the recommender has to provide recommendation without relying on the user's past interactions, since none has occurred yet.
This problem is of particular importance when the recommender is part of the service offered to users, since a user who is faced with recommendations of poor quality might soon decide to stop using the system before providing enough interaction to allow the recommender to understand his/her interests.
The main strategy in dealing with new users is to ask them to provide some preferences to build an initial user profile. A threshold has to be found between the length of the user registration process, which if too long might induce too many users to abandon it, and the amount of initial data required for the recommender to work properly.
Similarly to the new items case, not all recommender algorithms are affected in the same way.
Item-item recommenders will be affected as they rely on user profile to weight how relevant other user's preferences are. Collaborative filtering algorithms are the most affected as without interactions no inference can be made about the user's preferences.
User-user recommender algorithms behave slightly differently. A user-user content based algorithm will rely on user's features (e.g. age, gender, country) to find similar users and recommend the items they interacted with in a positive way, therefore being robust to the new user case. Note that all these information is acquired during the registration process, either by asking the user to input the data himself, or by leveraging data already available e.g. in his social media accounts.
Due to the high number of recommender algorithms available as well as system type and characteristics, many strategies to mitigate the cold-start problem have been developed. The main approach is to rely on hybrid recommenders, in order to mitigate the disadvantages of one category or model by combining it with another.
All three categories of cold-start (new community, new item, and new user) have in common the lack of user interactions and presents some commonalities in the strategies available to address them.
A common strategy when dealing with new items is to couple a collaborative filtering recommender, for warm items, with a content-based filtering recommender, for cold-items. While the two algorithms can be combined in different ways, the main drawback of this method is related to the poor recommendation quality often exhibited by content-based recommenders in scenarios where it is difficult to provide a comprehensive description of the item characteristics.
In case of new users, if no demographic feature is present or their quality is too poor, a common strategy is to offer them non-personalized recommendations. This means that they could be recommended simply the most popular items either globally or for their specific geographical region or language.
One of the available options when dealing with cold users or items is to rapidly acquire some preference data. There are various ways to do that depending on the amount of information required. These techniques are called preference elicitation strategies.
This may be done either explicitly (by querying the user) or implicitly (by observing the user's behaviour). In both cases, the cold start problem would imply that the user has to dedicate an amount of effort using the system in its 'dumb' state – contributing to the construction of their user profile – before the system can start providing any intelligent recommendations.
For example MovieLens, a web-based recommender system for movies, asks the user to rate some movies as a part of the registration.
While preference elicitation strategy are a simple and effective way to deal with new users, the additional requirements during the registration will make the process more time-consuming for the user. Moreover, the quality of the obtained preferences might not be ideal as the user could rate items they had seen months or years ago or the provided ratings could be almost random if the user provided them without paying attention just to complete the registration quickly.
The construction of the user's profile may also be automated by integrating information from other user activities, such as browsing histories or social media platforms. If, for example, a user has been reading information about a particular music artist from a media portal, then the associated recommender system would automatically propose that artist's releases when the user visits the music store.
A variation of the previous approach is to automatically assign ratings to new items, based on the ratings assigned by the community to other similar items. Item similarity would be determined according to the items' content-based characteristics.
It is also possible to create initial profile of a user based on the personality characteristics of the user and use such profile to generate personalized recommendation.
Personality characteristics of the user can be identified using a personality model such as five factor model (FFM).
Another of the possible techniques is to apply active learning (machine learning). The main goal of active learning is to guide the user in the preference elicitation process in order to ask him to rate only the items that for the recommender point of view will be the most informative ones. This is done by analysing the available data and estimating the usefulness of the data points (e.g., ratings, interactions).
As an example, say that we want to build two clusters from a certain cloud of points. As soon as we have identified two points each belonging to a different cluster, which is the next most informative point? If we take a point close to one we already know we can expect that it will likely belong to the same cluster. If we choose a point which is in between the two clusters, knowing which cluster it belongs to will help us in finding where the boundary is, allowing to classify many other points with just a few observations.
The cold start problem is also exhibited by interface agents. Since such an agent typically learns the user's preferences implicitly by observing patterns in the user's behaviour – "watching over the shoulder" – it would take time before the agent may perform any adaptations personalised to the user. Even then, its assistance would be limited to activities which it has formerly observed the user engaging in.
The cold start problem may be overcome by introducing an element of collaboration amongst agents assisting various users. This way, novel situations may be handled by requesting other agents to share what they have already learnt from their respective users.
In recent years more advanced strategies have been proposed, they all rely on machine learning and attempt to merge the content and collaborative information in a single model.
One example of this approaches is called attribute to feature mapping which is tailored to matrix factorization algorithms. The basic idea is the following. A matrix factorization model represents the user-item interactions as the product of two rectangular matrices whose content is learned using the known interactions via machine learning. Each user will be associated to a row of the first matrix and each item with a column of the second matrix. The row or column associated to a specific user or item is called latent factors. When a new item is added it has no associated latent factors and the lack of interactions does not allow to learn them, as it was done with other items. If each item is associated to some features (e.g. author, year, publisher, actors) it is possible to define an embedding function, which given the item features estimates the corresponding item latent factors. The embedding function can be designed in many ways and it is trained with the data already available from warm items. Alternatively, one could apply a group-specific method. A group-specific method further decomposes each latent factor into two additive parts: One part corresponds to each item (and/or each user), while the other part is shared among items within each item group (e.g., a group of movies could be movies of the same genre). Then once a new item arrives, we can assign a group label to it, and approximates its latent factor by the group-specific part (of the corresponding item group). Therefore, although the individual part of the new item is not available, the group-specific part provides an immediate and effective solution. The same applies for a new user, as if some information is available for them (e.g. age, nationality, gender) then his/her latent factors can be estimated via an embedding function or a group-specific latent factor.
Another recent approach which bears similarities with feature mapping is building a hybrid content-based filtering recommender in which features, either of the items or of the users, are weighted according to the user's perception of importance. In order to identify a movie that the user could like, different attributes (e.g. which are the actors, director, country, title) will have different importance. As an example consider the James Bond movie series, the main actor changed many times during the years, while some did not, like Lois Maxwell. Therefore, her presence will probably be a better identifier of that kind of movie than the presence of one of the various main actors.
Although various techniques exist to apply feature weighting to user or item features in recommender systems, most of them are from the information retrieval domain like tf–idf, Okapi BM25, only a few have been developed specifically for recommenders.
Hybrid feature weighting techniques in particular are tailored for the recommender system domain. Some of them learn feature weight by exploiting directly the user's interactions with items, like FBSM. Others rely on an intermediate collaborative model trained on warm items and attempt to learn the content feature weights which will better approximate the collaborative model.
Many of the hybrid methods can be considered special cases of factorization machines.
The above methods rely on affiliated information from users or items. Recently, another approach mitigates the cold start problem by assigning lower constraints to the latent factors associated with the items or users that reveal more information (i.e., popular items and active users), and set higher constraints to the others (i.e., less popular items and inactive users). It is shown that various recommendation models benefit from this strategy. Differentiating regularization weights can be integrated with the other cold start mitigating strategies.
Active learning (machine learning)
http://activeintelligence.org/research/al-rs/ Archived 2011-08-26 at the Wayback Machine Matrix factorization is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices. This family of methods became widely known during the Netflix prize challenge due to its effectiveness as reported by Simon Funk in his 2006 blog post, where he shared his findings with the research community. The prediction results can be improved by assigning different regularization weights to the latent factors based on items' popularity and users' activeness.
The idea behind matrix factorization is to represent users and items in a lower dimensional latent space. Since the initial work by Funk in 2006 a multitude of matrix factorization approaches have been proposed for recommender systems. Some of the most used and simpler ones are listed in the following sections.
The original algorithm proposed by Simon Funk in his blog post factorized the user-item rating matrix as the product of two lower dimensional matrices, the first one has a row for each user, while the second has a column for each item. The row or column associated to a specific user or item is referred to as latent factors. Note that, in Funk MF no singular value decomposition is applied, it is a SVD-like machine learning model.
The predicted ratings can be computed as
=HW
\in \mathbb  ^\times 
is the user-item rating matrix,
 ^\times 
contains the user's latent factors and
 ^\times 
the item's latent factors.
Specifically, the predicted rating user u will give to item i is computed as:
_=\sum _^H_W_
It is possible to tune the expressive power of the model by changing the number of latent factors. It has been demonstrated that a matrix factorization with one latent factor is equivalent to a most popular or top popular recommender (e.g. recommends the items with the most interactions without any personalization). Increasing the number of latent factors will improve personalization, therefore recommendation quality, until the number of factors becomes too high, at which point the model starts to overfit and the recommendation quality will decrease. A common strategy to avoid overfitting is to add regularization terms to the objective function.
Funk MF was developed as a rating prediction problem, therefore it uses explicit numerical ratings as user-item interactions.
All things considered, Funk MF minimizes the following objective function:
 \,\|R-\|_+\alpha \|H\|+\beta \|W\|

is defined to be the frobenius norm whereas the other norms might be either frobenius or another norm depending on the specific recommending problem.
While Funk MF is able to provide very good recommendation quality, its ability to use only explicit numerical ratings as user-items interactions constitutes a limitation. Modern day recommender systems should exploit all available interactions both explicit (e.g. numerical ratings) and implicit (e.g. likes, purchases, skipped, bookmarked). To this end SVD++ was designed to take into account implicit interactions as well.
Compared to Funk MF, SVD++ takes also into account user and item bias.
The predicted rating user u will give to item i is computed as:
_=\mu +b_+b_+\sum _^H_W_

refers to the overall average rating over all items and
refers to the observed deviation of the item i and the user u respectively from the average. SVD++ has however some disadvantages, with the main drawback being that this method is not model-based. This means that if a new user is added, the algorithm is incapable of modeling it unless the whole model is retrained. Even though the system might have gathered some interactions for that new user, its latent factors are not available and therefore no recommendations can be computed. This is an example of a cold-start problem, that is the recommender cannot deal efficiently with new users or items and specific strategies should be put in place to handle this disadvantage.
A possible way to address this cold start problem is to modify SVD++ in order for it to become a model-based algorithm, therefore allowing to easily manage new items and new users.
As previously mentioned in SVD++ we don't have the latent factors of new users, therefore it is necessary to represent them in a different way. The user's latent factors represent the preference of that user for the corresponding item's latent factors, therefore user's latent factors can be estimated via the past user interactions. If the system is able to gather some interactions for the new user it is possible to estimate its latent factors.
Note that this does not entirely solve the cold-start problem, since the recommender still requires some reliable interactions for new users, but at least there is no need to recompute the whole model every time. It has been demonstrated that this formulation is almost equivalent to a SLIM model, which is an item-item model based recommender.
_=\mu +b_+b_+\sum _^\sum _^r_W_^W_
With this formulation, the equivalent item-item recommender would be
=RS=RW^W
. Therefore the similarity matrix is symmetric.
Asymmetric SVD aims at combining the advantages of SVD++ while being a model based algorithm, therefore being able to consider new users with a few ratings without needing to retrain the whole model. As opposed to the model-based SVD here the user latent factor matrix H is replaced by Q, which learns the user's preferences as function of their ratings.
The predicted rating user u will give to item i is computed as:
_=\mu +b_+b_+\sum _^\sum _^r_Q_W_
With this formulation, the equivalent item-item recommender would be
=RS=RQ^W
. Since matrices Q and W are different the similarity matrix is asymmetric, hence the name of the model.
A group-specific SVD can be an effective approach for the cold-start problem in many scenarios. It clusters users and items based on dependency information and similarities in characteristics. Then once a new user or item arrives, we can assign a group label to it, and approximates its latent factor by the group effects (of the corresponding group). Therefore, although ratings associated with the new user or item are not necessarily available, the group effects provide immediate and effective predictions.
The predicted rating user u will give to item i is computed as:
_=\sum _^(H_+S_,f)(W_+T_)
represent the group label of user u and item i, respectively, which are identical across members from the same group. And S and T are matrices of group effects. For example, for a new user
whose latent factor
is not available, we can at least identify their group label
, and predict their ratings as:
_i=\sum _^S_,f(W_+T_)
This provides a good approximation to the unobserved ratings.
In recent years many other matrix factorization models have been developed to exploit the ever increasing amount and variety of available interaction data and use cases. Hybrid matrix factorization algorithms are capable of merging explicit and implicit interactions or both content and collaborative data
In recent years a number of neural and deep-learning techniques have been proposed, some of which generalize traditional Matrix factorization algorithms via a non-linear neural architecture.
While deep learning has been applied to many different scenarios (context-aware, sequence-aware, social tagging, etc.), its real effectiveness when used in a simple Collaborative filtering scenario has been put into question. Systematic analysis of publications applying deep learning or neural methods to the top-k recommendation problem, published in top conferences (SIGIR, KDD, WWW, RecSys, IJCAI), has shown that on average less than 40% of articles are reproducible, with as little as 14% in some conferences. Overall the studies identify 26 articles, only 12 of them could be reproduced and 11 of them could be outperformed by much older and simpler properly tuned baselines. The articles also highlights a number of potential problems in today's research scholarship and call for improved scientific practices in that area. Similar issues have been spotted also in sequence-aware recommender systems. ACM Conference on Recommender Systems (ACM RecSys) is an A-ranked peer-reviewed academic conference series about recommender systems. It is held annually in different locations, and organized by different organizers, but a Steering Committee supervises the organization. The conference proceedings are published by the Association for Computing Machinery. Acceptance rates for full papers are typically below 20%. This conference series focuses on issues such as algorithms, machine learning, human-computer interaction, and data science from a multi-disciplinary perspective. The conference community includes computer scientists, statisticians, social scientists, psychologists, and others.
The conference is sponsored every year by ten to 20 Big Tech companies such as Amazon, Netflix, Meta, Nvidia, Microsoft, Google, and Spotify.
While an academic conference, RecSys attracts many practitioners and industry researchers, with industry attendance making up the majority of attendees, this is also reflected in the authorship of research papers. Many works published at the conference have direct impact on recommendation and personalization practice in industry affecting millions of users.
Recommender systems are pervasive in online systems, the conference provides opportunities for researchers and practitioners to address specific problems in various workshops in conjunction with the conference, topics include responsible recommendation, causal reasoning, and others. The workshop themes follow recent developments in the broader machine learning and human-computer interaction topics.
The conference is the host of the ACM RecSys Challenge, a yearly competition in the spirit of the Netflix Prize focussing on a specific recommendation problem. The Challenge has been organized by companies such as Twitter, and Spotify. Participation in the challenge is open to everyone and participation in it has become a means of showcasing ones skills in recommendations, similar to Kaggle competitions.
The Netflix Prize was a recommendation challenge organized by Netflix between 2006 and 2009. Shortly prior to ACM RecSys 2009, the winners of the Netflix Prize were announced. At the 2009 conference, members of the winning team (Bellkor's Pragmatich Chaos) as well as representatives from Netflix convened in a panel on the lessons learnt from the Netflix Prize
In 2022, at one of the workshops at the conference, a paper from ByteDance, the company behind TikTok, described in detail how a recommendation algorithm for video worked.
While the paper did not point out the algorithm as the one that generates TikTok's recommendations, the paper received significant attention in technology-focused media.
Past and future RecSys conferences include:
The ACM Recommender Systems Conference (RecSys) has experienced significant growth since its first event in 2007. The number of paper submissions has steadily increased over the years. From an initial 35 submissions in 2007, the conference has seen over 250 submissions annually in recent years. While the number of submissions has increased, the conference's acceptance rate has become more selective, declining from 46% in its inaugural year to a range of 17-24% in more recent editions. The Journal of Time Series Analysis is a bimonthly peer-reviewed academic journal covering mathematical statistics as it relates to the analysis of time series data. It was established in 1980 and is published by John Wiley & Sons. The editor-in-chief is Robert Taylor (University of Essex). According to the Journal Citation Reports, the journal has a 2021 impact factor of 1.208, ranking it 94th out of 108 journals in the category "Mathematics, Interdisciplinary Applications" and 88th out of 125 in the category "Statistics & Probability". In data analysis, anomaly detection (also referred to as outlier detection and sometimes as novelty detection) is generally understood to be the identification of rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well defined notion of normal behavior. Such examples may arouse suspicions of being generated by a different mechanism, or appear inconsistent with the remainder of that set of data.
Anomaly detection finds application in many domains including cybersecurity, medicine, machine vision, statistics, neuroscience, law enforcement and financial fraud to name only a few. Anomalies were initially searched for clear rejection or omission from the data to aid statistical analysis, for example to compute the mean or standard deviation. They were also removed to better predictions from models such as linear regression, and more recently their removal aids the performance of machine learning algorithms. However, in many applications anomalies themselves are of interest and are the observations most desirous in the entire data set, which need to be identified and separated from noise or irrelevant outliers.
Three broad categories of anomaly detection techniques exist. Supervised anomaly detection techniques require a data set that has been labeled as "normal" and "abnormal" and involves training a classifier. However, this approach is rarely used in anomaly detection due to the general unavailability of labelled data and the inherent unbalanced nature of the classes. Semi-supervised anomaly detection techniques assume that some portion of the data is labelled. This may be any combination of the normal or anomalous data, but more often than not, the techniques construct a model representing normal behavior from a given normal training data set, and then test the likelihood of a test instance to be generated by the model. Unsupervised anomaly detection techniques assume the data is unlabelled and are by far the most commonly used due to their wider and relevant application.
Many attempts have been made in the statistical and computer science communities to define an anomaly. The most prevalent ones include the following, and can be categorised into three groups: those that are ambiguous, those that are specific to a method with pre-defined thresholds usually chosen empirically, and those that are formally defined:
An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism.
Anomalies are instances or collections of data that occur very rarely in the data set and whose features differ significantly from most of the data.
An outlier is an observation (or subset of observations) which appears to be inconsistent with the remainder of that set of data.
An anomaly is a point or collection of points that is relatively distant from other points in multi-dimensional space of features.
Anomalies are patterns in data that do not conform to a well-defined notion of normal behaviour.
Let T be observations from a univariate Gaussian distribution and O a point from T. Then the z-score for O is greater than a pre-selected threshold if and only if O is an outlier.
The concept of intrusion detection, a critical component of anomaly detection, has evolved significantly over time. Initially, it was a manual process where system administrators would monitor for unusual activities, such as a vacationing user's account being accessed or unexpected printer activity. This approach was not scalable and was soon superseded by the analysis of audit logs and system logs for signs of malicious behavior.
By the late 1970s and early 1980s, the analysis of these logs was primarily used retrospectively to investigate incidents, as the volume of data made it impractical for real-time monitoring. The affordability of digital storage eventually led to audit logs being analyzed online, with specialized programs being developed to sift through the data. These programs, however, were typically run during off-peak hours due to their computational intensity.
The 1990s brought the advent of real-time intrusion detection systems capable of analyzing audit data as it was generated, allowing for immediate detection of and response to attacks. This marked a significant shift towards proactive intrusion detection.
As the field has continued to develop, the focus has shifted to creating solutions that can be efficiently implemented across large and complex network environments, adapting to the ever-growing variety of security threats and the dynamic nature of modern computing infrastructures.
Anomaly detection is applicable in a very large number and variety of domains, and is an important subarea of unsupervised machine learning. As such it has applications in cyber-security, intrusion detection, fraud detection, fault detection, system health monitoring, event detection in sensor networks, detecting ecosystem disturbances, defect detection in images using machine vision, medical diagnosis and law enforcement.
Anomaly detection was proposed for intrusion detection systems (IDS) by Dorothy Denning in 1986. Anomaly detection for IDS is normally accomplished with thresholds and statistics, but can also be done with soft computing, and inductive learning. Types of features proposed by 1999 included profiles of users, workstations, networks, remote hosts, groups of users, and programs based on frequencies, means, variances, covariances, and standard deviations. The counterpart of anomaly detection in intrusion detection is misuse detection.
Anomaly detection is vital in fintech for fraud prevention.
Preprocessing data to remove anomalies can be an important step in data analysis, and is done for a number of reasons. Statistics such as the mean and standard deviation are more accurate after the removal of anomalies, and the visualisation of data can also be improved. In supervised learning, removing the anomalous data from the dataset often results in a statistically significant increase in accuracy.
Anomaly detection has become increasingly vital in video surveillance to enhance security and safety. With the advent of deep learning technologies, methods using Convolutional Neural Networks (CNNs) and Simple Recurrent Units (SRUs) have shown significant promise in identifying unusual activities or behaviors in video data. These models can process and analyze extensive video feeds in real-time, recognizing patterns that deviate from the norm, which may indicate potential security threats or safety violations. An important aspect for video surveillance is the development of scalable real-time frameworks. Such pipelines are required for processing multiple video streams with low computational resources.
In IT infrastructure management, anomaly detection is crucial for ensuring the smooth operation and reliability of services. These are complex systems, composed of many interactive elements and large data quantities, requiring methods to process and reduce this data into a human and machine interpretable format. Techniques like the IT Infrastructure Library (ITIL) and monitoring frameworks are employed to track and manage system performance and user experience. Detected anomalies can help identify and pre-empt potential performance degradations or system failures, thus maintaining productivity and business process effectiveness.
Anomaly detection is critical for the security and efficiency of Internet of Things (IoT) systems. It helps in identifying system failures and security breaches in complex networks of IoT devices. The methods must manage real-time data, diverse device types, and scale effectively. Garg et al. have introduced a multi-stage anomaly detection framework that improves upon traditional methods by incorporating spatial clustering, density-based clustering, and locality-sensitive hashing. This tailored approach is designed to better handle the vast and varied nature of IoT data, thereby enhancing security and operational reliability in smart infrastructure and industrial IoT systems.
Anomaly detection is crucial in the petroleum industry for monitoring critical machinery. Martí et al. used a novel segmentation algorithm to analyze sensor data for real-time anomaly detection. This approach helps promptly identify and address any irregularities in sensor readings, ensuring the reliability and safety of petroleum operations.
In the oil and gas sector, anomaly detection is not just crucial for maintenance and safety, but also for environmental protection. Aljameel et al. propose an advanced machine learning-based model for detecting minor leaks in oil and gas pipelines, a task traditional methods may miss.
Many anomaly detection techniques have been proposed in literature. The performance of methods usually depend on the data sets. For example, some may be suited to detecting local outliers, while others global, and methods have little systematic advantages over another when compared across many data sets. Almost all algorithms also require the setting of non-intuitive parameters critical for performance, and usually unknown before application. Some of the popular techniques are mentioned below and are broken down into categories:
Also referred to as frequency-based or counting-based, the simplest non-parametric anomaly detection method is to build a histogram with the training data or a set of known normal instances, and if a test point does not fall in any of the histogram bins mark it as anomalous, or assign an anomaly score to test data based on the height of the bin it falls in. The size of bins are key to the effectiveness of this technique but must be determined by the implementer.
A more sophisticated technique uses kernel functions to approximate the distribution of the normal data. Instances in low probability areas of the distribution are then considered anomalies.
Tukey's range test
Density-based techniques (k-nearest neighbor, local outlier factor, isolation forests, and many more variations of this concept)
Subspace-base (SOD), correlation-based (COP) and tensor-based outlier detection for high-dimensional data
One-class support vector machines (OCSVM, SVDD)
Replicator neural networks, autoencoders, variational autoencoders, long short-term memory neural networks
Hidden Markov models (HMMs)
Convolutional Neural Networks (CNNs): CNNs have shown exceptional performance in the unsupervised learning domain for anomaly detection, especially in image and video data analysis. Their ability to automatically and hierarchically learn spatial hierarchies of features from low to high-level patterns makes them particularly suited for detecting visual anomalies. For instance, CNNs can be trained on image datasets to identify atypical patterns indicative of defects or out-of-norm conditions in industrial quality control scenarios.
Simple Recurrent Units (SRUs): In time-series data, SRUs, a type of recurrent neural network, have been effectively used for anomaly detection by capturing temporal dependencies and sequence anomalies. Unlike traditional RNNs, SRUs are designed to be faster and more parallelizable, offering a better fit for real-time anomaly detection in complex systems such as dynamic financial markets or predictive maintenance in machinery, where identifying temporal irregularities promptly is crucial.
Foundation models: Since the advent of large-scale foundation models that have been used successfully on most downstream tasks, they have also been adapted for use in anomaly detection and segmentation. Methods utilizing pretrained foundation models include using the alignment of image and text embeddings (CLIP, etc.) for anomaly localization, while others may use the inpainting ability of generative image models for reconstruction-error based anomaly detection.
Clustering: Cluster analysis-based outlier detection
Deviations from association rules and frequent itemsets
Fuzzy logic-based outlier detection
Ensemble techniques, using feature bagging, score normalization and different sources of diversity
Histogram-based Outlier Score (HBOS) uses value histograms and assumes feature independence for fast predictions.
Dynamic networks, such as those representing financial systems, social media interactions, and transportation infrastructure, are subject to constant change, making anomaly detection within them a complex task. Unlike static graphs, dynamic networks reflect evolving relationships and states, requiring adaptive techniques for anomaly detection.
Probabilistic model anomalies
Many of the methods discussed above only yield an anomaly score prediction, which often can be explained to users as the point being in a region of low data density (or relatively low density compared to the neighbor's densities). In explainable artificial intelligence, the users demand methods with higher explainability. Some methods allow for more detailed explanations:
The Subspace Outlier Degree (SOD) identifies attributes where a sample is normal, and attributes in which the sample deviates from the expected.
Correlation Outlier Probabilities (COP) compute an error vector of how a sample point deviates from an expected location, which can be interpreted as a counterfactual explanation: the sample would be normal if it were moved to that location.
ELKI is an open-source Java data mining toolkit that contains several anomaly detection algorithms, as well as index acceleration for them.
PyOD is an open-source Python library developed specifically for anomaly detection.
scikit-learn is an open-source Python library that contains some algorithms for unsupervised anomaly detection.
Wolfram Mathematica provides functionality for unsupervised anomaly detection across multiple data types
Anomaly detection benchmark data repository with carefully chosen data sets of the Ludwig-Maximilians-Universität München; Mirror Archived 2022-03-31 at the Wayback Machine at University of São Paulo.
ODDS – ODDS: A large collection of publicly available outlier detection datasets with ground truth in different domains.
Unsupervised Anomaly Detection Benchmark at Harvard Dataverse: Datasets for Unsupervised Anomaly Detection with ground truth.
KMASH Data Repository at Research Data Australia having more than 12,000 anomaly detection datasets with ground truth.
Statistical process control
Hierarchical temporal memory Network behavior anomaly detection (NBAD) is a security technique that provides network security threat detection. It is a complementary technology to systems that detect security threats based on packet signatures.
NBAD is the continuous monitoring of a network for unusual events or trends. NBAD is an integral part of network behavior analysis (NBA), which offers security in addition to that provided by traditional anti-threat applications such as firewalls, intrusion detection systems, antivirus software and spyware-detection software.
Most security monitoring systems utilize a signature-based approach to detect threats. They generally monitor packets on the network and look for patterns in the packets which match their database of signatures representing pre-identified known security threats. NBAD-based systems are particularly helpful in detecting security threat vectors in two instances where signature-based systems cannot: (i) new zero-day attacks, and (ii) when the threat traffic is encrypted such as the command and control channel for certain Botnets.
An NBAD program tracks critical network characteristics in real time and generates an alarm if a strange event or trend is detected that could indicate the presence of a threat. Large-scale examples of such characteristics include traffic volume, bandwidth use and protocol use.
NBAD solutions can also monitor the behavior of individual network subscribers. In order for NBAD to be optimally effective, a baseline of normal network or user behavior must be established over a period of time. Once certain parameters have been defined as normal, any departure from one or more of them is flagged as anomalous.
NBAD technology/techniques are applied in a number of network and security monitoring domains including: (i) Log analysis (ii) Packet inspection systems (iii) Flow monitoring systems and (iv) Route analytics.
NBAD has also been described as outlier detection, novelty detection, deviation detection and exception mining.
Protocol Anomaly: MAC Spoofing
Protocol Anomaly: IP Spoofing
Protocol Anomaly: TCP/UDP Fanout
Protocol Anomaly: IP Fanout
Protocol Anomaly: Duplicate IP
Protocol Anomaly: Duplicate MAC
Palo Alto Networks – Cortex XDR
Darktrace - AI Enterprise Immune System | Antigena Autonomous Response
Allot Communications – Allot Communications DDoS Protection
Arbor Networks NSI – Arbor Network Security Intelligence
Cisco – Stealthwatch (formerly Lancope StealthWatch)
IBM – QRadar (since 2003)
Enterasys Networks – Enterasys Dragon
Exinda – Inbuilt (Application Performance Score (APS), Application Performance Metric (APM), SLA, and Adaptive Response)
ExtraHop Networks - Reveal(x)
Flowmon Networks – Flowmon ADS
FlowNBA – NetFlow
Juniper Networks – STRM
McAfee – McAfee Network Threat Behavior Analysis
HP ProCurve – Network Immunity Manager
Riverbed Technology – Riverbed Cascade
Sourcefire – Sourcefire 3D
Symantec – Symantec Advanced Threat Protection
GREYCORTEX – Mendel (formerly TrustPort Threat Intelligence)
ZOHO Corporation – ManageEngine NetFlow Analyzer's Advanced Security Analytics Module
Microsoft Corp – Windows Defender ATP and Advanced Threat Analytics
Vehere - PacketWorker Network Detection and Response
User behavior analytics Variational Bayesian methods are a family of techniques for approximating intractable integrals arising in Bayesian inference and machine learning. They are typically used in complex statistical models consisting of observed variables (usually termed "data") as well as unknown parameters and latent variables, with various sorts of relationships among the three types of random variables, as might be described by a graphical model. As typical in Bayesian inference, the parameters and latent variables are grouped together as "unobserved variables". Variational Bayesian methods are primarily used for two purposes:
To provide an analytical approximation to the posterior probability of the unobserved variables, in order to do statistical inference over these variables.
To derive a lower bound for the marginal likelihood (sometimes called the evidence) of the observed data (i.e. the marginal probability of the data given the model, with marginalization performed over unobserved variables). This is typically used for performing model selection, the general idea being that a higher marginal likelihood for a given model indicates a better fit of the data by that model and hence a greater probability that the model in question was the one that generated the data. (See also the Bayes factor article.)
In the former purpose (that of approximating a posterior probability), variational Bayes is an alternative to Monte Carlo sampling methods—particularly, Markov chain Monte Carlo methods such as Gibbs sampling—for taking a fully Bayesian approach to statistical inference over complex distributions that are difficult to evaluate directly or sample. In particular, whereas Monte Carlo techniques provide a numerical approximation to the exact posterior using a set of samples, variational Bayes provides a locally-optimal, exact analytical solution to an approximation of the posterior.
Variational Bayes can be seen as an extension of the expectation–maximization (EM) algorithm from maximum likelihood (ML) or maximum a posteriori (MAP) estimation of the single most probable value of each parameter to fully Bayesian estimation which computes (an approximation to) the entire posterior distribution of the parameters and latent variables. As in EM, it finds a set of optimal parameter values, and it has the same alternating structure as does EM, based on a set of interlocked (mutually dependent) equations that cannot be solved analytically.
For many applications, variational Bayes produces solutions of comparable accuracy to Gibbs sampling at greater speed. However, deriving the set of equations used to update the parameters iteratively often requires a large amount of work compared with deriving the comparable Gibbs sampling equations. This is the case even for many models that are conceptually quite simple, as is demonstrated below in the case of a basic non-hierarchical model with only two parameters and no latent variables.
In variational inference, the posterior distribution over a set of unobserved variables
 =\\dots Z_\
given some data
 
is approximated by a so-called variational distribution,
 ):
 \mid \mathbf  )\approx Q(\mathbf  ).
 )
is restricted to belong to a family of distributions of simpler form than
 \mid \mathbf  )
(e.g. a family of Gaussian distributions), selected with the intention of making
 )
similar to the true posterior,
 \mid \mathbf  )
The similarity (or dissimilarity) is measured in terms of a dissimilarity function
and hence inference is performed by selecting the distribution
 )
The most common type of variational Bayes uses the Kullback–Leibler divergence (KL-divergence) of Q from P as the choice of dissimilarity function. This choice makes this minimization tractable. The KL-divergence is defined as
 (Q\parallel P)\triangleq \sum _ Q(\mathbf  )\log  ) \mid \mathbf  ).
Note that Q and P are reversed from what one might expect. This use of reversed KL-divergence is conceptually similar to the expectation–maximization algorithm. (Using the KL-divergence in the other way produces the expectation propagation algorithm.)
Variational techniques are typically used to form an approximation for:
 \mid \mathbf  )= \mid \mathbf  )P(\mathbf  ) )= \mid \mathbf  )P(\mathbf  ) P(\mathbf  ,\mathbf  ')\,d\mathbf  '
The marginalization over
 
 )
in the denominator is typically intractable, because, for example, the search space of
 
is combinatorially large. Therefore, we seek an approximation, using
 )\approx P(\mathbf  \mid \mathbf  )
 \mid \mathbf  )= ,\mathbf  ) )
, the KL-divergence above can also be written as
D_ (Q\parallel P)&=\sum _ Q(\mathbf  )\left[\log  ) ,\mathbf  )+\log P(\mathbf  )\right]\\&=\sum _ Q(\mathbf  )\left[\log Q(\mathbf  )-\log P(\mathbf  ,\mathbf  )\right]+\sum _ Q(\mathbf  )\left[\log P(\mathbf  )\right]\end
 )
is a constant with respect to
 
 Q(\mathbf  )=1
 )
is a distribution, we have
 (Q\parallel P)=\sum _ Q(\mathbf  )\left[\log Q(\mathbf  )-\log P(\mathbf  ,\mathbf  )\right]+\log P(\mathbf  )
which, according to the definition of expected value (for a discrete random variable), can be written as follows
 (Q\parallel P)=\mathbb  _ \left[\log Q(\mathbf  )-\log P(\mathbf  ,\mathbf  )\right]+\log P(\mathbf  )
which can be rearranged to become
\log P(\mathbf  )&=D_ (Q\parallel P)-\mathbb  _ \left[\log Q(\mathbf  )-\log P(\mathbf  ,\mathbf  )\right]\\&=D_ (Q\parallel P)+(Q)\end
As the log-evidence
 )
is fixed with respect to
, maximizing the final term
(Q)
minimizes the KL divergence of
. By appropriate choice of
(Q)
becomes tractable to compute and to maximize. Hence we have both an analytical approximation
for the posterior
 \mid \mathbf  )
, and a lower bound
(Q)
for the log-evidence
 )
(since the KL-divergence is non-negative).
The lower bound
(Q)
is known as the (negative) variational free energy in analogy with thermodynamic free energy because it can also be expressed as a negative energy
 _[\log P(\mathbf  ,\mathbf  )]
plus the entropy of
. The term
(Q)
is also known as Evidence Lower Bound, abbreviated as ELBO, to emphasize that it is a lower (worst-case) bound on the log-evidence of the data.
By the generalized Pythagorean theorem of Bregman divergence, of which KL-divergence is a special case, it can be shown that:
 (Q\parallel P)\geq D_ (Q\parallel Q^)+D_ (Q^\parallel P),\forall Q^\in 

is a convex set and the equality holds if:
\triangleq \arg \min _D_ (Q\parallel P).
In this case, the global minimizer
(\mathbf  )=q^(\mathbf  _\mid \mathbf  _)q^(\mathbf  _)=q^(\mathbf  _\mid \mathbf  _)q^(\mathbf  _),
 =\ ,\mathbf  \,
can be found as follows:
q^(\mathbf  _)&= ) ) _\mid \mathbf  ) (q^(\mathbf  _\mid \mathbf  _)\parallel P(\mathbf  _\mid \mathbf  _,\mathbf  )))\\&= )\exp \mathbb  _(\mathbf  _\mid \mathbf  _)\left(\log  ,\mathbf  )(\mathbf  _\mid \mathbf  _)\right),\end
in which the normalizing constant is:
\zeta (\mathbf  )&=P(\mathbf  )\int _ _ _\mid \mathbf  ) (q^(\mathbf  _\mid \mathbf  _)\parallel P(\mathbf  _\mid \mathbf  _,\mathbf  )))\\&=\int _ _\exp \mathbb  _(\mathbf  _\mid \mathbf  _)\left(\log  ,\mathbf  )(\mathbf  _\mid \mathbf  _)\right).\end
 )
is often called the evidence lower bound (ELBO) in practice, since
 )\geq \zeta (\mathbf  )=\exp((Q^))
, as shown above.
By interchanging the roles of
 _
 _,
we can iteratively compute the approximated
(\mathbf  _)
(\mathbf  _)
of the true model's marginals
 _\mid \mathbf  )
 _\mid \mathbf  ),
respectively. Although this iterative scheme is guaranteed to converge monotonically, the converged
is only a local minimizer of
 (Q\parallel P)
If the constrained space

is confined within independent space, i.e.
(\mathbf  _\mid \mathbf  _)=q^(\mathbf  ),
the above iterative scheme will become the so-called mean field approximation
(\mathbf  )=q^(\mathbf  _)q^(\mathbf  _),
as shown below.
The variational distribution
 )
is usually assumed to factorize over some partition of the latent variables, i.e. for some partition of the latent variables
 
 _\dots \mathbf  _
 )=\prod _^q_(\mathbf  _\mid \mathbf  )
It can be shown using the calculus of variations (hence the name "variational Bayes") that the "best" distribution
for each of the factors
(in terms of the distribution minimizing the KL divergence, as described above) satisfies:
^(\mathbf  _\mid \mathbf  )= _^[\ln p(\mathbf  ,\mathbf  )] _^[\ln p(\mathbf  ,\mathbf  )]\,d\mathbf  _
 _^[\ln p(\mathbf  ,\mathbf  )]
is the expectation of the logarithm of the joint probability of the data and latent variables, taken with respect to
over all variables not in the partition: refer to Lemma 4.1 of for a derivation of the distribution
^(\mathbf  _\mid \mathbf  )
In practice, we usually work in terms of logarithms, i.e.:
^(\mathbf  _\mid \mathbf  )=\operatorname  _^[\ln p(\mathbf  ,\mathbf  )]+
The constant in the above expression is related to the normalizing constant (the denominator in the expression above for
) and is usually reinstated by inspection, as the rest of the expression can usually be recognized as being a known type of distribution (e.g. Gaussian, gamma, etc.).
Using the properties of expectations, the expression
 _^[\ln p(\mathbf  ,\mathbf  )]
can usually be simplified into a function of the fixed hyperparameters of the prior distributions over the latent variables and of expectations (and sometimes higher moments such as the variance) of latent variables not in the current partition (i.e. latent variables not included in
 _
). This creates circular dependencies between the parameters of the distributions over variables in one partition and the expectations of variables in the other partitions. This naturally suggests an iterative algorithm, much like EM (the expectation–maximization algorithm), in which the expectations (and possibly higher moments) of the latent variables are initialized in some fashion (perhaps randomly), and then the parameters of each distribution are computed in turn using the current values of the expectations, after which the expectation of the newly computed distribution is set appropriately according to the computed parameters. An algorithm of this sort is guaranteed to converge.
In other words, for each of the partitions of variables, by simplifying the expression for the distribution over the partition's variables and examining the distribution's functional dependency on the variables in question, the family of the distribution can usually be determined (which in turn determines the value of the constant). The formula for the distribution's parameters will be expressed in terms of the prior distributions' hyperparameters (which are known constants), but also in terms of expectations of functions of variables in other partitions. Usually these expectations can be simplified into functions of expectations of the variables themselves (i.e. the means); sometimes expectations of squared variables (which can be related to the variance of the variables), or expectations of higher powers (i.e. higher moments) also appear. In most cases, the other variables' distributions will be from known families, and the formulas for the relevant expectations can be looked up. However, those formulas depend on those distributions' parameters, which depend in turn on the expectations about other variables. The result is that the formulas for the parameters of each variable's distributions can be expressed as a series of equations with mutual, nonlinear dependencies among the variables. Usually, it is not possible to solve this system of equations directly. However, as described above, the dependencies suggest a simple iterative algorithm, which in most cases is guaranteed to converge. An example will make this process clearer.
The following theorem is referred to as a duality formula for variational inference. It explains some important properties of the variational distributions used in variational Bayes methods.
Theorem Consider two probability spaces
,P)
,Q)

. Assume that there is a common dominating probability measure



denote any real-valued random variable on
,P)
(P)
. Then the following equality holds
[\exp h]=_\[h]-D_(Q\parallel P)\.
Further, the supremum on the right-hand side is attained if and only if it holds
=[\exp h],
almost surely with respect to probability measure


denote the Radon–Nikodym derivatives of the probability measures
with respect to

Consider a simple non-hierarchical Bayesian model consisting of a set of i.i.d. observations from a Gaussian distribution, with unknown mean and variance. In the following, we work through this model in great detail to illustrate the workings of the variational Bayes method.
For mathematical convenience, in the following example we work in terms of the precision — i.e. the reciprocal of the variance (or in a multivariate Gaussian, the inverse of the covariance matrix) — rather than the variance itself. (From a theoretical standpoint, precision and variance are equivalent since there is a one-to-one correspondence between the two.)
We place conjugate prior distributions on the unknown mean


, i.e. the mean also follows a Gaussian distribution while the precision follows a gamma distribution. In other words:
number of data points
\tau &\sim \operatorname  (a_,b_)\\\mu |\tau &\sim (\mu _,(\lambda _\tau )^)\\\,\dots ,x_\&\sim (\mu ,\tau ^)\\N&=\end
,\lambda _,a_
in the prior distributions are fixed, given values. They can be set to small positive numbers to give broad prior distributions indicating ignorance about the prior distributions of


We are given
 =\,\ldots ,x_\
and our goal is to infer the posterior distribution
,\ldots ,x_)
of the parameters


The joint probability of all variables can be rewritten as
 ,\mu ,\tau )=p(\mathbf  \mid \mu ,\tau )p(\mu \mid \tau )p(\tau )
where the individual factors are
p(\mathbf  \mid \mu ,\tau )&=\prod _^(x_\mid \mu ,\tau ^)\\p(\mu \mid \tau )&=\left(\mu \mid \mu _,(\lambda _\tau )^\right)\\p(\tau )&=\operatorname  (\tau \mid a_,b_)\end
(x\mid \mu ,\sigma ^)&=e^\\\operatorname  (\tau \mid a,b)&=b^\tau ^e^\end

, i.e. that the posterior distribution factorizes into independent factors for


. This type of assumption underlies the variational Bayesian method. The true posterior distribution does not in fact factor this way (in fact, in this simple case, it is known to be a Gaussian-gamma distribution), and hence the result we obtain will be an approximation.
\ln q_^(\mu )&=\operatorname  _\left[\ln p(\mathbf  \mid \mu ,\tau )+\ln p(\mu \mid \tau )+\ln p(\tau )\right]+C\\&=\operatorname  _\left[\ln p(\mathbf  \mid \mu ,\tau )\right]+\operatorname  _\left[\ln p(\mu \mid \tau )\right]+\operatorname  _\left[\ln p(\tau )\right]+C\\&=\operatorname  _\left[\ln \prod _^\left(x_\mid \mu ,\tau ^\right)\right]+\operatorname  _\left[\ln \left(\mu \mid \mu _,(\lambda _\tau )^\right)\right]+C_\\&=\operatorname  _\left[\ln \prod _^e^-\mu )^\tau \right]+\operatorname  _\left[\ln \tau e^)^\lambda _\tau \right]+C_\\&=\operatorname  _\left[\sum _^\left((\ln \tau -\ln 2\pi )--\mu )^\tau \right)\right]+\operatorname  _\left[(\ln \lambda _+\ln \tau -\ln 2\pi )-)^\lambda _\tau \right]+C_\\&=\operatorname  _\left[\sum _^--\mu )^\tau \right]+\operatorname  _\left[-)^\lambda _\tau \right]+\operatorname  _\left[\sum _^(\ln \tau -\ln 2\pi )\right]+\operatorname  _\left[(\ln \lambda _+\ln \tau -\ln 2\pi )\right]+C_\\&=\operatorname  _\left[\sum _^--\mu )^\tau \right]+\operatorname  _\left[-)^\lambda _\tau \right]+C_\\&=- _[\tau ]\left\^(x_-\mu )^+\lambda _(\mu -\mu _)^\right\+C_\end
In the above derivation,
refer to values that are constant with respect to

. Note that the term
 _[\ln p(\tau )]
is not a function of

and will have the same value regardless of the value of

. Hence in line 3 we can absorb it into the constant term at the end. We do the same thing in line 7.
The last line is simply a quadratic polynomial in

. Since this is the logarithm of
^(\mu )
, we can see that
^(\mu )
itself is a Gaussian distribution.
With a certain amount of tedious math (expanding the squares inside of the braces, separating out and grouping the terms involving


and completing the square over

), we can derive the parameters of the Gaussian distribution:
\ln q_^(\mu )&=- _[\tau ]\left\^(x_-\mu )^+\lambda _(\mu -\mu _)^\right\+C_\\&=- _[\tau ]\left\^(x_^-2x_\mu +\mu ^)+\lambda _(\mu ^-2\mu _\mu +\mu _^)\right\+C_\\&=- _[\tau ]\left\^x_^\right)-2\left(\sum _^x_\right)\mu +\left(\sum _^\mu ^\right)+\lambda _\mu ^-2\lambda _\mu _\mu +\lambda _\mu _^\right\+C_\\&=- _[\tau ]\left\+N)\mu ^-2\left(\lambda _\mu _+\sum _^x_\right)\mu +\left(\sum _^x_^\right)+\lambda _\mu _^\right\+C_\\&=- _[\tau ]\left\+N)\mu ^-2\left(\lambda _\mu _+\sum _^x_\right)\mu \right\+C_\\&=- _[\tau ]\left\+N)\mu ^-2\left(\mu _+\sum _^x_+N\right)(\lambda _+N)\mu \right\+C_\\&=- _[\tau ]\left\+N)\left(\mu ^-2\left(\mu _+\sum _^x_+N\right)\mu \right)\right\+C_\\&=- _[\tau ]\left\+N)\left(\mu ^-2\left(\mu _+\sum _^x_+N\right)\mu +\left(\mu _+\sum _^x_+N\right)^-\left(\mu _+\sum _^x_+N\right)^\right)\right\+C_\\&=- _[\tau ]\left\+N)\left(\mu ^-2\left(\mu _+\sum _^x_+N\right)\mu +\left(\mu _+\sum _^x_+N\right)^\right)\right\+C_\\&=- _[\tau ]\left\+N)\left(\mu -\mu _+\sum _^x_+N\right)^\right\+C_\\&=-(\lambda _+N)\operatorname  _[\tau ]\left(\mu -\mu _+\sum _^x_+N\right)^+C_\end
Note that all of the above steps can be shortened by using the formula for the sum of two quadratics.
In other words:
q_^(\mu )&\sim (\mu \mid \mu _,\lambda _^)\\\mu _&=\mu _+N+N\\\lambda _&=(\lambda _+N)\operatorname  _[\tau ]\\&=\sum _^x_\end
The derivation of
^(\tau )
is similar to above, although we omit some of the details for the sake of brevity.
\ln q_^(\tau )&=\operatorname  _[\ln p(\mathbf  \mid \mu ,\tau )+\ln p(\mu \mid \tau )]+\ln p(\tau )+\\&=(a_-1)\ln \tau -b_\tau +\ln \tau +\ln \tau -\operatorname  _\left[\sum _^(x_-\mu )^+\lambda _(\mu -\mu _)^\right]+\end
Exponentiating both sides, we can see that
^(\tau )
is a gamma distribution. Specifically:
q_^(\tau )&\sim \operatorname  (\tau \mid a_,b_)\\a_&=a_+\\b_&=b_+\operatorname  _\left[\sum _^(x_-\mu )^+\lambda _(\mu -\mu _)^\right]\end
Let us recap the conclusions from the previous sections:
q_^(\mu )&\sim (\mu \mid \mu _,\lambda _^)\\\mu _&=\mu _+N+N\\\lambda _&=(\lambda _+N)\operatorname  _[\tau ]\\&=\sum _^x_\end
q_^(\tau )&\sim \operatorname  (\tau \mid a_,b_)\\a_&=a_+\\b_&=b_+\operatorname  _\left[\sum _^(x_-\mu )^+\lambda _(\mu -\mu _)^\right]\end
In each case, the parameters for the distribution over one of the variables depend on expectations taken with respect to the other variable. We can expand the expectations, using the standard formulas for the expectations of moments of the Gaussian and gamma distributions:
\operatorname  [\tau \mid a_,b_]&=\\\operatorname  \left[\mu \mid \mu _,\lambda _^\right]&=\mu _\\\operatorname  \left[X^\right]&=\operatorname  (X)+(\operatorname  [X])^\\\operatorname  \left[\mu ^\mid \mu _,\lambda _^\right]&=\lambda _^+\mu _^\end
Applying these formulas to the above equations is trivial in most cases, but the equation for
takes more work:
b_&=b_+\operatorname  _\left[\sum _^(x_-\mu )^+\lambda _(\mu -\mu _)^\right]\\&=b_+\operatorname  _\left[(\lambda _+N)\mu ^-2\left(\lambda _\mu _+\sum _^x_\right)\mu +\left(\sum _^x_^\right)+\lambda _\mu _^\right]\\&=b_+\left[(\lambda _+N)\operatorname  _[\mu ^]-2\left(\lambda _\mu _+\sum _^x_\right)\operatorname  _[\mu ]+\left(\sum _^x_^\right)+\lambda _\mu _^\right]\\&=b_+\left[(\lambda _+N)\left(\lambda _^+\mu _^\right)-2\left(\lambda _\mu _+\sum _^x_\right)\mu _+\left(\sum _^x_^\right)+\lambda _\mu _^\right]\\\end
We can then write the parameter equations as follows, without any expectations:
\mu _&=\mu _+N+N\\\lambda _&=(\lambda _+N)\\&=\sum _^x_\\a_&=a_+\\b_&=b_+\left[(\lambda _+N)\left(\lambda _^+\mu _^\right)-2\left(\lambda _\mu _+\sum _^x_\right)\mu _+\left(\sum _^x_^\right)+\lambda _\mu _^\right]\end
Note that there are circular dependencies among the formulas for

. This naturally suggests an EM-like algorithm:
^x_
^x_^.
Use these values to compute


to some arbitrary value.
Use the current value of
,
along with the known values of the other parameters, to compute
Use the current value of
along with the known values of the other parameters, to compute

Repeat the last two steps until convergence (i.e. until neither value has changed more than some small amount).
We then have values for the hyperparameters of the approximating distributions of the posterior parameters, which we can use to compute any properties we want of the posterior — e.g. its mean and variance, a 95% highest-density region (the smallest interval that includes 95% of the total probability), etc.
It can be shown that this algorithm is guaranteed to converge to a local maximum.
Note also that the posterior distributions have the same form as the corresponding prior distributions. We did not assume this; the only assumption we made was that the distributions factorize, and the form of the distributions followed naturally. It turns out (see below) that the fact that the posterior distributions have the same form as the prior distributions is not a coincidence, but a general result whenever the prior distributions are members of the exponential family, which is the case for most of the standard distributions.
The above example shows the method by which the variational-Bayesian approximation to a posterior probability density in a given Bayesian network is derived:
Describe the network with a graphical model, identifying the observed variables (data)
 
and unobserved variables (parameters

and latent variables
 
) and their conditional probability distributions. Variational Bayes will then construct an approximation to the posterior probability
 ,\mid \mathbf  )
. The approximation has the basic property that it is a factorized distribution, i.e. a product of two or more independent distributions over disjoint subsets of the unobserved variables.
Partition the unobserved variables into two or more subsets, over which the independent factors will be derived. There is no universal procedure for doing this; creating too many subsets yields a poor approximation, while creating too few makes the entire variational Bayes procedure intractable. Typically, the first split is to separate the parameters and latent variables; often, this is enough by itself to produce a tractable result. Assume that the partitions are called
 _,\ldots ,\mathbf  _
For a given partition
 _
, write down the formula for the best approximating distribution
^(\mathbf  _\mid \mathbf  )
using the basic equation
^(\mathbf  _\mid \mathbf  )=\operatorname  _[\ln p(\mathbf  ,\mathbf  )]+
Fill in the formula for the joint probability distribution using the graphical model. Any component conditional distributions that don't involve any of the variables in
 _
can be ignored; they will be folded into the constant term.
Simplify the formula and apply the expectation operator, following the above example. Ideally, this should simplify into expectations of basic functions of variables not in
 _
(e.g. first or second raw moments, expectation of a logarithm, etc.). In order for the variational Bayes procedure to work well, these expectations should generally be expressible analytically as functions of the parameters and/or hyperparameters of the distributions of these variables. In all cases, these expectation terms are constants with respect to the variables in the current partition.
The functional form of the formula with respect to the variables in the current partition indicates the type of distribution. In particular, exponentiating the formula generates the probability density function (PDF) of the distribution (or at least, something proportional to it, with unknown normalization constant). In order for the overall method to be tractable, it should be possible to recognize the functional form as belonging to a known distribution. Significant mathematical manipulation may be required to convert the formula into a form that matches the PDF of a known distribution. When this can be done, the normalization constant can be reinstated by definition, and equations for the parameters of the known distribution can be derived by extracting the appropriate parts of the formula.
When all expectations can be replaced analytically with functions of variables not in the current partition, and the PDF put into a form that allows identification with a known distribution, the result is a set of equations expressing the values of the optimum parameters as functions of the parameters of variables in other partitions.
When this procedure can be applied to all partitions, the result is a set of mutually linked equations specifying the optimum values of all parameters.
An expectation–maximization (EM) type procedure is then applied, picking an initial value for each parameter and the iterating through a series of steps, where at each step we cycle through the equations, updating each parameter in turn. This is guaranteed to converge.
Due to all of the mathematical manipulations involved, it is easy to lose track of the big picture. The important things are:
The idea of variational Bayes is to construct an analytical approximation to the posterior probability of the set of unobserved variables (parameters and latent variables), given the data. This means that the form of the solution is similar to other Bayesian inference methods, such as Gibbs sampling — i.e. a distribution that seeks to describe everything that is known about the variables. As in other Bayesian methods — but unlike e.g. in expectation–maximization (EM) or other maximum likelihood methods — both types of unobserved variables (i.e. parameters and latent variables) are treated the same, i.e. as random variables. Estimates for the variables can then be derived in the standard Bayesian ways, e.g. calculating the mean of the distribution to get a single point estimate or deriving a credible interval, highest density region, etc.
"Analytical approximation" means that a formula can be written down for the posterior distribution. The formula generally consists of a product of well-known probability distributions, each of which factorizes over a set of unobserved variables (i.e. it is conditionally independent of the other variables, given the observed data). This formula is not the true posterior distribution, but an approximation to it; in particular, it will generally agree fairly closely in the lowest moments of the unobserved variables, e.g. the mean and variance.
The result of all of the mathematical manipulations is (1) the identity of the probability distributions making up the factors, and (2) mutually dependent formulas for the parameters of these distributions. The actual values of these parameters are computed numerically, through an alternating iterative procedure much like EM.
Variational Bayes (VB) is often compared with expectation–maximization (EM). The actual numerical procedure is quite similar, in that both are alternating iterative procedures that successively converge on optimum parameter values. The initial steps to derive the respective procedures are also vaguely similar, both starting out with formulas for probability densities and both involving significant amounts of mathematical manipulations.
However, there are a number of differences. Most important is what is being computed.
EM computes point estimates of posterior distribution of those random variables that can be categorized as "parameters", but only estimates of the actual posterior distributions of the latent variables (at least in "soft EM", and often only when the latent variables are discrete). The point estimates computed are the modes of these parameters; no other information is available.
VB, on the other hand, computes estimates of the actual posterior distribution of all variables, both parameters and latent variables. When point estimates need to be derived, generally the mean is used rather than the mode, as is normal in Bayesian inference. Concomitant with this, the parameters computed in VB do not have the same significance as those in EM. EM computes optimum values of the parameters of the Bayes network itself. VB computes optimum values of the parameters of the distributions used to approximate the parameters and latent variables of the Bayes network. For example, a typical Gaussian mixture model will have parameters for the mean and variance of each of the mixture components. EM would directly estimate optimum values for these parameters. VB, however, would first fit a distribution to these parameters — typically in the form of a prior distribution, e.g. a normal-scaled inverse gamma distribution — and would then compute values for the parameters of this prior distribution, i.e. essentially hyperparameters. In this case, VB would compute optimum estimates of the four parameters of the normal-scaled inverse gamma distribution that describes the joint distribution of the mean and variance of the component.
Imagine a Bayesian Gaussian mixture model described as follows:
number of mixing components
number of data points
\mathbf  &\sim \operatorname  (K,\alpha _)\\\mathbf  _&\sim (\mathbf  _,\nu _)\\\mathbf  _&\sim (\mathbf  _,(\beta _\mathbf  _)^)\\\mathbf  [i=1\dots N]&\sim \operatorname  (1,\mathbf  )\\\mathbf  _&\sim (\mathbf  _, _^)\\K&=\\N&=\end
SymDir() is the symmetric Dirichlet distribution of dimension
, with the hyperparameter for each component set to

. The Dirichlet distribution is the conjugate prior of the categorical distribution or multinomial distribution.
()
is the Wishart distribution, which is the conjugate prior of the precision matrix (inverse covariance matrix) for a multivariate Gaussian distribution.
Mult() is a multinomial distribution over a single observation (equivalent to a categorical distribution). The state space is a "one-of-K" representation, i.e., a
-dimensional vector in which one of the elements is 1 (specifying the identity of the observation) and all other elements are 0.
()
is the Gaussian distribution, in this case specifically the multivariate Gaussian distribution.
The interpretation of the above variables is as follows:
 =\ _,\dots ,\mathbf  _\
is the set of
data points, each of which is a
-dimensional vector distributed according to a multivariate Gaussian distribution.
 =\ _,\dots ,\mathbf  _\
is a set of latent variables, one per data point, specifying which mixture component the corresponding data point belongs to, using a "one-of-K" vector representation with components

, as described above.
 
is the mixing proportions for the
 _
 _
specify the parameters (mean and precision) associated with each mixture component.
The joint probability of all variables can be rewritten as
 ,\mathbf  ,\mathbf  ,\mathbf  ,\mathbf  )=p(\mathbf  \mid \mathbf  ,\mathbf  ,\mathbf  )p(\mathbf  \mid \mathbf  )p(\mathbf  )p(\mathbf  \mid \mathbf  )p(\mathbf  )
where the individual factors are
p(\mathbf  \mid \mathbf  ,\mathbf  ,\mathbf  )&=\prod _^\prod _^(\mathbf  _\mid \mathbf  _,\mathbf  _^)^\\p(\mathbf  \mid \mathbf  )&=\prod _^\prod _^\pi _^\\p(\mathbf  )&=))^\prod _^\pi _^-1\\p(\mathbf  \mid \mathbf  )&=\prod _^(\mathbf  _\mid \mathbf  _,(\beta _\mathbf  _)^)\\p(\mathbf  )&=\prod _^(\mathbf  _\mid \mathbf  _,\nu _)\end
dimensionality of each data point
(\mathbf  \mid \mathbf  ,\mathbf  )&= |^\exp \left\(\mathbf  -\mathbf  )^\mathbf  ^(\mathbf  -\mathbf  )\right\\\(\mathbf  \mid \mathbf  ,\nu )&=B(\mathbf  ,\nu )|\mathbf  |^\exp \left(-\operatorname  (\mathbf  ^\mathbf  )\right)\\B(\mathbf  ,\nu )&=|\mathbf  |^\left\\pi ^\prod _^\Gamma \left(\right)\right\^\\D&=\end
 ,\mathbf  ,\mathbf  ,\mathbf  )=q(\mathbf  )q(\mathbf  ,\mathbf  ,\mathbf  )
\ln q^(\mathbf  )&=\operatorname  _ ,\mathbf  ,\mathbf  [\ln p(\mathbf  ,\mathbf  ,\mathbf  ,\mathbf  ,\mathbf  )]+\\&=\operatorname  _ [\ln p(\mathbf  \mid \mathbf  )]+\operatorname  _ ,\mathbf  [\ln p(\mathbf  \mid \mathbf  ,\mathbf  ,\mathbf  )]+\\&=\sum _^\sum _^z_\ln \rho _+\end
where we have defined
=\operatorname  [\ln \pi _]+\operatorname  [\ln |\mathbf  _|]-\ln(2\pi )-\operatorname  _ _,\mathbf  _[(\mathbf  _-\mathbf  _)^\mathbf  _(\mathbf  _-\mathbf  _)]
Exponentiating both sides of the formula for
(\mathbf  )
(\mathbf  )\propto \prod _^\prod _^\rho _^
Requiring that this be normalized ends up requiring that the

sum to 1 over all values of
(\mathbf  )=\prod _^\prod _^r_^
=^\rho _
In other words,
(\mathbf  )
is a product of single-observation multinomial distributions, and factors over each individual
 _
, which is distributed as a single-observation multinomial distribution with parameters

Furthermore, we note that
 [z_]=r_\,
which is a standard result for categorical distributions.
Now, considering the factor
 ,\mathbf  ,\mathbf  )
, note that it automatically factors into
 )\prod _^q(\mathbf  _,\mathbf  _)
due to the structure of the graphical model defining our Gaussian mixture model, which is specified above.
\ln q^(\mathbf  )&=\ln p(\mathbf  )+\operatorname  _ [\ln p(\mathbf  \mid \mathbf  )]+\\&=(\alpha _-1)\sum _^\ln \pi _+\sum _^\sum _^r_\ln \pi _+\end
Taking the exponential of both sides, we recognize
(\mathbf  )
as a Dirichlet distribution
(\mathbf  )\sim \operatorname  (\mathbf  )\,
=\alpha _+N_\,
=\sum _^r_\,
(\mathbf  _,\mathbf  _)=\ln p(\mathbf  _,\mathbf  _)+\sum _^\operatorname  [z_]\ln (\mathbf  _\mid \mathbf  _,\mathbf  _^)+
Grouping and reading off terms involving
 _
 _
, the result is a Gaussian-Wishart distribution given by
(\mathbf  _,\mathbf  _)=(\mathbf  _\mid \mathbf  _,(\beta _\mathbf  _)^)(\mathbf  _\mid \mathbf  _,\nu _)
given the definitions
\beta _&=\beta _+N_\\\mathbf  _&=(\beta _\mathbf  _+N_ _)\\\mathbf  _^&=\mathbf  _^+N_\mathbf  _+N_+N_( _-\mathbf  _)( _-\mathbf  _)^\\\nu _&=\nu _+N_\\N_&=\sum _^r_\\ _&=\sum _^r_\mathbf  _\\\mathbf  _&=\sum _^r_(\mathbf  _- _)(\mathbf  _- _)^\end
Finally, notice that these functions require the values of
, which make use of

, which is defined in turn based on
 [\ln \pi _]
 [\ln |\mathbf  _|]
 _ _,\mathbf  _[(\mathbf  _-\mathbf  _)^\mathbf  _(\mathbf  _-\mathbf  _)]
. Now that we have determined the distributions over which these expectations are taken, we can derive formulas for them:
\operatorname  _ _,\mathbf  _[(\mathbf  _-\mathbf  _)^\mathbf  _(\mathbf  _-\mathbf  _)]&=D\beta _^+\nu _(\mathbf  _-\mathbf  _)^\mathbf  _(\mathbf  _-\mathbf  _)\\\ln _&\equiv \operatorname  [\ln |\mathbf  _|]=\sum _^\psi \left(+1-i\right)+D\ln 2+\ln |\mathbf  _|\\\ln _&\equiv \operatorname  \left[\ln |\pi _|\right]=\psi (\alpha _)-\psi \left(\sum _^\alpha _\right)\end
These results lead to
\propto __^\exp \left\-(\mathbf  _-\mathbf  _)^\mathbf  _(\mathbf  _-\mathbf  _)\right\
These can be converted from proportional to absolute values by normalizing over
so that the corresponding values sum to 1.
The update equations for the parameters

 _
 _

of the variables
 _
 _
depend on the statistics
 _
 _
, and these statistics in turn depend on
The update equations for the parameters

of the variable
 
depend on the statistic
, which depends in turn on
The update equation for
has a direct circular dependence on

 _
 _

as well as an indirect circular dependence on
 _


_
_
This suggests an iterative procedure that alternates between two steps:
An E-step that computes the value of
using the current values of all the other parameters.
An M-step that uses the new value of
to compute new values of all the other parameters.
Note that these steps correspond closely with the standard EM algorithm to derive a maximum likelihood or maximum a posteriori (MAP) solution for the parameters of a Gaussian mixture model. The responsibilities
in the E step correspond closely to the posterior probabilities of the latent variables given the data, i.e.
 \mid \mathbf  )
; the computation of the statistics
 _
 _
corresponds closely to the computation of corresponding "soft-count" statistics over the data; and the use of those statistics to compute new values of the parameters corresponds closely to the use of soft counts to compute new parameter values in normal EM over a Gaussian mixture model.
Note that in the previous example, once the distribution over unobserved variables was assumed to factorize into distributions over the "parameters" and distributions over the "latent data", the derived "best" distribution for each variable was in the same family as the corresponding prior distribution over the variable. This is a general result that holds true for all prior distributions derived from the exponential family.
Variational message passing: a modular algorithm for variational Bayesian inference.
Variational autoencoder: an artificial neural network belonging to the families of probabilistic graphical models and Variational Bayesian methods.
Expectation–maximization algorithm: a related approach which corresponds to a special case of variational Bayesian inference.
Generalized filtering: a variational filtering scheme for nonlinear state space models.
Calculus of variations: the field of mathematical analysis that deals with maximizing or minimizing functionals.
Maximum entropy discrimination: This is a variational inference framework that allows for introducing and accounting for additional large-margin constraints
The on-line textbook: Information Theory, Inference, and Learning Algorithms Archived 2017-05-12 at the Wayback Machine, by David J.C. MacKay provides an introduction to variational methods (p. 422).
A Tutorial on Variational Bayes. Fox, C. and Roberts, S. 2012. Artificial Intelligence Review, doi:10.1007/s10462-011-9236-8.
Variational-Bayes Repository A repository of research papers, software, and links related to the use of variational methods for approximate Bayesian learning up to 2003.
Variational Algorithms for Approximate Bayesian Inference, by M. J. Beal includes comparisons of EM to Variational Bayesian EM and derivations of several models including Variational Bayesian HMMs.
High-Level Explanation of Variational Inference by Jason Eisner may be worth reading before a more mathematically detailed treatment.
Copula Variational Bayes inference via information geometry (pdf) by Tran, V.H. 2018. This paper is primarily written for students. Via Bregman divergence, the paper shows that Variational Bayes is simply a generalized Pythagorean projection of true model onto an arbitrarily correlated (copula) distributional space, of which the independent space is merely a special case.
An in depth introduction to Variational Bayes note. Nguyen, D. 2023 This article contains examples of Markov chains and Markov processes in action.
All examples are in the countable state space. For an overview of Markov chains in general state space, see Markov chains on a measurable state space.
A game of snakes and ladders or any other game whose moves are determined entirely by dice is a Markov chain, indeed, an absorbing Markov chain. This is in contrast to card games such as blackjack, where the cards represent a 'memory' of the past moves. To see the difference, consider the probability for a certain event in the game. In the above-mentioned dice games, the only thing that matters is the current state of the board. The next state of the board depends on the current state, and the next roll of the dice. It does not depend on how things got to their current state. In a game such as blackjack, a player can gain an advantage by remembering which cards have already been shown (and hence which cards are no longer in the deck), so the next state (or hand) of the game is not independent of the past states.
Consider a random walk on the number line where, at each step, the position (call it x) may change by +1 (to the right) or −1 (to the left) with probabilities:
 =+\left(\right)
 =1-P_ 
(where c is a constant greater than 0)
For example, if the constant, c, equals 1, the probabilities of a move to the left at positions x = −2,−1,0,1,2 are given by
,,,,
respectively. The random walk has a centering effect that weakens as c increases.
Since the probabilities depend only on the current position (value of x) and not on any prior positions, this biased random walk satisfies the definition of a Markov chain.
Suppose that one starts with 1 on an unending, fair, coin toss indefinitely, or until all of the money is lost. If
represents the number of dollars one has after n tosses, with
, then the sequence
:n\in \mathbb  \
is a Markov process. If one knows that one has 11 or 10, then went up to 10, up to 12. The fact that the guess is not improved by the knowledge of earlier tosses showcases the Markov property, the memoryless property of a stochastic process.
This example came from Markov himself. Markov chose 20,000 letters from Pushkin’s Eugene Onegin, classified them into vowels and consonants, and counted the transition probabilities.
The stationary distribution is 43.2 percent vowels and 56.8 percent consonants, which is close to the actual count in the book.
The probabilities of weather conditions (modeled as either rainy or sunny), given the weather on the preceding day,
can be represented by a transition matrix:
The matrix P represents the weather model in which a sunny day is 90% likely to be followed by another sunny day, and a rainy day is 50% likely to be followed by another rainy day. The columns can be labelled "sunny" and "rainy", and the rows can be labelled in the same order.
(P)i j is the probability that, if a given day is of type i, it will be
followed by a day of type j.
Notice that the rows of P sum to 1: this is because P is a stochastic matrix.
The weather on day 0 (today) is known to be sunny. This is represented by an initial state vector in which the "sunny" entry is 100%, and the "rainy" entry is 0%:
 ^=1&0\end
The weather on day 1 (tomorrow) can be predicted by multiplying the state vector from day 0 by the transition matrix:
 ^=\mathbf  ^P=1&0\end0.9&0.1\\0.5&0.5\end=0.9&0.1\end
Thus, there is a 90% chance that day 1 will also be sunny.
The weather on day 2 (the day after tomorrow) can be predicted in the same way, from the state vector we computed for day 1:
 ^=\mathbf  ^P=\mathbf  ^P^=1&0\end0.9&0.1\\0.5&0.5\end^=0.86&0.14\end
 ^=\mathbf  ^P=0.9&0.1\end0.9&0.1\\0.5&0.5\end=0.86&0.14\end
General rules for day n are:
 ^=\mathbf  ^P
 ^=\mathbf  ^P^
In this example, predictions for the weather on more distant days change less and less on each subsequent day and tend towards a steady state vector. This vector represents the probabilities of sunny and rainy weather on all days, and is independent of the initial weather.
The steady state vector is defined as:
 =\lim _\mathbf  ^
but converges to a strictly positive vector only if P is a regular transition matrix (that is, there
is at least one Pn with all non-zero entries).
Since q is independent from initial conditions, it must be unchanged when transformed by P. This makes it an eigenvector (with eigenvalue 1), and means it can be derived from P.
In layman's terms, the steady-state vector is the vector that, when we multiply it by P, we get the exact same vector back. For the weather example, we can use this to set up a matrix equation:
is unchanged by
P&=0.9&0.1\\0.5&0.5\end\\\mathbf  P&=\mathbf  &&\mathbf  P\\&=\mathbf  I\\\mathbf  (P-I)&=\mathbf  \\\mathbf  \left(0.9&0.1\\0.5&0.5\end-1&0\\0&1\end\right)&=\mathbf  \\\mathbf  -0.1&0.1\\0.5&-0.5\end&=\mathbf  \\q_&q_\end-0.1&0.1\\0.5&-0.5\end&=0&0\end\\-0.1q_+0.5q_&=0\end
and since they are a probability vector we know that
Solving this pair of simultaneous equations gives the steady state vector:
In conclusion, in the long term about 83.3% of days are sunny. Not all Markov processes have a steady state vector. In particular, the transition matrix must be regular. Otherwise, the state vectors will oscillate over time without converging.
A state diagram for a simple example is shown in the figure on the right, using a directed graph to picture the state transitions. The states represent whether a hypothetical stock market is exhibiting a bull market, bear market, or stagnant market trend during a given week. According to the figure, a bull week is followed by another bull week 90% of the time, a bear week 7.5% of the time, and a stagnant week the other 2.5% of the time. Labeling the state space  the transition matrix for this example is
The distribution over states can be written as a stochastic row vector x with the relation x(n + 1) = x(n)P. So if at time n the system is in state x(n), then three time periods later, at time n + 3 the distribution is
In particular, if at time n the system is in state 2 (bear), then at time n + 3 the distribution is
Using the transition matrix it is possible to calculate, for example, the long-term fraction of weeks during which the market is stagnant, or the average number of weeks it will take to go from a stagnant to a bull market. Using the transition probabilities, the steady-state probabilities indicate that 62.5% of weeks will be in a bull market, 31.25% of weeks will be in a bear market and 6.25% of weeks will be stagnant, since:
\,P^=0.625&0.3125&0.0625\\0.625&0.3125&0.0625\\0.625&0.3125&0.0625\end
A thorough development and many examples can be found in the on-line monograph Meyn & Tweedie 2005.
A finite-state machine can be used as a representation of a Markov chain. Assuming a sequence of independent and identically distributed input signals (for example, symbols from a binary alphabet chosen by coin tosses), if the machine is in state y at time n, then the probability that it moves to state x at time n + 1 depends only on the current state.
If one pops one hundred kernels of popcorn in an oven, each kernel popping at an independent exponentially-distributed time, then this would be a continuous-time Markov process. If
denotes the number of kernels which have popped up to time t, the problem can be defined as finding the number of kernels that will pop in some later time. The only thing one needs to know is the number of kernels that have popped prior to the time "t". It is not necessary to know when they popped, so knowing
for previous times "t" is not relevant.
The process described here is an approximation of a Poisson point process – Poisson processes are also Markov processes.
Mark V. Shaney
Interacting particle system
Stochastic cellular automata
Monopoly as a Markov chain   Q-learning is a reinforcement learning algorithm that trains an agent to assign values to its possible actions based on its current state, without requiring a model of the environment (model-free). It can handle problems with stochastic transitions and rewards without requiring adaptations.
For example, in a grid maze, an agent learns to reach an exit worth 10 points. At a junction, Q-learning might assign a higher value to moving right than left if right gets to the exit faster, improving this choice by trying both directions over time.
For any finite Markov decision process, Q-learning finds an optimal policy in the sense of maximizing the expected value of the total reward over any and all successive steps, starting from the current state. Q-learning can identify an optimal action-selection policy for any given finite Markov decision process, given infinite exploration time and a partly random policy.
"Q" refers to the function that the algorithm computes: the expected reward—that is, the quality—of an action taken in a given state.
Reinforcement learning involves an agent, a set of states

, and a set

of actions per state. By performing an action

, the agent transitions from state to state. Executing an action in a specific state provides the agent with a reward (a numerical score).
The goal of the agent is to maximize its total reward. It does this by adding the maximum reward attainable from future states to the reward for achieving its current state, effectively influencing the current action by the potential future reward. This potential reward is a weighted sum of expected values of the rewards of all future steps starting from the current state.
As an example, consider the process of boarding a train, in which the reward is measured by the negative of the total time spent boarding (alternatively, the cost of boarding the train is equal to the boarding time). One strategy is to enter the train door as soon as they open, minimizing the initial wait time for yourself. If the train is crowded, however, then you will have a slow entry after the initial action of entering the door as people are fighting you to depart the train as you attempt to board. The total boarding time, or cost, is then:
0 seconds wait time + 15 seconds fight time
On the next day, by random chance (exploration), you decide to wait and let other people depart first. This initially results in a longer wait time. However, less time is spent fighting the departing passengers. Overall, this path has a higher reward than that of the previous day, since the total boarding time is now:
5 second wait time + 0 second fight time
Through exploration, despite the initial (patient) action resulting in a larger cost (or negative reward) than in the forceful strategy, the overall cost is lower, thus revealing a more rewarding strategy.

steps into the future the agent will decide some next step. The weight for this step is calculated as


(the discount factor) is a number between 0 and 1 (


, it has the effect of valuing rewards received earlier higher than those received later (reflecting the value of a "good start").

may also be interpreted as the probability to succeed (or survive) at every step

The algorithm, therefore, has a function that calculates the quality of a state–action combination:
\times \to \mathbb  
Before learning begins, ⁠
⁠ is initialized to a possibly arbitrary fixed value (chosen by the programmer). Then, at each time
the agent selects an action
, observes a reward
, enters a new state
(that may depend on both the previous state
and the selected action), and
is updated. The core of the algorithm is a Bellman equation as a simple value iteration update, using the weighted average of the current value and the new information:
estimate of optimal future value
new value (temporal difference target)
(S_,A_)\leftarrow (1-\underbrace  _)\cdot \underbrace ,A_) _+\underbrace  _\cdot \underbrace  _+\underbrace  _\cdot \underbrace Q(S_,a) _ _
is the reward received when moving from the state
to the state

is the learning rate

is the sum of three terms:
,A_)
: the current value (weighted by one minus the learning rate)

: the reward
to obtain if action
is taken when in state
(weighted by learning rate)
Q(S_,a)
: the maximum reward that can be obtained from state
(weighted by learning rate and discount factor)
An episode of the algorithm ends when state
is a final or terminal state. However, Q-learning can also learn in non-episodic tasks (as a result of the property of convergent infinite series). If the discount factor is lower than 1, the action values are finite even if the problem can contain infinite loops or paths.
For all final states
is never updated, but is set to the reward value
observed for state
. In most cases,
can be taken to equal zero.
The learning rate or step size determines to what extent newly acquired information overrides old information. A factor of 0 makes the agent learn nothing (exclusively exploiting prior knowledge), while a factor of 1 makes the agent consider only the most recent information (ignoring prior knowledge to explore possibilities). In fully deterministic environments, a learning rate of
=1
is optimal. When the problem is stochastic, the algorithm converges under some technical conditions on the learning rate that require it to decrease to zero. In practice, often a constant learning rate is used, such as
=0.1
The discount factor ⁠

⁠ determines the importance of future rewards. A factor of 0 will make the agent "myopic" (or short-sighted) by only considering current rewards, i.e.
(in the update rule above), while a factor approaching 1 will make it strive for a long-term high reward. If the discount factor meets or exceeds 1, the action values may diverge. For ⁠

⁠, without a terminal state, or if the agent never reaches one, all environment histories become infinitely long, and utilities with additive, undiscounted rewards generally become infinite. Even with a discount factor only slightly lower than 1, Q-function learning leads to propagation of errors and instabilities when the value function is approximated with an artificial neural network. In that case, starting with a lower discount factor and increasing it towards its final value accelerates learning.
Since Q-learning is an iterative algorithm, it implicitly assumes an initial condition before the first update occurs. High initial values, also known as "optimistic initial conditions", can encourage exploration: no matter what action is selected, the update rule will cause it to have lower values than the other alternative, thus increasing their choice probability. The first reward
can be used to reset the initial conditions. According to this idea, the first time an action is taken the reward is used to set the value of
. This allows immediate learning in case of fixed deterministic rewards. A model that incorporates reset of initial conditions (RIC) is expected to predict participants' behavior better than a model that assumes any arbitrary initial condition (AIC). RIC seems to be consistent with human behaviour in repeated binary choice experiments.
Q-learning at its simplest stores data in tables. This approach falters with increasing numbers of states/actions since the likelihood of the agent visiting a particular state and performing a particular action is increasingly small.
Q-learning can be combined with function approximation. This makes it possible to apply the algorithm to larger problems, even when the state space is continuous.
One solution is to use an (adapted) artificial neural network as a function approximator. Another possibility is to integrate Fuzzy Rule Interpolation (FRI) and use sparse fuzzy rule-bases instead of discrete Q-tables or ANNs, which has the advantage of being a human-readable knowledge representation form. Function approximation may speed up learning in finite problems, due to the fact that the algorithm can generalize earlier experiences to previously unseen states.
Another technique to decrease the state/action space quantizes possible values. Consider the example of learning to balance a stick on a finger. To describe a state at a certain point in time involves the position of the finger in space, its velocity, the angle of the stick and the angular velocity of the stick. This yields a four-element vector that describes one state, i.e. a snapshot of one state encoded into four values. The problem is that infinitely many possible states are present. To shrink the possible space of valid actions multiple values can be assigned to a bucket. The exact distance of the finger from its starting position (-Infinity to Infinity) is not known, but rather whether it is far away or not (Near, Far).
Q-learning was introduced by Chris Watkins in 1989. A convergence proof was presented by Watkins and Peter Dayan in 1992.
Watkins was addressing “Learning from delayed rewards”, the title of his PhD thesis. Eight years earlier in 1981 the same problem, under the name of “Delayed reinforcement learning”, was solved by Bozinovski's Crossbar Adaptive Array (CAA). The memory matrix
was the same as the eight years later Q-table of Q-learning. The architecture introduced the term “state evaluation” in reinforcement learning. The crossbar learning algorithm, written in mathematical pseudocode in the paper, in each iteration performs the following computation:
In state s perform action a;
Receive consequence state s';
Compute state evaluation ⁠
Update crossbar value
The term “secondary reinforcement” is borrowed from animal learning theory, to model state values via backpropagation: the state value ⁠
⁠ of the consequence situation is backpropagated to the previously encountered situations. CAA computes state values vertically and actions horizontally (the "crossbar"). Demonstration graphs showing delayed reinforcement learning contained states (desirable, undesirable, and neutral states), which were computed by the state evaluation function. This learning system was a forerunner of the Q-learning algorithm.
In 2014, Google DeepMind patented an application of Q-learning to deep learning, titled "deep reinforcement learning" or "deep Q-learning" that can play Atari 2600 games at expert human levels.
The DeepMind system used a deep convolutional neural network, with layers of tiled convolutional filters to mimic the effects of receptive fields. Reinforcement learning is unstable or divergent when a nonlinear function approximator such as a neural network is used to represent Q. This instability comes from the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy of the agent and the data distribution, and the correlations between Q and the target values. The method can be used for stochastic search in various domains and applications.
The technique used experience replay, a biologically inspired mechanism that uses a random sample of prior actions instead of the most recent action to proceed. This removes correlations in the observation sequence and smooths changes in the data distribution. Iterative updates adjust Q towards target values that are only periodically updated, further reducing correlations with the target.
Because the future maximum approximated action value in Q-learning is evaluated using the same Q function as in current action selection policy, in noisy environments Q-learning can sometimes overestimate the action values, slowing the learning. A variant called Double Q-learning was proposed to correct this. Double Q-learning is an off-policy reinforcement learning algorithm, where a different policy is used for value evaluation than what is used to select the next action.
In practice, two separate value functions
are trained in a mutually symmetric fashion using separate experiences. The double Q-learning update step is then as follows:
^(s_,a_)=Q_^(s_,a_)+\alpha _(s_,a_)\left(r_+\gamma Q_^\left(s_,\mathop   _Q_^(s_,a)\right)-Q_^(s_,a_)\right)
^(s_,a_)=Q_^(s_,a_)+\alpha _(s_,a_)\left(r_+\gamma Q_^\left(s_,\mathop   _Q_^(s_,a)\right)-Q_^(s_,a_)\right).
Now the estimated value of the discounted future is evaluated using a different policy, which solves the overestimation issue.
This algorithm was later modified in 2015 and combined with deep learning, as in the DQN algorithm, resulting in Double DQN, which outperforms the original DQN algorithm.
Delayed Q-learning is an alternative implementation of the online Q-learning algorithm, with probably approximately correct (PAC) learning.
Greedy GQ is a variant of Q-learning to use in combination with (linear) function approximation. The advantage of Greedy GQ is that convergence is guaranteed even when function approximation is used to estimate the action values.
Distributional Q-learning is a variant of Q-learning which seeks to model the distribution of returns rather than the expected return of each action. It has been observed to facilitate estimate by deep neural networks and can enable alternative control methods, such as risk-sensitive control.
Q-learning has been proposed in the multi-agent setting (see Section 4.1.2 in ). One approach consists in pretending the environment is passive. Littman proposes the minimax Q learning algorithm.
The standard Q-learning algorithm (using a
table) applies only to discrete action and state spaces. Discretization of these values leads to inefficient learning, largely due to the curse of dimensionality. However, there are adaptations of Q-learning that attempt to solve this problem such as Wire-fitted Neural Network Q-Learning.
Temporal difference learning
Iterated prisoner's dilemma
Watkins, C.J.C.H. (1989). Learning from Delayed Rewards. PhD thesis, Cambridge University, Cambridge, England.
Strehl, Li, Wiewiora, Langford, Littman (2006). PAC model-free reinforcement learning
Reinforcement Learning: An Introduction by Richard Sutton and Andrew S. Barto, an online textbook. See "6.5 Q-Learning: Off-Policy TD Control".
Piqle: a Generic Java Platform for Reinforcement Learning
Reinforcement Learning Maze, a demonstration of guiding an ant through a maze using Q-learning
Q-learning work by Gerald Tesauro      Data augmentation is a statistical technique which allows maximum likelihood estimation from incomplete data. Data augmentation has important applications in Bayesian analysis, and the technique is widely used in machine learning to reduce overfitting when training machine learning models, achieved by training models on several slightly-modified copies of existing data.
Synthetic Minority Over-sampling Technique (SMOTE) is a method used to address imbalanced datasets in machine learning. In such datasets, the number of samples in different classes varies significantly, leading to biased model performance. For example, in a medical diagnosis dataset with 90 samples representing healthy individuals and only 10 samples representing individuals with a particular disease, traditional algorithms may struggle to accurately classify the minority class. SMOTE rebalances the dataset by generating synthetic samples for the minority class. For instance, if there are 100 samples in the majority class and 10 in the minority class, SMOTE can create synthetic samples by randomly selecting a minority class sample and its nearest neighbors, then generating new samples along the line segments joining these neighbors. This process helps increase the representation of the minority class, improving model performance.
When convolutional neural networks grew larger in mid-1990s, there was a lack of data to use, especially considering that some part of the overall dataset should be spared for later testing. It was proposed to perturb existing data with affine transformations to create new examples with the same labels, which were complemented by so-called elastic distortions in 2003, and the technique was widely used as of 2010s. Data augmentation can enhance CNN performance and acts as a countermeasure against CNN profiling attacks.
Data augmentation has become fundamental in image classification, enriching training dataset diversity to improve model generalization and performance. The evolution of this practice has introduced a broad spectrum of techniques, including geometric transformations, color space adjustments, and noise injection.
Geometric transformations alter the spatial properties of images to simulate different perspectives, orientations, and scales. Common techniques include:
Rotation: Rotating images by a specified degree to help models recognize objects at various angles.
Flipping: Reflecting images horizontally or vertically to introduce variability in orientation.
Cropping: Removing sections of the image to focus on particular features or simulate closer views.
Translation: Shifting images in different directions to teach models positional invariance.
Morphing within the same class: Generating new samples by applying morphing techniques between two images belonging to the same class, thereby increasing intra-class diversity.
Color space transformations modify the color properties of images, addressing variations in lighting, color saturation, and contrast. Techniques include:
Brightness Adjustment: Varying the image's brightness to simulate different lighting conditions.
Contrast Adjustment: Changing the contrast to help models recognize objects under various clarity levels.
Saturation Adjustment: Altering saturation to prepare models for images with diverse color intensities.
Color Jittering: Randomly adjusting brightness, contrast, saturation, and hue to introduce color variability.
Injecting noise into images simulates real-world imperfections, teaching models to ignore irrelevant variations. Techniques involve:
Gaussian Noise: Adding Gaussian noise mimics sensor noise or graininess.
Salt and Pepper Noise: Introducing black or white pixels at random simulates sensor dust or dead pixels.
Residual or block bootstrap can be used for time series augmentation.
Synthetic data augmentation is of paramount importance for machine learning classification, particularly for biological data, which tend to be high dimensional and scarce. The applications of robotic control and augmentation in disabled and able-bodied subjects still rely mainly on subject-specific analyses. Data scarcity is notable in signal processing problems such as for Parkinson's Disease Electromyography signals, which are difficult to source - Zanini, et al. noted that it is possible to use a generative adversarial network (in particular, a DCGAN) to perform style transfer in order to generate synthetic electromyographic signals that corresponded to those exhibited by sufferers of Parkinson's Disease.
The approaches are also important in electroencephalography (brainwaves). Wang, et al. explored the idea of using deep convolutional neural networks for EEG-Based Emotion Recognition, results show that emotion recognition was improved when data augmentation was used.
A common approach is to generate synthetic signals by re-arranging components of real data. Lotte proposed a method of "Artificial Trial Generation Based on Analogy" where three data examples
provide examples and an artificial
is formed which is to
. A transformation is applied to
to make it more similar to
, the same transformation is then applied to
. This approach was shown to improve performance of a Linear Discriminant Analysis classifier on three different datasets.
Current research shows great impact can be derived from relatively simple techniques. For example, Freer observed that introducing noise into gathered data to form additional data points improved the learning ability of several models which otherwise performed relatively poorly. Tsinganos et al. studied the approaches of magnitude warping, wavelet decomposition, and synthetic surface EMG models (generative approaches) for hand gesture recognition, finding classification performance increases of up to +16% when augmented data was introduced during training. More recently, data augmentation studies have begun to focus on the field of deep learning, more specifically on the ability of generative models to create artificial data which is then introduced during the classification model training process. In 2018, Luo et al. observed that useful EEG signal data could be generated by Conditional Wasserstein Generative Adversarial Networks (GANs) which was then introduced to the training set in a classical train-test learning framework. The authors found classification performance was improved when such techniques were introduced.
The prediction of mechanical signals based on data augmentation brings a new generation of technological innovations, such as new energy dispatch, 5G communication field, and robotics control engineering. In 2022, Yang et al. integrate constraints, optimization and control into a deep network framework based on data augmentation and data pruning with spatio-temporal data correlation, and improve the interpretability, safety and controllability of deep learning in real industrial projects through explicit mathematical programming equations and analytical solutions.
Oversampling and undersampling in data analysis
Generative adversarial network
Convolutional neural network Data preprocessing can refer to manipulation, filtration or augmentation of data before it is analyzed, and is often an important step in the data mining process. Data collection methods are often loosely controlled, resulting in out-of-range values, impossible data combinations, and missing values, amongst other issues.
Preprocessing is the process by which unstructured data is transformed into intelligible representations suitable for machine-learning models. This phase of model deals with noise in order to arrive at better and improved results from the original data set which was noisy. This dataset also has some level of missing value present in it.
The preprocessing pipeline used can often have large effects on the conclusions drawn from the downstream analysis. Thus, representation and quality of data is necessary before running any analysis.
Often, data preprocessing is the most important phase of a machine learning project, especially in computational biology. If there is a high proportion of irrelevant and redundant information present or noisy and unreliable data, then knowledge discovery during the training phase may be more difficult. Data preparation and filtering steps can take a considerable amount of processing time. Examples of methods used in data preprocessing include cleaning, instance selection, normalization, one-hot encoding, data transformation, feature extraction and feature selection.
Data preprocessing allows for the removal of unwanted data with the use of data cleaning, this allows the user to have a dataset to contain more valuable information after the preprocessing stage for data manipulation later in the data mining process. Editing such dataset to either correct data corruption or human error is a crucial step to get accurate quantifiers like true positives, true negatives, false positives and false negatives found in a confusion matrix that are commonly used for a medical diagnosis. Users are able to join data files together and use preprocessing to filter any unnecessary noise from the data which can allow for higher accuracy. Users use Python programming scripts accompanied by the pandas library which gives them the ability to import data from a comma-separated values as a data-frame. The data-frame is then used to manipulate data that can be challenging otherwise to do in Excel. Pandas (software) which is a powerful tool that allows for data analysis and manipulation; which makes data visualizations, statistical operations and much more, a lot easier. Many also use the R programming language to do such tasks as well.
The reason why a user transforms existing files into a new one is because of many reasons. Aspects of data preprocessing may include imputing missing values, aggregating numerical quantities and transforming continuous data into categories (data binning). More advanced techniques like principal component analysis and feature selection are working with statistical formulas and are applied to complex datasets which are recorded by GPS trackers and motion capture devices.
Semantic data mining is a subset of data mining that specifically seeks to incorporate domain knowledge, such as formal semantics, into the data mining process. Domain knowledge is the knowledge of the environment the data was processed in. Domain knowledge can have a positive influence on many aspects of data mining, such as filtering out redundant or inconsistent data during the preprocessing phase. Domain knowledge also works as constraint. It does this by using working as set of prior knowledge to reduce the space required for searching and acting as a guide to the data. Simply put, semantic preprocessing seeks to filter data using the original environment of said data more correctly and efficiently.
There are increasingly complex problems which are asking to be solved by more elaborate techniques to better analyze existing information. Instead of creating a simple script for aggregating different numerical values into a single value, it make sense to focus on semantic based data preprocessing. The idea is to build a dedicated ontology, which explains on a higher level what the problem is about. In regards to semantic data mining and semantic pre-processing, ontologies are a way to conceptualize and formally define semantic knowledge and data. The Protégé (software) is the standard tool for constructing an ontology. In general, the use of ontologies bridges the gaps between data, applications, algorithms, and results that occur from semantic mismatches. As a result, semantic data mining combined with ontology has many applications where semantic ambiguity can impact the usefulness and efficiency of data systems. Applications include the medical field, language processing, banking, and even tutoring, among many more.
There are various strengths to using a semantic data mining and ontological based approach. As previously mentioned, these tools can help during the per-processing phase by filtering out non-desirable data from the data set. Additionally, well-structured formal semantics integrated into well designed ontologies can return powerful data that can be easily read and processed by machines. A specifically useful example of this exists in the medical use of semantic data processing. As an example, a patient is having a medical emergency and is being rushed to hospital. The emergency responders are trying to figure out the best medicine to administer to help the patient. Under normal data processing, scouring all the patient’s medical data to ensure they are getting the best treatment could take too long and risk the patients’ health or even life. However, using semantically processed ontologies, the first responders could save the patient’s life. Tools like a semantic reasoner can use ontology to infer the what best medicine to administer to the patient is based on their medical history, such as if they have a certain cancer or other conditions, simply by examining the natural language used in the patient's medical records. This would allow the first responders to quickly and efficiently search for medicine without having worry about the patient’s medical history themselves, as the semantic reasoner would already have analyzed this data and found solutions. In general, this illustrates the incredible strength of using semantic data mining and ontologies. They allow for quicker and more efficient data extraction on the user side, as the user has fewer variables to account for, since the semantically pre-processed data and ontology built for the data have already accounted for many of these variables. However, there are some drawbacks to this approach. Namely, it requires a high amount of computational power and complexity, even with relatively small data sets. This could result in higher costs and increased difficulties in building and maintaining semantic data processing systems. This can be mitigated somewhat if the data set is already well organized and formatted, but even then, the complexity is still higher when compared to standard data processing.
Below is a simple a diagram combining some of the processes, in particular semantic data mining and their use in ontology.
The diagram depicts a data set being broken up into two parts: the characteristics of its domain, or domain knowledge, and then the actual acquired data. The domain characteristics are then processed to become user understood domain knowledge that can be applied to the data. Meanwhile, the data set is processed and stored so that the domain knowledge can applied to it, so that the process may continue. This application forms the ontology. From there, the ontology can be used to analyze data and process results.
Fuzzy preprocessing is another, more advanced technique for solving complex problems. Fuzzy preprocessing and fuzzy data mining make use of fuzzy sets. These data sets are composed of two elements: a set and a membership function for the set which comprises 0 and 1. Fuzzy preprocessing uses this fuzzy data set to ground numerical values with linguistic information. Raw data is then transformed into natural language. Ultimately, fuzzy data mining's goal is to help deal with inexact information, such as an incomplete database. Currently fuzzy preprocessing, as well as other fuzzy based data mining techniques see frequent use with neural networks and artificial intelligence.
Data preprocessing in predictive data mining. Knowledge Eng. Review 34: e1 (2019)   TensorFlow is a software library for machine learning and artificial intelligence. It can be used across a range of tasks, but is used mainly for training and inference of neural networks. It is one of the most popular deep learning frameworks, alongside others such as PyTorch. It is free and open-source software released under the Apache License 2.0.
It was developed by the Google Brain team for Google's internal use in research and production. The initial version was released under the Apache License 2.0 in 2015. Google released an updated version, TensorFlow 2.0, in September 2019.
TensorFlow can be used in a wide variety of programming languages, including Python, JavaScript, C++, and Java, facilitating its use in a range of applications in many sectors.
Starting in 2011, Google Brain built DistBelief as a proprietary machine learning system based on deep learning neural networks. Its use grew rapidly across diverse Alphabet companies in both research and commercial applications. Google assigned multiple computer scientists, including Jeff Dean, to simplify and refactor the codebase of DistBelief into a faster, more robust application-grade library, which became TensorFlow. In 2009, the team, led by Geoffrey Hinton, had implemented generalized backpropagation and other improvements, which allowed generation of neural networks with substantially higher accuracy, for instance a 25% reduction in errors in speech recognition.
TensorFlow is Google Brain's second-generation system. Version 1.0.0 was released on February 11, 2017. While the reference implementation runs on single devices, TensorFlow can run on multiple CPUs and GPUs (with optional CUDA and SYCL extensions for general-purpose computing on graphics processing units). TensorFlow is available on 64-bit Linux, macOS, Windows, and mobile computing platforms including Android and iOS.
Its flexible architecture allows for easy deployment of computation across a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters of servers to mobile and edge devices.
TensorFlow computations are expressed as stateful dataflow graphs. The name TensorFlow derives from the operations that such neural networks perform on multidimensional data arrays, which are referred to as tensors. During the Google I/O Conference in June 2016, Jeff Dean stated that 1,500 repositories on GitHub mentioned TensorFlow, of which only 5 were from Google.
In March 2018, Google announced TensorFlow.js version 1.0 for machine learning in JavaScript.
In Jan 2019, Google announced TensorFlow 2.0. It became officially available in September 2019.
In May 2019, Google announced TensorFlow Graphics for deep learning in computer graphics.
In May 2016, Google announced its Tensor processing unit (TPU), an application-specific integrated circuit (ASIC, a hardware chip) built specifically for machine learning and tailored for TensorFlow. A TPU is a programmable AI accelerator designed to provide high throughput of low-precision arithmetic (e.g., 8-bit), and oriented toward using or running models rather than training them. Google announced they had been running TPUs inside their data centers for more than a year, and had found them to deliver an order of magnitude better-optimized performance per watt for machine learning.
In May 2017, Google announced the second-generation, as well as the availability of the TPUs in Google Compute Engine. The second-generation TPUs deliver up to 180 teraflops of performance, and when organized into clusters of 64 TPUs, provide up to 11.5 petaflops.
In May 2018, Google announced the third-generation TPUs delivering up to 420 teraflops of performance and 128 GB high bandwidth memory (HBM). Cloud TPU v3 Pods offer 100+ petaflops of performance and 32 TB HBM.
In February 2018, Google announced that they were making TPUs available in beta on the Google Cloud Platform.
In July 2018, the Edge TPU was announced. Edge TPU is Google's purpose-built ASIC chip designed to run TensorFlow Lite machine learning (ML) models on small client computing devices such as smartphones known as edge computing.
In May 2017, Google announced a software stack specifically for mobile development, TensorFlow Lite. In January 2019, the TensorFlow team released a developer preview of the mobile GPU inference engine with OpenGL ES 3.1 Compute Shaders on Android devices and Metal Compute Shaders on iOS devices. In May 2019, Google announced that their TensorFlow Lite Micro (also known as TensorFlow Lite for Microcontrollers) and ARM's uTensor would be merging.
As TensorFlow's market share among research papers was declining to the advantage of PyTorch, the TensorFlow Team announced a release of a new major version of the library in September 2019. TensorFlow 2.0 introduced many changes, the most significant being TensorFlow eager, which changed the automatic differentiation scheme from the static computational graph to the "Define-by-Run" scheme originally made popular by Chainer and later PyTorch. Other major changes included removal of old libraries, cross-compatibility between trained models on different versions of TensorFlow, and significant improvements to the performance on GPU.
AutoDifferentiation is the process of automatically calculating the gradient vector of a model with respect to each of its parameters. With this feature, TensorFlow can automatically compute the gradients for the parameters in a model, which is useful to algorithms such as backpropagation which require gradients to optimize performance. To do so, the framework must keep track of the order of operations done to the input Tensors in a model, and then compute the gradients with respect to the appropriate parameters.
TensorFlow includes an “eager execution” mode, which means that operations are evaluated immediately as opposed to being added to a computational graph which is executed later. Code executed eagerly can be examined step-by step-through a debugger, since data is augmented at each line of code rather than later in a computational graph. This execution paradigm is considered to be easier to debug because of its step by step transparency.
In both eager and graph executions, TensorFlow provides an API for distributing computation across multiple devices with various distribution strategies. This distributed computing can often speed up the execution of training and evaluating of TensorFlow models and is a common practice in the field of AI.
To train and assess models, TensorFlow provides a set of loss functions (also known as cost functions). Some popular examples include mean squared error (MSE) and binary cross entropy (BCE).
In order to assess the performance of machine learning models, TensorFlow gives API access to commonly used metrics. Examples include various accuracy metrics (binary, categorical, sparse categorical) along with other metrics such as Precision, Recall, and Intersection-over-Union (IoU).
TensorFlow.nn is a module for executing primitive neural network operations on models. Some of these operations include variations of convolutions (1/2/3D, Atrous, depthwise), activation functions (Softmax, RELU, GELU, Sigmoid, etc.) and their variations, and other operations (max-pooling, bias-add, etc.).
TensorFlow offers a set of optimizers for training neural networks, including ADAM, ADAGRAD, and Stochastic Gradient Descent (SGD). When training a model, different optimizers offer different modes of parameter tuning, often affecting a model's convergence and performance.
TensorFlow serves as a core platform and library for machine learning. TensorFlow's APIs use Keras to allow users to make their own machine-learning models. In addition to building and training their model, TensorFlow can also help load the data to train the model, and deploy it using TensorFlow Serving.
TensorFlow provides a stable Python Application Program Interface (API), as well as APIs without backwards compatibility guarantee for Javascript, C++, and Java. Third-party language binding packages are also available for C#, Haskell, Julia, MATLAB, Object Pascal, R, Scala, Rust, OCaml, and Crystal. Bindings that are now archived and unsupported include Go and Swift.
TensorFlow also has a library for machine learning in JavaScript. Using the provided JavaScript APIs, TensorFlow.js allows users to use either Tensorflow.js models or converted models from TensorFlow or TFLite, retrain the given models, and run on the web.
LiteRT, formerly known as TensorFlow Lite, has APIs for mobile apps or embedded devices to generate and deploy TensorFlow models. These models are compressed and optimized in order to be more efficient and have a higher performance on smaller capacity devices.
LiteRT uses FlatBuffers as the data serialization format for network models, eschewing the Protocol Buffers format used by standard TensorFlow models.
TensorFlow Extended (abbrev. TFX) provides numerous components to perform all the operations needed for end-to-end production. Components include loading, validating, and transforming data, tuning, training, and evaluating the machine learning model, and pushing the model itself into production.
Numpy is one of the most popular Python data libraries, and TensorFlow offers integration and compatibility with its data structures. Numpy NDarrays, the library's native datatype, are automatically converted to TensorFlow Tensors in TF operations; the same is also true vice versa. This allows for the two libraries to work in unison without requiring the user to write explicit data conversions. Moreover, the integration extends to memory optimization by having TF Tensors share the underlying memory representations of Numpy NDarrays whenever possible.
TensorFlow also offers a variety of libraries and extensions to advance and extend the models and methods used. For example, TensorFlow Recommenders and TensorFlow Graphics are libraries for their respective functional. Other add-ons, libraries, and frameworks include TensorFlow Model Optimization, TensorFlow Probability, TensorFlow Quantum, and TensorFlow Decision Forests.
Google also released Colaboratory, a TensorFlow Jupyter notebook environment that does not require any setup. It runs on Google Cloud and allows users free access to GPUs and the ability to store and share notebooks on Google Drive.
Google JAX is a machine learning framework for transforming numerical functions. It is described as bringing together a modified version of autograd (automatic obtaining of the gradient function through differentiation of a function) and TensorFlow's XLA (Accelerated Linear Algebra). It is designed to follow the structure and workflow of NumPy as closely as possible and works with TensorFlow as well as other frameworks such as PyTorch. The primary functions of JAX are:
grad: automatic differentiation
pmap: SPMD programming
GE Healthcare used TensorFlow to increase the speed and accuracy of MRIs in identifying specific body parts. Google used TensorFlow to create DermAssist, a free mobile application that allows users to take pictures of their skin and identify potential health complications. Sinovation Ventures used TensorFlow to identify and classify eye diseases from optical coherence tomography (OCT) scans.
Twitter implemented TensorFlow to rank tweets by importance for a given user, and changed their platform to show tweets in order of this ranking. Previously, tweets were simply shown in reverse chronological order. The photo sharing app VSCO used TensorFlow to help suggest custom filters for photos.
Google officially released RankBrain on October 26, 2015, backed by TensorFlow.
InSpace, a virtual learning platform, used TensorFlow to filter out toxic chat messages in classrooms. Liulishuo, an online English learning platform, utilized TensorFlow to create an adaptive curriculum for each student. TensorFlow was used to accurately assess a student's current abilities, and also helped decide the best future content to show based on those capabilities.
The e-commerce platform Carousell used TensorFlow to provide personalized recommendations for customers. The cosmetics company ModiFace used TensorFlow to create an augmented reality experience for customers to test various shades of make-up on their face.
TensorFlow is the foundation for the automated image-captioning software DeepDream.
Comparison of deep learning software
Learning TensorFlow.js Book (ENG) PyTorch is an open-source machine learning library based on the Torch library, used for applications such as computer vision, deep learning research and natural language processing, originally developed by Meta AI and now part of the Linux Foundation umbrella. It is one of the most popular deep learning frameworks, alongside others such as TensorFlow, offering free and open-source software released under the modified BSD license. Although the Python interface is more polished and the primary focus of development, PyTorch also has a C++ interface.
PyTorch empowers researchers and developers with an intuitive, Pythonic framework for building and experimenting with deep learning models — without sacrificing performance, scalability, or production-readiness.
A number of pieces of deep learning software are built on top of PyTorch, including Tesla Autopilot, Uber's Pyro, Hugging Face's Transformers, and Catalyst.
PyTorch provides two high-level features:
Tensor computing (like NumPy) with strong acceleration via graphics processing units (GPU)
Deep neural networks built on a tape-based automatic differentiation system
In 2001, Torch was written and released under a GPL license. It was a machine-learning library written in C++, supporting methods including neural networks, SVM, hidden Markov models, etc. It was improved to Torch7 in 2012. Development on Torch ceased in 2018 and was subsumed by the PyTorch project.
Meta (formerly known as Facebook) operates both PyTorch and Convolutional Architecture for Fast Feature Embedding (Caffe2), but models defined by the two frameworks were mutually incompatible. The Open Neural Network Exchange (ONNX) project was created by Meta and Microsoft in September 2017 for converting models between frameworks. Caffe2 was merged into PyTorch at the end of March 2018. In September 2022, Meta announced that PyTorch would be governed by the independent PyTorch Foundation, a newly created subsidiary of the Linux Foundation.
PyTorch 2.0 was released on 15 March 2023, introducing TorchDynamo, a Python-level compiler that makes code run up to 2x faster, along with significant improvements in training and inference performance across major cloud platforms.
PyTorch defines a class called Tensor (torch.Tensor) to store and operate on homogeneous multidimensional rectangular arrays of numbers. PyTorch Tensors are similar to NumPy Arrays, but can also be operated on a CUDA-capable NVIDIA GPU. PyTorch has also been developing support for other GPU platforms, for example, AMD's ROCm and Apple's Metal Framework.
PyTorch supports various sub-types of Tensors.
Note that the term "tensor" here does not carry the same meaning as tensor in mathematics or physics. The meaning of the word in machine learning is only superficially related to its original meaning as a certain kind of object in linear algebra. Tensors in PyTorch are simply multi-dimensional arrays.
PyTorch defines a module called nn (torch.nn) to describe neural networks and to support training. This module offers a comprehensive collection of building blocks for neural networks, including various layers and activation functions, enabling the construction of complex models. Networks are built by inheriting from the torch.nn module and defining the sequence of operations in the forward() function.
The following program shows the low-level functionality of the library with a simple example.
The following code-block defines a neural network with linear layers using the nn module.
Comparison of deep learning software PyTorch Lightning is an open-source Python library that provides a high-level interface for PyTorch, a popular deep learning framework. It is a lightweight and high-performance framework that organizes PyTorch code to decouple research from engineering, thus making deep learning experiments easier to read and reproduce. It is designed to create scalable deep learning models that can easily run on distributed hardware while keeping the models' hardware agnostic.
In 2019, Lightning was adopted by the NeurIPS Reproducibility Challenge as a standard for submitting PyTorch code to the conference.
In 2022, the PyTorch Lightning library officially became a part of the Lightning framework, an open-source framework managed by the original creators of PyTorch Lightning.
pytorch-lightning on GitHub scikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.
It features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.
The scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project derives from its role as a "scientific toolkit for machine learning", originally developed and distributed as a third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the "well-maintained and popular" scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.
Large catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering)
Utility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search
Consistent way of running machine learning models (estimator.fit() and estimator.predict()), which libraries can implement
Declarative way of structuring a data science process (the Pipeline), including data pre-processing and model fitting
Fitting a random forest classifier:
scikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.
scikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.
scikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.
2019 Inria-French Academy of Sciences-Dassault Systèmes Innovation Prize
2022 Open Science Award for Open Source Research Software
scikit-learn on GitHub Keras is an open-source library that provides a Python interface for artificial neural networks. Keras was first independent software, then integrated into the TensorFlow library, and later added support for more. "Keras 3 is a full rewrite of Keras [and can be used] as a low-level cross-framework language to develop custom components such as layers, models, or metrics that can be used in native workflows in JAX, TensorFlow, or PyTorch — with one codebase." Keras 3 will be the default Keras version for TensorFlow 2.16 onwards, but Keras 2 can still be used.
The name 'Keras' derives from the Ancient Greek word κέρας (Keras) meaning 'horn'.
Designed to enable fast experimentation with deep neural networks, Keras focuses on being user-friendly, modular, and extensible. It was developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System), and its primary author and maintainer is François Chollet, who was a Google engineer until leaving the company in 2024. Chollet is also the author of the Xception deep neural network model.
Up until version 2.3, Keras supported multiple backends, including TensorFlow, Microsoft Cognitive Toolkit, Theano, and PlaidML.
From version 2.4 up until version 3.0, only TensorFlow was supported. Starting with version 3.0 (as well as its preview version, Keras Core), however, Keras has become multi-backend again, supporting TensorFlow, JAX, and PyTorch. It now also supports OpenVINO.
Keras contains numerous implementations of commonly used neural-network building blocks such as layers, objectives, activation functions, optimizers, and a host of tools for working with image and text data to simplify programming for deep neural networks. The code is hosted on GitHub, and community support forums include the GitHub issues page.
In addition to standard neural networks, Keras has support for convolutional and recurrent neural networks. It supports other common utility layers like dropout, batch normalization, and pooling.
Keras allows users to produce deep models on smartphones (iOS and Android), on the web, or on the Java Virtual Machine. It also allows use of distributed training of deep-learning models on clusters of graphics processing units (GPU) and tensor processing units (TPU).
Comparison of deep-learning software
Keras on GitHub The Keras Kardiotissas Monastery or simply Keras Monastery (Greek: Μονή Κεράς Καρδιώτισσας or Μονή Κεράς) is an Eastern Orthodox monastery dedicated to Virgin Mary that is situated near the village of Kera of the Heraklion regional unit in Crete, Greece. It is built on the north slopes of Mt. Dikti, at an altitude of 650 m and a location that is approximately 50 km east of Heraklion, next to the road to Lasithi Plateau.
The exact date of the monastery's establishment is unknown. However, references to it are made in manuscripts dating from the early fourteenth century. The monastery was named after an old icon of Theotokos that according to tradition was miraculous. That icon was transferred to Rome by a wine merchant in 1498, where it is now permanently enshrined in the Church of St. Alphonsus near the Esquiline Hill. The stolen icon was replaced by another one in 1735 that is also regarded as miraculous.
During the Ottoman occupation of Crete, the monastery often served as a local revolutionary center and suffered several retaliatory attacks as a result. In 1720, Kera monastery became Stauropegic (independent of the local Bishop).
The monastery is surrounded by fortified walls. The main church (katholikon) was originally built as an arch-covered single space structure and was later expanded with two narthexes and a smaller chapel. The church features murals dating to the 14th and 15th centuries.
Today, the monastery functions as a nunnery. It celebrates the birth of Mary on September 8 every year. Calliotropis keras is a species of sea snail, a marine gastropod mollusk in the family Eucyclidae.
The length of the shell reaches 14 mm.
This species occurs in the Pacific Ocean off Fiji and Tonga.
Vilvens C. (2007) New records and new species of Calliotropis from Indo-Pacific. Novapex 8 (Hors Série 5): 1–72.
"Calliotropis keras". Gastropods.com. Retrieved 15 January 2019.     Federated learning (also known as collaborative learning) is a machine learning technique in a setting where multiple entities (often called clients) collaboratively train a model while keeping their data decentralized, rather than centrally stored. A defining characteristic of federated learning is data heterogeneity. Because client data is decentralized, data samples held by each client may not be independently and identically distributed.
Federated learning is generally concerned with and motivated by issues such as data privacy, data minimization, and data access rights. Its applications involve a variety of research areas including defence, telecommunications, the Internet of things, and pharmaceuticals.
Federated learning aims at training a machine learning algorithm, for instance deep neural networks, on multiple local datasets contained in local nodes without explicitly exchanging data samples. The general principle consists in training local models on local data samples and exchanging parameters (e.g. the weights and biases of a deep neural network) between these local nodes at some frequency to generate a global model shared by all nodes.
The main difference between federated learning and distributed learning lies in the assumptions made on the properties of the local datasets, as distributed learning originally aims at parallelizing computing power where federated learning originally aims at training on heterogeneous datasets. While distributed learning also aims at training a single model on multiple servers, a common underlying assumption is that the local datasets are independent and identically distributed (i.i.d.) and roughly have the same size. None of these hypotheses are made for federated learning; instead, the datasets are typically heterogeneous and their sizes may span several orders of magnitude. Moreover, the clients involved in federated learning may be unreliable as they are subject to more failures or drop out since they commonly rely on less powerful communication media (i.e. Wi-Fi) and battery-powered systems (i.e. smartphones and IoT devices) compared to distributed learning where nodes are typically datacenters that have powerful computational capabilities and are connected to one another with fast networks.
The objective function for federated learning is as follows:
 _,\dots ,\mathbf  _)=\sum _^f_(\mathbf  _)
is the number of nodes,
 _
are the weights of model as viewed by node
's local objective function, which describes how model weights
 _
conforms to node
's local dataset.
The goal of federated learning is to train a common model on all of the nodes' local datasets, in other words:
Optimizing the objective function
 _,\dots ,\mathbf  _)
Achieving consensus on
 _
. In other words,
 _,\dots ,\mathbf  _
converge to some common
 
at the end of the training process.
In the centralized federated learning setting, a central server is used to orchestrate the different steps of the algorithms and coordinate all the participating nodes during the learning process. The server is responsible for the nodes selection at the beginning of the training process and for the aggregation of the received model updates. Since all the selected nodes have to send updates to a single entity, the server may become a bottleneck of the system.
In the decentralized federated learning setting, the nodes are able to coordinate themselves to obtain the global model. This setup prevents single point failures as the model updates are exchanged only between interconnected nodes without the orchestration of the central server. Nevertheless, the specific network topology may affect the performances of the learning process. See blockchain-based federated learning and the references therein.
An increasing number of application domains involve a large set of heterogeneous clients, e.g., mobile phones and IoT devices. Most of the existing federated learning strategies assume that local models share the same global model architecture. Recently, a new federated learning framework named HeteroFL was developed to address heterogeneous clients equipped with very different computation and communication capabilities. The HeteroFL technique can enable the training of heterogeneous local models with dynamically varying computation and non-IID data complexities while still producing a single accurate global inference model.
The iterative process of federated learning is composed of a series of fundamental client-server interactions, each of which is known as a federated learning round. Each round of this process consists in transmitting the current global model state to participating nodes, training local models on these local nodes to produce a set of potential model updates at each node, and then aggregating and processing these local updates into a single global update and applying it to the global model.
In the methodology below, a central server is used for aggregation, while local nodes perform local training depending on the central server's orders. However, other strategies lead to the same results without central servers, in a peer-to-peer approach, using gossip or consensus methodologies.
Assuming a federated round composed by one iteration of the learning process, the learning procedure can be summarized as follows:
Initialization: according to the server inputs, a machine learning model (e.g., linear regression, neural network, boosting) is chosen to be trained on local nodes and initialized. Then, nodes are activated and wait for the central server to give the calculation tasks.
Client selection: a fraction of local nodes are selected to start training on local data. The selected nodes acquire the current statistical model while the others wait for the next federated round.
Configuration: the central server orders selected nodes to undergo training of the model on their local data in a pre-specified fashion (e.g., for some mini-batch updates of gradient descent).
Reporting: each selected node sends its local model to the server for aggregation. The central server aggregates the received models and sends back the model updates to the nodes. It also handles failures for disconnected nodes or lost model updates. The next federated round is started returning to the client selection phase.
Termination: once a pre-defined termination criterion is met (e.g., a maximum number of iterations is reached or the model accuracy is greater than a threshold) the central server aggregates the updates and finalizes the global model.
The procedure considered before assumes synchronized model updates. Recent federated learning developments introduced novel techniques to tackle asynchronicity during the training process, or training with dynamically varying models. Compared to synchronous approaches where local models are exchanged once the computations have been performed for all layers of the neural network, asynchronous ones leverage the properties of neural networks to exchange model updates as soon as the computations of a certain layer are available. These techniques are also commonly referred to as split learning and they can be applied both at training and inference time regardless of centralized or decentralized federated learning settings.
In most cases, the assumption of independent and identically distributed samples across local nodes does not hold for federated learning setups. Under this setting, the performances of the training process may vary significantly according to the unbalanced local data samples as well as the particular probability distribution of the training examples (i.e., features and labels) stored at the local nodes. To further investigate the effects of non-IID data, the following description considers the main categories presented in the preprint by Peter Kairouz et al. from 2019.
The description of non-IID data relies on the analysis of the joint probability between features and labels for each node.
This allows decoupling of each contribution according to the specific distribution available at the local nodes.
The main categories for non-iid data can be summarized as follows:
Covariate shift: local nodes may store examples that have different statistical distributions compared to other nodes. An example occurs in natural language processing datasets where people typically write the same digits/letters with different stroke widths or slants.
Prior probability shift: local nodes may store labels that have different statistical distributions compared to other nodes. This can happen if datasets are regional and/or demographically partitioned. For example, datasets containing images of animals vary significantly from country to country.
Concept drift (same label, different features): local nodes may share the same labels but some of them correspond to different features at different local nodes. For example, images that depict a particular object can vary according to the weather condition in which they were captured.
Concept shift (same features, different labels): local nodes may share the same features but some of them correspond to different labels at different local nodes. For example, in natural language processing, the sentiment analysis may yield different sentiments even if the same text is observed.
Unbalanced: the amount of data available at the local nodes may vary significantly in size.
The loss in accuracy due to non-iid data can be bounded through using more sophisticated means of doing data normalization, rather than batch normalization.
The way the statistical local outputs are pooled and the way the nodes communicate with each other can change from the centralized model explained in the previous section. This leads to a variety of federated learning approaches: for instance no central orchestrating server, or stochastic communication.
In particular, orchestrator-less distributed networks are one important variation. In this case, there is no central server dispatching queries to local nodes and aggregating local models. Each local node sends its outputs to several randomly-selected others, which aggregate their results locally. This restrains the number of transactions, thereby sometimes reducing training time and computing cost.
Once the topology of the node network is chosen, one can control different parameters of the federated learning process (in addition to the machine learning model's own hyperparameters) to optimize learning:
Number of federated learning rounds:
Total number of nodes used in the process:
Fraction of nodes used at each iteration for each node:
Local batch size used at each learning iteration:
Other model-dependent parameters can also be tinkered with, such as:
Number of iterations for local training before pooling:
Local learning rate:

Those parameters have to be optimized depending on the constraints of the machine learning application (e.g., available computing power, available memory, bandwidth). For instance, stochastically choosing a limited fraction
of nodes for each iteration diminishes computing cost and may prevent overfitting, in the same way that stochastic gradient descent can reduce overfitting.
Federated learning requires frequent communication between nodes during the learning process. Thus, it requires not only enough local computing power and memory, but also high bandwidth connections to be able to exchange parameters of the machine learning model. However, the technology also avoids data communication, which can require significant resources before starting centralized machine learning. Nevertheless, the devices typically employed in federated learning are communication-constrained, for example IoT devices or smartphones are generally connected to Wi-Fi networks, thus, even if the models are commonly less expensive to be transmitted compared to raw data, federated learning mechanisms may not be suitable in their general form.
Federated learning raises several statistical challenges:
Heterogeneity between the different local datasets: each node may have some bias with respect to the general population, and the size of the datasets may vary significantly;
Temporal heterogeneity: each local dataset's distribution may vary with time;
Interoperability of each node's dataset is a prerequisite;
Each node's dataset may require regular curations;
Hiding training data might allow attackers to inject backdoors into the global model;
Lack of access to global training data makes it harder to identify unwanted biases entering the training e.g. age, gender, sexual orientation;
Partial or total loss of model updates due to node failures affecting the global model;
Lack of annotations or labels on the client side.
Heterogeneity between processing platforms
A number of different algorithms for federated optimization have been proposed.
Stochastic gradient descent is an approach used in deep learning, where gradients are computed on a random subset of the total dataset and then used to make one step of the gradient descent.
Federated stochastic gradient descent is the analog of this algorithm to the federated setting, but uses a random subset of the nodes, each node using all its data. The server averages the gradients in proportion to the number of training data on each node, and uses the average to make a gradient descent step.
Federated averaging (FedAvg) is a generalization of FedSGD which allows nodes to do more than one batch update on local data and exchange updated weights rather than gradients. This reduces communication and is equivalent to averaging the weights if all nodes start with the same weights. It does not seem to hurt the resulting averaged model's performance compared to FedSGD. FedAvg variations have been proposed based on adaptive optimizers such as ADAM and AdaGrad, and tend to outperform FedAvg.
Federated learning methods suffer when node datasets are distributed heterogeneously, because then minimizing the node losses is not the same as minimizing the global loss. In 2021, Acar et al. introduced a solution called FedDyn, which dynamically regularizes each node loss function so that they converge to the global loss. Since the local losses are aligned, FedDyn is robust to the different heterogeneity levels and so it can safely perform full minimization in each device. In theory, FedDyn converges to the optimal (a stationary point for nonconvex losses) by being agnostic to the heterogeneity levels. These claims are verified with extensive experiments on various datasets.
Besides reducing communication, it is also beneficial to reduce local computation. To do this, FedDynOneGD modifies FedDyn to calculate only one gradient per node per round, regularizes it and updates the global model with it. Hence, the computational complexity is linear in local dataset size. Moreover, gradient computation can be parallelized on each node, unlike successive SGD steps. In theory, FedDynOneGD achieves the same convergence guarantees as in FedDyn with less local computation.
Federated learning methods have poor global performance under non-IID settings. This motivates clients to yield personalized models in federation. To change this, Vahidian et al. recently introduced the algorithm Sub-FedAvg which does hybrid pruning (structured and unstructured pruning) with averaging on the intersection of clients’ drawn subnetworks. This simultaneously addresses communication efficiency, resource constraints and personalized models accuracies.
Sub-FedAvg also extends the "lottery ticket hypothesis" of centrally trained neural networks to federated learning, with the research question: “Do winning tickets exist for clients’ neural networks being trained in federated learning? If so, how to effectively draw the personalized subnetworks for each client?” Sub-FedAvg experimentally answers "yes" and proposes two algorithms to effectively draw the personalized subnetworks.
IDA (Inverse Distance Aggregation) is a novel adaptive weighting approach for federated learning nodes based on meta-information which handles unbalanced and non-iid data. It uses the distance of the model parameters as a strategy to minimize the effect of outliers and improve the model's convergence rate.
Very few methods for hybrid federated learning, where clients only hold subsets of both features and samples, exist. Yet, this scenario is very important in practical settings. Hybrid Federated Dual Coordinate Ascent (HyFDCA) is a novel algorithm proposed in 2024 that solves convex problems in the hybrid FL setting. This algorithm extends CoCoA, a primal-dual distributed optimization algorithm introduced by Jaggi et al. (2014) and Smith et al. (2017), to the case where both samples and features are partitioned across clients.
HyFDCA claims several improvement over existing algorithms:
HyFDCA is a provably convergent primal-dual algorithm for hybrid FL in at least the following settings.
Hybrid Federated Setting with Complete Client Participation
Horizontal Federated Setting with Random Subsets of Available Clients
The authors show HyFDCA enjoys a convergence rate of O(1⁄t) which matches the convergence rate of FedAvg (see below).
Vertical Federated Setting with Incomplete Client Participation
The authors show HyFDCA enjoys a convergence rate of O(log(t)⁄t) whereas FedBCD exhibits a slower O(1⁄sqrt(t)) convergence rate and requires full client participation.
HyFDCA provides the privacy steps that ensure privacy of client data in the primal-dual setting. These principles apply to future efforts in developing primal-dual algorithms for FL.
HyFDCA empirically outperforms HyFEM and FedAvg in loss function value and validation accuracy across a multitude of problem settings and datasets (see below for more details). The authors also introduce a hyperparameter selection framework for FL with competing metrics using ideas from multiobjective optimization.
There is only one other algorithm that focuses on hybrid FL, HyFEM proposed by Zhang et al. (2020). This algorithm uses a feature matching formulation that balances clients building accurate local models and the server learning an accurate global model. This requires a matching regularizer constant that must be tuned based on user goals and results in disparate local and global models. Furthermore, the convergence results provided for HyFEM only prove convergence of the matching formulation not of the original global problem. This work is substantially different than HyFDCA's approach which uses data on local clients to build a global model that converges to the same solution as if the model was trained centrally. Furthermore, the local and global models are synchronized and do not require the adjustment of a matching parameter between local and global models. However, HyFEM is suitable for a vast array of architectures including deep learning architectures, whereas HyFDCA is designed for convex problems like logistic regression and support vector machines.
HyFDCA is empirically benchmarked against the aforementioned HyFEM as well as the popular FedAvg in solving convex problem (specifically classification problems) for several popular datasets (MNIST, Covtype, and News20). The authors found HyFDCA converges to a lower loss value and higher validation accuracy in less overall time in 33 of 36 comparisons examined and 36 of 36 comparisons examined with respect to the number of outer iterations. Lastly, HyFDCA only requires tuning of one hyperparameter, the number of inner iterations, as opposed to FedAvg (which requires tuning three) or HyFEM (which requires tuning four). In addition to FedAvg and HyFEM being quite difficult to optimize hyperparameters in turn greatly affecting convergence, HyFDCA's single hyperparameter allows for simpler practical implementations and hyperparameter selection methodologies.
While most federated‑learning research focuses on gradient‑based models, ensemble trees have also been adapted. Cotorobai et al. (2025) introduced a privacy‑preserving framework for training Random Forest classifiers across multiple institutions without sharing raw data, achieving predictive performance within 9 % of centralized baselines on healthcare benchmarks.
Federated learning has started to emerge as an important research topic in 2015 and 2016, with the first publications on federated averaging in telecommunication settings. Before that, in a thesis work titled "A Framework for Multi-source Prefetching Through Adaptive Weight", an approach to aggregate predictions from multiple models trained at three location of a request response cycle with was proposed. Another important aspect of active research is the reduction of the communication burden during the federated learning process. In 2017 and 2018, publications have emphasized the development of resource allocation strategies, especially to reduce communication requirements between nodes with gossip algorithms as well as on the characterization of the robustness to differential privacy attacks. Other research activities focus on the reduction of the bandwidth during training through sparsification and quantization methods, where the machine learning models are sparsified and/or compressed before they are shared with other nodes. Developing ultra-light DNN architectures is essential for device-/edge- learning and recent work recognises both the energy efficiency requirements for future federated learning and the need to compress deep learning, especially during learning.
Recent research advancements are starting to consider real-world propagating channels as in previous implementations ideal channels were assumed. Another active direction of research is to develop Federated learning for training heterogeneous local models with varying computation complexities and producing a single powerful global inference model.
A learning framework named Assisted learning was recently developed to improve each agent's learning capabilities without transmitting private data, models, and even learning objectives. Compared with Federated learning that often requires a central controller to orchestrate the learning and optimization, Assisted learning aims to provide protocols for the agents to optimize and learn among themselves without a global model.
Federated learning typically applies when individual actors need to train models on larger datasets than their own, but cannot afford to share the data in itself with others (e.g., for legal, strategic or economic reasons). The technology yet requires good connections between local servers and minimum computational power for each node.
Self-driving cars encapsulate many machine learning technologies to function: computer vision for analyzing obstacles, machine learning for adapting their pace to the environment (e.g., bumpiness of the road). Due to the potential high number of self-driving cars and the need for them to quickly respond to real world situations, traditional cloud approach may generate safety risks. Federated learning can represent a solution for limiting volume of data transfer and accelerating learning processes.
In Industry 4.0, there is a widespread adoption of machine learning techniques to improve the efficiency and effectiveness of industrial process while guaranteeing a high level of safety. Nevertheless, privacy of sensitive data for industries and manufacturing companies is of paramount importance. Federated learning algorithms can be applied to these problems as they do not disclose any sensitive data. In addition, FL also implemented for PM2.5 prediction to support Smart city sensing applications.
Federated learning seeks to address the problem of data governance and privacy by training algorithms collaboratively without exchanging the data itself. Today's standard approach of centralizing data from multiple centers comes at the cost of critical concerns regarding patient privacy and data protection. To solve this problem, the ability to train machine learning models at scale across multiple medical institutions without moving the data is a critical technology. Nature Digital Medicine published the paper "The Future of Digital Health with Federated Learning" in September 2020, in which the authors explore how federated learning may provide a solution for the future of digital health, and highlight the challenges and considerations that need to be addressed. Recently, a collaboration of 20 different institutions around the world validated the utility of training AI models using federated learning. In a paper published in Nature Medicine "Federated learning for predicting clinical outcomes in patients with COVID-19", they showcased the accuracy and generalizability of a federated AI model for the prediction of oxygen needs in patients with COVID-19 infections. Furthermore, in a published paper "A Systematic Review of Federated Learning in the Healthcare Area: From the Perspective of Data Properties and Applications", the authors trying to provide a set of challenges on FL challenges on medical data-centric perspective.
A coalition from industry and academia has developed MedPerf, an open source platform that enables validation of medical AI models in real world data. The platform relies technically on federated evaluation of AI models aiming to alleviate concerns of patient privacy and conceptually on diverse benchmark committees to build the specifications of neutral clinically impactful benchmarks.
Robotics includes a wide range of applications of machine learning methods: from perception and decision-making to control. As robotic technologies have been increasingly deployed from simple and repetitive tasks (e.g. repetitive manipulation) to complex and unpredictable tasks (e.g. autonomous navigation), the need for machine learning grows. Federated Learning provides a solution to improve over conventional machine learning training methods. In the paper, mobile robots learned navigation over diverse environments using the FL-based method, helping generalization. In the paper, Federated Learning is applied to improve multi-robot navigation under limited communication bandwidth scenarios, which is a current challenge in real-world learning-based robotic tasks. In the paper, Federated Learning is used to learn Vision-based navigation, helping better sim-to-real transfer.
Federated Learning (FL) is transforming biometric recognition by enabling collaborative model training across distributed data sources while preserving privacy. By eliminating the need to share sensitive biometric templates like fingerprints, facial images, and iris scans, FL addresses privacy concerns and regulatory constraints, allowing for improved model accuracy and generalizability. It mitigates challenges of data fragmentation by leveraging scattered datasets, making it particularly effective for diverse biometric applications such as facial and iris recognition. However, FL faces challenges, including model and data heterogeneity, computational overhead, and vulnerability to security threats like inference attacks. Future directions include developing personalized FL frameworks, enhancing system efficiency, and expanding FL applications to biometric presentation attack detection (PAD) and quality assessment, fostering innovation and robust solutions in privacy-sensitive environments.
"Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016" at eur-lex.europa.eu. Retrieved October 18, 2019.
"Data minimisation and privacy-preserving techniques in AI systems" Archived 2020-07-23 at the Wayback Machine at UK Information Commissioners Office. Retrieved July 22, 2020
"Realising the Potential of Data Whilst Preserving Privacy with EyA and Conclave from R3" at eya.global. Retrieved March 31, 2022. Federated Learning of Cohorts (FLoC) is a type of web tracking. It groups people into "cohorts" based on their browsing history for the purpose of interest-based advertising. FLoC was being developed as a part of Google's Privacy Sandbox initiative, which includes several other advertising-related technologies with bird-themed names. Despite "federated learning" in the name, FLoC does not utilize any federated learning.
Google began testing the technology in Chrome 89 released in March 2021 as a replacement for third-party cookies. By April 2021, every major browser aside from Google Chrome that is based on Google's open-source Chromium platform had declined to implement FLoC. The technology was criticized on privacy grounds by groups including the Electronic Frontier Foundation and DuckDuckGo, and has been described as anti-competitive; it generated an antitrust response in multiple countries as well as questions about General Data Protection Regulation compliance. In July 2021, Google quietly suspended development of FLoC; Chrome 93, released on August 31, 2021, became the first version which disabled FLoC, but did not remove the internal programming.
On January 25, 2022, Google officially announced it had ended development of FLoC technologies and proposed the new Topics API to replace it. Brave developers criticized Topics API as a rebranding of FLoC with only minor changes and without addressing their main concerns.
The Federated Learning of Cohorts algorithm analyzes users' online activity within the browser, and generates a "cohort ID" using the SimHash algorithm to group a given user with other users who access similar content. Each cohort contains several thousand users in order to make identifying individual users more difficult, and cohorts are updated weekly. Websites are then able to access the cohort ID using an API and determine what advertisements to serve. Google does not label cohorts based on interest beyond grouping users and assigning an ID, so advertisers need to determine the user types of each cohort on their own.
FLoC experiment was active only in Google Chrome browser and ran from Chrome 89 (inclusive) to Chrome 93 (not inclusive). Modern browsers do not support FLoC. While the experiment was active, users could opt out of FLoC experiment by disabling third-party cookies. Website administrators could opt out from cohort calculation via special HTTP headers. It can be accomplished with a new interest-cohort permissions policy or feature policy, the default behavior is to allow cohort calculation. To opt-out of all FLoC cohort calculations a website could send either of the following HTTP response headers:
Feature-Policy: browsing-topics 'none'
Google Chrome applies interest-cohort Feature Policy restrictions to Browsing Topics API as well.
On August 22, 2019, Google Chrome developers coined the term FLoC and first started discussing the upcoming replacement for cookies. In July 2020, the United Kingdom's Competition and Markets Authority found the FLoC proposal to be anti-competitive, since it would "place the browser in a vital gatekeeper position for the adtech ecosystem." Instead, the authority recommended adoption of a competing proposal called SPARROW, which maintains the same privacy-enhancing objectives but creates a different completely independent "Gatekeeper" which does not have any other role in the adtech ecosystem and does not have access to user-level information.
Google began testing FLoC in the Chrome 89 released in March 2021 as a replacement for third-party cookies, which Google plans to stop supporting in Chrome by mid-2023. (Initially Google announced plans to remove third-party cookies by late 2021, then postponed it to early 2022, and then to 2023 due to delay of FLoC technology.) The initial trial turned on FLoC for 0.5% of Chrome users across 10 countries: the United States, Australia, Brazil, Canada, India, Indonesia, Japan, Mexico, New Zealand and the Philippines. Users were automatically placed in the trial and were not notified, but could opt out by turning off third-party cookies. Furthermore, site administrators could disable FLoC and opt out from interest calculation via a Feature-Policy header. The initial trial did not include users in the United Kingdom or the European Economic Area due to concerns about legality under the area's privacy regulations.
In July 2021, Google suspended development of FLoC; Chrome 93, released on August 31, 2021, became the first version which rendered FLoC feature void, but did not remove the internal programming. Chrome 100, released on March 29, 2022, removed most of old FLoC code.
On January 25, 2022, Google officially announced it had ended development of FLoC APIs and proposed a new Topics API to replace it. This API would use three weeks of the browser's history to identify user interests based on defined topics. Participating websites could then call this API to get three topics which could be used to tailor advertising. Developers of the Brave web browser called Topics API a "rebranding [of] FLoC without addressing key privacy issues.
Google claimed in January 2021 that FLoC was at least 95% effective compared to tracking using third-party cookies, but AdExchanger reported that some people in the advertising technology industry expressed skepticism about the claim and the methodology behind it. As every website that opts into FLoC will have the same access about which cohort the user belongs to, the technology's developers say this democratizes access to some information about a user's general browser history, in contrast to the status quo, where websites have to use tracking techniques.
The Electronic Frontier Foundation has criticized FLoC, with one EFF researcher calling the testing of the technology in Chrome "a concrete breach of user trust in service of a technology that should not exist" in a post on the organization's blog. The EFF also created a website which allows Chrome users to check whether FLoC is being tested in their browsers. The EFF criticized the fact that every site will be able to access data about a user, without having to track them across the web first. Additionally on the EFF blog, Cory Doctorow praised Chrome's planned removal of third-party cookies, but added that "[just] because FLoC is billed as pro-privacy and also criticized as anti-competitive, it doesn't mean that privacy and competition aren't compatible", stating that Google is "appointing itself the gatekeeper who decides when we're spied on while skimming from advertisers with nowhere else to go."
On April 10, 2021, the CEO of DuckDuckGo released a statement telling people not to use Google Chrome, stating that Chrome users can be included in FLoC without choosing to be and that no other browser vendor has expressed interest in using the tracking method. The statement said that "there is no such thing as a behavioral tracking mechanism imposed without consent that respects people's privacy" and that Google should make FLoC "explicitly opt-in" and "free of dark patterns". DuckDuckGo also announced that its website will not collect FLoC IDs or use them to target ads, and updated its Chrome extension to block websites from interacting with FLoC.
On April 12, 2021, Brave, a web browser built on the Chromium platform, criticized FLoC in a blog post and announced plans to disable FLoC in the Brave browser and make company's main website opt out of FLoC. The blog post, co-written by the company's CEO Brendan Eich, described Google's efforts to replace third-party cookies as "Titanic-level deckchair-shuffling" and "a step backward from more fundamental, privacy-and-user focused changes the Web needs."
Tech and media news site The Verge noted that not all possible repercussions of FLoC for ad tech are known, and that its structure could benefit or harm smaller ad tech companies, noting specifically that larger ad tech companies may be better equipped to "parse what FLoCs mean and what ads to target against them."
Multiple companies including GitHub, Drupal and Amazon declined to enable FLoC, instead opting to disable FLoC outright by including the HTTP Header Permissions-Policy: interest-cohort=(). WordPress, a widely used website framework floated a proposal to disable FLoC based tracking across all websites that used the framework.
Almost all major browsers based on Google's open-source Chromium platform declined to implement FLoC, including Microsoft Edge, Vivaldi, Brave, and Opera.
In May 2021, The Economist reported that it may be hard for Google to "stop the system from grouping people by characteristics they wish to keep private, such as race or sexuality."
In May 2021, The Economist said some critics have suggested that the cohort system will facilitate fingerprinting of individual devices, compromising privacy.
Wired magazine additionally reported that FLoC could "be used as a point of entry for fingerprinting".
Mozilla, the creators of the Firefox browser, expressed concerns that FLoC can be used as an additional fingerprinting vector. Furthermore, they stated that a user's FLoC group can be tracked during multiple visits and correlated via different means and, based on a user's membership in multiple FLoC cohorts, a website might be able to infer information about the user which FLoC aimed to keep private. Since a FLoC cohort is shared across websites, its ID might be abused as an alternative to a unique cookie in third-party contexts.
In July 2020, the United Kingdom's Competition and Markets Authority found that the FLoC proposal "place[s] the browser in a vital gatekeeper position for the adtech ecosystem."
In March 2021, 15 attorneys general of U.S. states and Puerto Rico amended an antitrust complaint filed in December; the updated complaint says that Google Chrome's phase-out of third-party cookies in 2022 will "disable the primary cookie-tracking technology almost all non-Google publishers currently use to track users and target ads. Then [...] Chrome, will offer [...] new and alternative tracking mechanisms [...] dubbed Privacy Sandbox. Overall, the changes are anticompetitive".
In June 2021, EU antitrust regulators launched a formal investigation to assess whether Google violated competition rules, with a focus on display advertising, notably whether it restricts access to user data by third parties while reserving it for its own use. Among the things that will be investigated is Google's plan to prohibit the placement of third-party cookies and replace them with the Privacy Sandbox set of tools.
As of April 2021, Google was not testing FLoC in the United Kingdom or the European Economic Area due to concerns about compliance with the General Data Protection Regulation and the ePrivacy Directive.
Johannes Caspar, the Data Protection Commissioner of Hamburg, Germany, told Wired UK that FLoC "leads to several questions concerning the legal requirements of the GDPR," explaining that FLoC "could be seen as an act of processing personal data" which requires "freely given consent and clear and transparent information about these operations." A spokesperson of the French National Commission on Informatics and Liberty said that the FLoC system would require "specific, informed and unambiguous consent".
As of April 2021, the Irish Data Protection Commission, which is the lead data supervisor for Google under GDPR, was consulting with Google about the FLoC proposal.
Am I FLoCed?—EFF website reporting to users if FLoC is enabled
FLoCs explained at the Privacy Sandbox Initiative website
FLoC Origin Trial & Clustering – infos from the Chromium project Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately.
Inherently, Multi-task learning is a multi-objective optimization problem having trade-offs between different tasks.
Early versions of MTL were called "hints".
In a widely cited 1997 paper, Rich Caruana gave the following characterization:Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better.
In the classification context, MTL aims to improve the performance of multiple classification tasks by learning them jointly. One example is a spam-filter, which can be treated as distinct but related classification tasks across different users. To make this more concrete, consider that different people have different distributions of features which distinguish spam emails from legitimate ones, for example an English speaker may find that all emails in Russian are spam, not so for Russian speakers. Yet there is a definite commonality in this classification task across users, for example one common feature might be text related to money transfer. Solving each user's spam classification problem jointly via MTL can let the solutions inform each other and improve performance. Further examples of settings for MTL include multiclass classification and multi-label classification.
Multi-task learning works because regularization induced by requiring an algorithm to perform well on a related task can be superior to regularization that prevents overfitting by penalizing all complexity uniformly. One situation where MTL may be particularly helpful is if the tasks share significant commonalities and are generally slightly under sampled. However, as discussed below, MTL has also been shown to be beneficial for learning unrelated tasks.
The key challenge in multi-task learning, is how to combine learning signals from multiple tasks into a single model. This may strongly depend on how well different task agree with each other, or contradict each other. There are several ways to address this challenge:
Within the MTL paradigm, information can be shared across some or all of the tasks. Depending on the structure of task relatedness, one may want to share information selectively across the tasks. For example, tasks may be grouped or exist in a hierarchy, or be related according to some general metric. Suppose, as developed more formally below, that the parameter vector modeling each task is a linear combination of some underlying basis. Similarity in terms of this basis can indicate the relatedness of the tasks. For example, with sparsity, overlap of nonzero coefficients across tasks indicates commonality. A task grouping then corresponds to those tasks lying in a subspace generated by some subset of basis elements, where tasks in different groups may be disjoint or overlap arbitrarily in terms of their bases. Task relatedness can be imposed a priori or learned from the data. Hierarchical task relatedness can also be exploited implicitly without assuming a priori knowledge or learning relations explicitly. For example, the explicit learning of sample relevance across tasks can be done to guarantee the effectiveness of joint learning across multiple domains.
One can attempt learning a group of principal tasks using a group of auxiliary tasks, unrelated to the principal ones. In many applications, joint learning of unrelated tasks which use the same input data can be beneficial. The reason is that prior knowledge about task relatedness can lead to sparser and more informative representations for each task grouping, essentially by screening out idiosyncrasies of the data distribution. Novel methods which builds on a prior multitask methodology by favoring a shared low-dimensional representation within each task grouping have been proposed. The programmer can impose a penalty on tasks from different groups which encourages the two representations to be orthogonal. Experiments on synthetic and real data have indicated that incorporating unrelated tasks can result in significant improvements over standard multi-task learning methods.
Related to multi-task learning is the concept of knowledge transfer. Whereas traditional multi-task learning implies that a shared representation is developed concurrently across tasks, transfer of knowledge implies a sequentially shared representation. Large scale machine learning projects such as the deep convolutional neural network GoogLeNet, an image-based object classifier, can develop robust representations which may be useful to further algorithms learning related tasks. For example, the pre-trained model can be used as a feature extractor to perform pre-processing for another learning algorithm. Or the pre-trained model can be used to initialize a model with similar architecture which is then fine-tuned to learn a different classification task.
Traditionally Multi-task learning and transfer of knowledge are applied to stationary learning settings. Their extension to non-stationary environments is termed Group online adaptive learning (GOAL). Sharing information could be particularly useful if learners operate in continuously changing environments, because a learner could benefit from previous experience of another learner to quickly adapt to their new environment. Such group-adaptive learning has numerous applications, from predicting financial time-series, through content recommendation systems, to visual understanding for adaptive autonomous agents.
Multi-task optimization focuses on solving optimizing the whole process. The paradigm has been inspired by the well-established concepts of transfer learning and multi-task learning in predictive analytics.
The key motivation behind multi-task optimization is that if optimization tasks are related to each other in terms of their optimal solutions or the general characteristics of their function landscapes, the search progress can be transferred to substantially accelerate the search on the other.
The success of the paradigm is not necessarily limited to one-way knowledge transfers from simpler to more complex tasks. In practice an attempt is to intentionally solve a more difficult task that may unintentionally solve several smaller problems.
There is a direct relationship between multitask optimization and multi-objective optimization.
In some cases, the simultaneous training of seemingly related tasks may hinder performance compared to single-task models. Commonly, MTL models employ task-specific modules on top of a joint feature representation obtained using a shared module. Since this joint representation must capture useful features across all tasks, MTL may hinder individual task performance if the different tasks seek conflicting representation, i.e., the gradients of different tasks point to opposing directions or differ significantly in magnitude. This phenomenon is commonly referred to as negative transfer. To mitigate this issue, various MTL optimization methods have been proposed. Commonly, the per-task gradients are combined into a joint update direction through various aggregation algorithms or heuristics.
There are several common approaches for multi-task optimization: Bayesian optimization, evolutionary computation, and approaches based on Game theory.
Multi-task Bayesian optimization is a modern model-based approach that leverages the concept of knowledge transfer to speed up the automatic hyperparameter optimization process of machine learning algorithms. The method builds a multi-task Gaussian process model on the data originating from different searches progressing in tandem. The captured inter-task dependencies are thereafter utilized to better inform the subsequent sampling of candidate solutions in respective search spaces.
Evolutionary multi-tasking has been explored as a means of exploiting the implicit parallelism of population-based search algorithms to simultaneously progress multiple distinct optimization tasks. By mapping all tasks to a unified search space, the evolving population of candidate solutions can harness the hidden relationships between them through continuous genetic transfer. This is induced when solutions associated with different tasks crossover. Recently, modes of knowledge transfer that are different from direct solution crossover have been explored.
Game-theoretic approaches to multi-task optimization propose to view the optimization problem as a game, where each task is a player. All players compete through the reward matrix of the game, and try to reach a solution that satisfies all players (all tasks). This view provide insight about how to build efficient algorithms based on gradient descent optimization (GD), which is particularly important for training deep neural networks. In GD for MTL, the problem is that each task provides its own loss, and it is not clear how to combine all losses and create a single unified gradient, leading to several different aggregation strategies. This aggregation problem can be solved by defining a game matrix where the reward of each player is the agreement of its own gradient with the common gradient, and then setting the common gradient to be the Nash Cooperative bargaining of that system.
Algorithms for multi-task optimization span a wide array of real-world applications. Recent studies highlight the potential for speed-ups in the optimization of engineering design parameters by conducting related designs jointly in a multi-task manner. In machine learning, the transfer of optimized features across related data sets can enhance the efficiency of the training process as well as improve the generalization capability of learned models. In addition, the concept of multi-tasking has led to advances in automatic hyperparameter optimization of machine learning models and ensemble learning.
Applications have also been reported in cloud computing, with future developments geared towards cloud-based on-demand optimization services that can cater to multiple customers simultaneously. Recent work has additionally shown applications in chemistry. In addition, some recent works have applied multi-task optimization algorithms in industrial manufacturing.
The MTL problem can be cast within the context of RKHSvv (a complete inner product space of vector-valued functions equipped with a reproducing kernel). In particular, recent focus has been on cases where task structure can be identified via a separable kernel, described below. The presentation here derives from Ciliberto et al., 2015.
Suppose the training data set is
_=\^,y_^)\_^
^\in 
^\in 
, where t indexes task, and

^n_
. In this setting there is a consistent input and output space and the same loss function
:\mathbb  \times \mathbb  \rightarrow \mathbb  _
for each task: . This results in the regularized machine learning problem:

is a vector valued reproducing kernel Hilbert space with functions
\rightarrow ^
:\rightarrow 
The reproducing kernel for the space

\rightarrow \mathbb  ^
is a symmetric matrix-valued function
\times \rightarrow \mathbb  ^
, such that

and the following reproducing property holds:
The reproducing kernel gives rise to a representer theorem showing that any solution to equation 1 has the form:
The form of the kernel Γ induces both the representation of the feature space and structures the output across tasks. A natural simplification is to choose a separable kernel, which factors into separate kernels on the input space X and on the tasks
. In this case the kernel relating scalar components
is given by
,t),(x_,s))=k(x_,x_)k_(s,t)=k(x_,x_)A_
. For vector valued functions

we can write
,x_)=k(x_,x_)A
, where k is a scalar reproducing kernel, and A is a symmetric positive semi-definite

matrix. Henceforth denote
^=\\\subset \mathbb  ^
This factorization property, separability, implies the input feature space representation does not vary by task. That is, there is no interaction between the input kernel and the task kernel. The structure on tasks is represented solely by A. Methods for non-separable kernels Γ is a current field of research.
For the separable case, the representation theorem is reduced to
^k(x,x_)Ac_
. The model output on the training data is then KCA , where K is the

empirical kernel matrix with entries
, and C is the

matrix of rows
With the separable kernel, equation 1 can be rewritten as
where V is a (weighted) average of L applied entry-wise to Y and KCA. (The weight is zero if
is a missing observation).
Note the second term in P can be derived as follows:
\|f\|_^&=\left\langle \sum _^k(\cdot ,x_)Ac_,\sum _^k(\cdot ,x_)Ac_\right\rangle _\\&=\sum _^\langle k(\cdot ,x_)Ac_,k(\cdot ,x_)Ac_\rangle _&\\&=\sum _^\langle k(x_,x_)Ac_,c_\rangle _ ^&\\&=\sum _^k(x_,x_)c_^Ac_=tr(KCAC^)\end
There are three largely equivalent ways to represent task structure: through a regularizer; through an output metric, and through an output mapping.
Via the regularizer formulation, one can represent a variety of task structures easily.
=\gamma I_+(\gamma -\lambda )\mathbf  \mathbf  ^
is the TxT identity matrix, and
 \mathbf  ^
is the TxT matrix of ones) is equivalent to letting Γ control the variance
||f_-||__
of tasks from their mean
\sum _f_
. For example, blood levels of some biomarker may be taken on T patients at
time points during the course of a day and interest may lie in regularizing the variance of the predictions across patients.
=\alpha I_+(\alpha -\lambda )M
=|\mathbb  (t,s\in G_)
is equivalent to letting

control the variance measured with respect to a group mean:
\sum _||f_-|\sum _)f_||
the cardinality of group r, and
 
is the indicator function). For example, people in different political parties (groups) might be regularized together with respect to predicting the favorability rating of a politician. Note that this penalty reduces to the first when all tasks are in the same group.
=\delta I_+(\delta -\lambda )L
is the Laplacian for the graph with adjacency matrix M giving pairwise similarities of tasks. This is equivalent to giving a larger penalty to the distance separating tasks t and s when they are more similar (according to the weight

||f_-f_||__^M_
All of the above choices of A also induce the additional regularization term
||f||__^
which penalizes complexity in f more broadly.
Learning problem P can be generalized to admit learning task matrix A as follows:
^\rightarrow \mathbb  _
must be designed to learn matrices A of a given type. See "Special cases" below.
Restricting to the case of convex losses and coercive penalties Ciliberto et al. have shown that although Q is not convex jointly in C and A, a related problem is jointly convex.
Specifically on the convex set
=\ ^\times S_^|Range(C^KC)\subseteq Range(A)\
, the equivalent problem
is convex with the same minimum value. And if
is a minimizer for R then
A_^,A_)
is a minimizer for Q.
R may be solved by a barrier method on a closed set by introducing the following perturbation:
The perturbation via the barrier
tr(A^)
forces the objective functions to be equal to

on the boundary of
\times S_^
S can be solved with a block coordinate descent method, alternating in C and A. This results in a sequence of minimizers
in S that converges to the solution in R as
\rightarrow 0
, and hence gives the solution to Q.
Spectral penalties - Dinnuzo et al suggested setting F as the Frobenius norm
A)
. They optimized Q directly using block coordinate descent, not accounting for difficulties at the boundary of
 ^\times S_^
Clustered tasks learning - Jacob et al suggested to learn A in the setting where T tasks are organized in R disjoint clusters. In this case let
^
be the matrix with
=\mathbb  (t\in r)
E^
\mathbf  ^
, the task matrix

can be parameterized as a function of
(M)=\epsilon _U+\epsilon _(M-U)+\epsilon (I-M)
, with terms that penalize the average, between clusters variance and within clusters variance respectively of the task predictions. M is not convex, but there is a convex relaxation
_=\^:I-M\in S_^\land tr(M)=r\
. In this formulation,
 (A(M)\in \_\)
Non-convex penalties - Penalties can be constructed such that A is constrained to be a graph Laplacian, or that A has low rank factorization. However these penalties are not convex, and the analysis of the barrier method proposed by Ciliberto et al. does not go through in these cases.
Non-separable kernels - Separable kernels are limited, in particular they do not account for structures in the interaction space between the input and output domains jointly. Future work is needed to develop models for these kernels.
A Matlab package called Multi-Task Learning via StructurAl Regularization (MALSAR) implements the following multi-task learning algorithms: Mean-Regularized Multi-Task Learning, Multi-Task Learning with Joint Feature Selection, Robust Multi-Task Feature Learning, Trace-Norm Regularized Multi-Task Learning, Alternating Structural Optimization, Incoherent Low-Rank and Sparse Learning, Robust Low-Rank Multi-Task Learning, Clustered Multi-Task Learning, Multi-Task Learning with Graph Structures.
Multi-Target Prediction: A Unifying View on Problems and Methods Willem Waegeman, Krzysztof Dembczynski, Eyke Huellermeier https://arxiv.org/abs/1809.02352v1
The Biosignals Intelligence Group at UIUC
Washington University in St. Louis Department of Computer Science
The Multi-Task Learning via Structural Regularization Package
Online Multi-Task Learning Toolkit (OMT) A general-purpose online multi-task learning toolkit based on conditional random field models and stochastic gradient descent training (C#, .NET) Self-supervised learning (SSL) is a paradigm in machine learning where a model is trained on a task using the data itself to generate supervisory signals, rather than relying on externally-provided labels. In the context of neural networks, self-supervised learning aims to leverage inherent structures or relationships within the input data to create meaningful training signals. SSL tasks are designed so that solving them requires capturing essential features or relationships in the data. The input data is typically augmented or transformed in a way that creates pairs of related samples, where one sample serves as the input, and the other is used to formulate the supervisory signal. This augmentation can involve introducing noise, cropping, rotation, or other transformations. Self-supervised learning more closely imitates the way humans learn to classify objects.
During SSL, the model learns in two steps. First, the task is solved based on an auxiliary or pretext classification task using pseudo-labels, which help to initialize the model parameters. Next, the actual task is performed with supervised or unsupervised learning.
Self-supervised learning has produced promising results in recent years, and has found practical application in fields such as audio processing, and is being used by Facebook and others for speech recognition.
Autoassociative self-supervised learning is a specific category of self-supervised learning where a neural network is trained to reproduce or reconstruct its own input data. In other words, the model is tasked with learning a representation of the data that captures its essential features or structure, allowing it to regenerate the original input.
The term "autoassociative" comes from the fact that the model is essentially associating the input data with itself. This is often achieved using autoencoders, which are a type of neural network architecture used for representation learning. Autoencoders consist of an encoder network that maps the input data to a lower-dimensional representation (latent space), and a decoder network that reconstructs the input from this representation.
The training process involves presenting the model with input data and requiring it to reconstruct the same data as closely as possible. The loss function used during training typically penalizes the difference between the original input and the reconstructed output (e.g. mean squared error). By minimizing this reconstruction error, the autoencoder learns a meaningful representation of the data in its latent space.
For a binary classification task, training data can be divided into positive examples and negative examples. Positive examples are those that match the target. For example, if training a classifier to identify birds, the positive training data would include images that contain birds. Negative examples would be images that do not. Contrastive self-supervised learning uses both positive and negative examples. The loss function in contrastive learning is used to minimize the distance between positive sample pairs, while maximizing the distance between negative sample pairs.
An early example uses a pair of 1-dimensional convolutional neural networks to process a pair of images and maximize their agreement.
Contrastive Language-Image Pre-training (CLIP) allows joint pretraining of a text encoder and an image encoder, such that a matching image-text pair have image encoding vector and text encoding vector that span a small angle (having a large cosine similarity).
InfoNCE (Noise-Contrastive Estimation) is a method to optimize two models jointly, based on Noise Contrastive Estimation (NCE). Given a set
,\ldots x_\right\
random samples containing one positive sample from
\mid c_\right)
negative samples from the 'proposal' distribution
, it minimizes the following loss function:
_ =-\mathbb  _\left[\log \left(x_,c_\right)\in Xf_\left(x_,c_\right)\right]
Non-contrastive self-supervised learning (NCSSL) uses only positive examples. Counterintuitively, NCSSL converges on a useful local minimum rather than reaching a trivial solution, with zero loss. For the example of binary classification, it would trivially learn to classify each example as positive. Effective NCSSL requires an extra predictor on the online side that does not back-propagate on the target side.
SSL belongs to supervised learning methods insofar as the goal is to generate a classified output from the input. At the same time, however, it does not require the explicit use of labeled input-output pairs. Instead, correlations, metadata embedded in the data, or domain knowledge present in the input are implicitly and autonomously extracted from the data. These supervisory signals, extracted from the data, can then be used for training.
SSL is similar to unsupervised learning in that it does not require labels in the sample data. Unlike unsupervised learning, however, learning is not done using inherent data structures.
Semi-supervised learning combines supervised and unsupervised learning, requiring only a small portion of the learning data be labeled.
In transfer learning, a model designed for one task is reused on a different task.
Training an autoencoder intrinsically constitutes a self-supervised process, because the output pattern needs to become an optimal reconstruction of the input pattern itself. However, in current jargon, the term 'self-supervised' often refers to tasks based on a pretext-task training setup. This involves the (human) design of such pretext task(s), unlike
the case of fully self-contained autoencoder training.
In reinforcement learning, self-supervising learning from a combination of losses can create abstract representations where only the most important information about the state are kept in a compressed way.
Self-supervised learning is particularly suitable for speech recognition. For example, Facebook developed wav2vec, a self-supervised algorithm, to perform speech recognition using two deep convolutional neural networks that build on each other.
Google's Bidirectional Encoder Representations from Transformers (BERT) model is used to better understand the context of search queries.
OpenAI's GPT-3 is an autoregressive language model that can be used in language processing. It can be used to translate texts or answer questions, among other things.
Bootstrap Your Own Latent (BYOL) is a NCSSL that produced excellent results on ImageNet and on transfer and semi-supervised benchmarks.
The Yarowsky algorithm is an example of self-supervised learning in natural language processing. From a small number of labeled examples, it learns to predict which word sense of a polysemous word is being used at a given point in text.
DirectPred is a NCSSL that directly sets the predictor weights instead of learning it via typical gradient descent.
Self-GenomeNet is an example of self-supervised learning in genomics.
Self-supervised learning continues to gain prominence as a new approach across diverse fields. Its ability to leverage unlabeled data effectively opens new possibilities for advancement in machine learning, especially in data-driven application domains.
Balestriero, Randall; Ibrahim, Mark; Sobal, Vlad; Morcos, Ari; Shekhar, Shashank; Goldstein, Tom; Bordes, Florian; Bardes, Adrien; Mialon, Gregoire; Tian, Yuandong; Schwarzschild, Avi; Wilson, Andrew Gordon; Geiping, Jonas; Garrido, Quentin; Fernandez, Pierre (24 April 2023). "A Cookbook of Self-Supervised Learning". arXiv:2304.12210 [cs.LG].
Doersch, Carl; Zisserman, Andrew (October 2017). "Multi-task Self-Supervised Visual Learning". 2017 IEEE International Conference on Computer Vision (ICCV). pp. 2070–2079. arXiv:1708.07860. doi:10.1109/ICCV.2017.226. ISBN 978-1-5386-1032-9. S2CID 473729.
Doersch, Carl; Gupta, Abhinav; Efros, Alexei A. (December 2015). "Unsupervised Visual Representation Learning by Context Prediction". 2015 IEEE International Conference on Computer Vision (ICCV). pp. 1422–1430. arXiv:1505.05192. doi:10.1109/ICCV.2015.167. ISBN 978-1-4673-8391-2. S2CID 9062671.
Zheng, Xin; Wang, Yong; Wang, Guoyou; Liu, Jianguo (1 April 2018). "Fast and robust segmentation of white blood cell images by self-supervised learning". Micron. 107: 55–71. doi:10.1016/j.micron.2018.01.010. ISSN 0968-4328. PMID 29425969. S2CID 3796689.
Yarowsky, David (1995). "Unsupervised Word Sense Disambiguation Rivaling Supervised Methods". Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics. Cambridge, MA: Association for Computational Linguistics: 189–196. doi:10.3115/981658.981684. Retrieved 1 November 2022.   Zero-shot learning (ZSL) is a problem setup in deep learning where, at test time, a learner observes samples from classes which were not observed during training, and needs to predict the class that they belong to. The name is a play on words based on the earlier concept of one-shot learning, in which classification can be learned from only one, or a few, examples.
Zero-shot methods generally work by associating observed and non-observed classes through some form of auxiliary information, which encodes observable distinguishing properties of objects. For example, given a set of images of animals to be classified, along with auxiliary textual descriptions of what animals look like, an artificial intelligence model which has been trained to recognize horses, but has never been given a zebra, can still recognize a zebra when it also knows that zebras look like striped horses. This problem is widely studied in computer vision, natural language processing, and machine perception.
The first paper on zero-shot learning in natural language processing appeared in a 2008 paper by Chang, Ratinov, Roth, and Srikumar, at the AAAI’08, but the name given to the learning paradigm there was dataless classification. The first paper on zero-shot learning in computer vision appeared at the same conference, under the name zero-data learning. The term zero-shot learning itself first appeared in the literature in a 2009 paper from Palatucci, Hinton, Pomerleau, and Mitchell at NIPS’09. This terminology was repeated later in another computer vision paper and the term zero-shot learning caught on, as a take-off on one-shot learning that was introduced in computer vision years earlier.
In computer vision, zero-shot learning models learned parameters for seen classes along with their class representations and rely on representational similarity among class labels so that, during inference, instances can be classified into new classes.
In natural language processing, the key technical direction developed builds on the ability to "understand the labels"—represent the labels in the same semantic space as that of the documents to be classified. This supports the classification of a single example without observing any annotated data, the purest form of zero-shot classification. The original paper made use of the Explicit Semantic Analysis (ESA) representation but later papers made use of other representations, including dense representations. This approach was also extended to multilingual domains, fine entity typing and other problems. Moreover, beyond relying solely on representations, the computational approach has been extended to depend on transfer from other tasks, such as textual entailment and question answering.
The original paper also points out that, beyond the ability to classify a single example, when a collection of examples is given, with the assumption that they come from the same distribution, it is possible to bootstrap the performance in a semi-supervised like manner (or transductive learning).
Unlike standard generalization in machine learning, where classifiers are expected to correctly classify new samples to classes they have already observed during training, in ZSL, no samples from the classes have been given during training the classifier. It can therefore be viewed as an extreme case of domain adaptation.
Naturally, some form of auxiliary information has to be given about these zero-shot classes, and this type of information can be of several types.
Learning with attributes: classes are accompanied by pre-defined structured description. For example, for bird descriptions, this could include "red head", "long beak". These attributes are often organized in a structured compositional way, and taking that structure into account improves learning. While this approach was used mostly in computer vision, there are some examples for it also in natural language processing.
Learning from textual description. As pointed out above, this has been the key direction pursued in natural language processing. Here class labels are taken to have a meaning and are often augmented with definitions or free-text natural-language description. This could include for example a wikipedia description of the class.
Class-class similarity. Here, classes are embedded in a continuous space. A zero-shot classifier can predict that a sample corresponds to some position in that space, and the nearest embedded class is used as a predicted class, even if no such samples were observed during training.
The above ZSL setup assumes that at test time, only zero-shot samples are given, namely, samples from new unseen classes. In generalized zero-shot learning, samples from both new and known classes, may appear at test time. This poses new challenges for classifiers at test time, because it is very challenging to estimate if a given sample is new or known. Some approaches to handle this include:
a gating module, which is first trained to decide if a given sample comes from a new class or from an old one, and then, at inference time, outputs either a hard decision, or a soft probabilistic decision
a generative module, which is trained to generate feature representation of the unseen classes--a standard classifier can then be trained on samples from all classes, seen and unseen.
Zero shot learning has been applied to the following fields:
natural language processing
One-shot learning in computer vision